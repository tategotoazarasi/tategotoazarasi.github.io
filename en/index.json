[{"content":"Recently, I undertook a rather challenging task: to fully reproduce a paper titled \u0026ldquo;Physics-informed deep generative learning for quantitative assessment of the retina\u0026rdquo; on a High-Performance Computing (HPC) cluster. The core software repository for this paper is RetinaSim. The goal was not merely to run the code, but to completely replicate its complex software stack and simulation workflow in a strictly managed computational environment, one that likely differed significantly from the original developers\u0026rsquo;. This blog post will chronicle my entire journey from the initial attempt to the final successful run, focusing on my chain of thought as I diagnosed and resolved a series of tricky issues.\nOur battlefield was a typical HPC cluster, \u0026ldquo;Barkla2,\u0026rdquo; running Rocky Linux 9 and using Slurm as its job scheduler. This meant all operations had to be performed via the command line, and any time-consuming computational tasks had to be submitted as batch jobs rather than run directly on the login node. The RetinaSim project itself is a complex hybrid, merging Python scripts for main workflow control with a high-performance fluid dynamics simulator (Reanimate) written in C++ and a vessel generation program (RetinaGen) written in .NET (C#). This heterogeneous technology stack almost guaranteed that we would encounter a variety of unexpected compilation and runtime problems in a new environment.\nMy first step was to obtain the code and draft an initial Slurm script. The code was cloned via git, and its directory structure clearly laid out the various submodules. After a preliminary analysis of files like README and CMakeLists.txt, I understood that compiling Reanimate required CMake and a C++ compiler, while RetinaGen needed the .NET SDK. The Python part depended on a requirements.txt file.\nBased on this information, I wrote the first version of my Slurm script. The script\u0026rsquo;s goal was to complete all preparatory work sequentially: load the necessary environment modules (like the GCC compiler, CMake, and Python), create a Python virtual environment and install dependencies, compile the C++ and .NET submodules, and finally, attempt to run the main Python script, main.py. This was a standard, seemingly straightforward process. However, reality quickly delivered its first blow.\nFirst Failure: CMakeLists.txt not found Seconds after submitting the first job, it failed. Checking the error log, I saw a familiar and fundamental error message: CMake Error: The source directory \u0026quot;...\u0026quot; does not appear to contain CMakeLists.txt. This error means that CMake could not find its core configuration file, CMakeLists.txt, in the directory I had specified.\nMy immediate reaction was to check the paths in my script. To compile the Reanimate submodule, my Slurm script had changed directory to retinasim/Reanimate/Reanimate and then executed the cmake . command. This command tells CMake to use the current directory as the root of the source tree. However, after carefully inspecting the project structure with the ls -R command, I discovered that the CMakeLists.txt file was actually located in the retinasim/Reanimate directory, not in its subdirectory retinasim/Reanimate/Reanimate. This was a classic relative path error, one that is easy to make, especially when dealing with nested subprojects.\nThe diagnosis was straightforward. Since CMakeLists.txt was in the parent directory, the solution was to tell CMake to look there. In Unix-like systems, \u0026ldquo;..\u0026rdquo; represents the parent directory. Therefore, I needed to change the compile command from cmake . to cmake ... This small change had significant meaning: it instructed CMake to look one level up from the current directory (Reanimate/Reanimate) for CMakeLists.txt, while still using the current Reanimate/Reanimate directory as the build directory. This way, CMake could find the configuration file and place all generated build files (like Makefiles, object files, and the final executable) in my current location, keeping the source tree clean.\nThis small episode, though simple, served as a reminder that when dealing with unfamiliar and complex projects, especially those involving multiple languages and build systems, the first step is always to carefully and patiently review the directory structure and build scripts. Assuming that configuration files will be in some \u0026ldquo;obvious\u0026rdquo; location is a common cause of elementary errors. After fixing this issue, I resubmitted the job with renewed confidence, hoping the compilation process would now proceed smoothly. However, the complexity of the HPC environment was far greater than this, and a deeper problem was waiting for me.\nSecond Failure: Conflict Between NVHPC and GCC After correcting the CMake path issue, the compilation process did indeed begin, but it came to a grinding halt shortly thereafter. This time, the error log was far more cryptic. It was no longer a simple file-not-found error but pointed deep inside the header files of a C++ library, reporting a series of errors like error: extra text after expected end of number. These errors all originated from the file armadillo_bits/include_superlu.hpp, which is part of the Armadillo linear algebra library and is used to integrate the SuperLU sparse matrix solver.\nThe error message itself was very strange. It complained about extraneous text following a number, and all instances pointed to the same preprocessor macro line: #if __has_include(ARMA_INCFILE_WRAP(ARMA_SLU_HEADER_A)) \u0026amp;\u0026amp; __has_include(ARMA_INCFILE_WRAP(ARMA_SLU_HEADER_B)). __has_include is a feature supported by modern C++ compilers to check for the existence of a header file at compile time. This type of error usually implies that the compiler is having trouble parsing this macro; it might not recognize the syntax, or the expanded content of the macro might not meet its expectations.\nInitially, I suspected that the versions of the Armadillo or SuperLU libraries were incompatible with the code. However, I was using Armadillo and SuperLU loaded via the Spack package manager, as recommended by the HPC administrators. The versions were relatively new and shouldn\u0026rsquo;t have had such basic syntax issues. I began to carefully review the job\u0026rsquo;s output log for more clues about the compilation process. Soon, I found the critical piece of information in the output from the CMake configuration stage:\n-- The C compiler identification is NVHPC 25.3.0 -- The CXX compiler identification is NVHPC 25.3.0\nThe mystery was solved. Even though I had loaded the GCC compiler via module load gcc/14.2.0, CMake had automatically selected the compiler from the NVIDIA HPC SDK (NVHPC). This is a common phenomenon on many modern HPC clusters, as the NVHPC compiler is often deeply integrated with the GPU environment, and the system may set it as the default C++ compiler.\nThis created a problem: the armadillo and superlu libraries I had loaded via Spack were almost certainly compiled using the system\u0026rsquo;s primary compiler, GCC 14.2.0. But now, CMake was instructing the NVHPC compiler to compile the Reanimate code, which depended on these GCC-compiled libraries. Armadillo\u0026rsquo;s header files, in an effort to be cross-platform compatible, contain a large number of preprocessor macros targeted at different compilers. The line that was failing, __has_include, was likely a modern feature specific to GCC or Clang. The version of NVHPC 25.3.0 I was using might not have supported it well, or it might have produced syntactically incompatible code upon macro expansion. This was a classic case of \u0026ldquo;environment hell\u0026rdquo; caused by mixed compilers.\nThe solution had to be to force CMake to use the GCC compiler I had specified, ensuring consistency across the entire build toolchain. To achieve this, I took several steps. First, in the Slurm script, I exported two crucial environment variables: export CC=gcc and export CXX=g++. These variables are a convention in Unix-like environments for specifying the default C and C++ compilers. When CMake starts, it checks these environment variables and gives them priority.\nHowever, simply setting environment variables wasn\u0026rsquo;t foolproof. CMake has a very important feature: it caches the compiler and environment information it detects during the first configuration into a file named CMakeCache.txt. If I didn\u0026rsquo;t clear this cache, CMake would stubbornly continue to use the NVHPC compiler it had found in the previous failed attempt, even with the new environment variables set. Therefore, before running the cmake command, I had to clean the build directory and remove all old CMake-generated files. I added the command rm -rf CMakeCache.txt CMakeFiles cmake_install.cmake Makefile to my script to ensure a completely fresh configuration every time.\nTo be absolutely certain, I also added parameters directly to the cmake command itself to specify the compilers: cmake -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++ ... This method has the highest priority and overrides any environment variables or system defaults.\nThis repair process was far more complex than the first. It required an understanding of the HPC\u0026rsquo;s module system, the Spack package manager, CMake\u0026rsquo;s inner workings, and the differences between C++ compilers. This problem also highlighted how crucial the principle of \u0026ldquo;explicit is better than implicit\u0026rdquo; is when building software in an HPC environment. One cannot rely on automatic detection by tools; one must explicitly tell them which compiler and which libraries to use to achieve predictable and reproducible results in a complex environment. With this deeper understanding, I resubmitted the job. This time, the compilation process successfully generated the object files, but the linking stage threw a new challenge.\nThird Failure: Linker Cannot Find Libraries The compilation process went through smoothly; all .cpp files were successfully compiled into .o object files. However, during the final linking stage, when ld (the linker) tried to link all the object files and external libraries into the final executable Reanimate, it failed. The error message was crystal clear:\n/usr/bin/ld: cannot find -lopenblas /usr/bin/ld: cannot find -lsuperlu\nThe linker was complaining that it couldn\u0026rsquo;t find the openblas and superlu libraries. This was very puzzling because I had explicitly loaded them at the beginning of the script using module load openblas/0.3.29/gcc-14.2.0 and spack load superlu@5.3.0. Theoretically, the environment should have been configured correctly.\nTo diagnose this issue, I needed to understand how library search paths work during compilation and linking. In Unix-like systems, there are two key environment variables: LD_LIBRARY_PATH and LIBRARY_PATH. LD_LIBRARY_PATH is primarily for runtime, telling the dynamic linker where to find shared libraries (.so files) when a program starts. LIBRARY_PATH, on the other hand, is for compile-time, providing the linker ld with an additional list of paths to search for both static (.a) and shared libraries. The module load and spack load commands typically update LD_LIBRARY_PATH correctly, but whether they update LIBRARY_PATH, or whether cmake automatically uses LIBRARY_PATH, is not always guaranteed.\nThe CMakeLists.txt file used commands like link_libraries(-llapack -lopenblas -lsuperlu). The -l\u0026lt;name\u0026gt; syntax only tells the linker that it needs a library named lib\u0026lt;name\u0026gt;.so or lib\u0026lt;name\u0026gt;.a, but it doesn\u0026rsquo;t tell it where to find it. The linker searches a series of default paths (like /usr/lib, /usr/local/lib) as well as paths specified by the -L/path/to/lib argument. Clearly, the installation paths for openblas and superlu were not in the default search paths, and CMake had not automatically added them.\nMy solution had to be to find the actual installation paths of these libraries in the Slurm script and explicitly pass them to CMake. This required some scripting skills. For Spack-installed packages, I could use the command spack location -i \u0026lt;package_name\u0026gt; to get the installation root directory. For example, spack location -i superlu@5.3.0 would return a path like /mnt/data2/users/.../superlu-5.3.0-.... The library files are usually in a lib or lib64 subdirectory. For packages loaded via module, there is often an environment variable like OPENBLAS_ROOT that points to the installation root. If not, I could fall back to parsing the LD_LIBRARY_PATH environment variable to find the path containing \u0026ldquo;openblas\u0026rdquo;.\nI added logic to my script to automatically detect these paths and store them in variables like SUPERLU_LIB, ARMADILLO_LIB, and OPENBLAS_LIB. Next, I needed to pass this path information to the linker. The most direct and robust method is to use CMake\u0026rsquo;s CMAKE_EXE_LINKER_FLAGS variable. I constructed a string like LINKER_FLAGS=\u0026quot;-L/path/to/superlu/lib -L/path/to/openblas/lib\u0026quot; and then passed it to CMake via the argument -DCMAKE_EXE_LINKER_FLAGS=\u0026quot;$LINKER_FLAGS\u0026quot;. This ensures that in the final g++ linking command, these -L flags are correctly added, allowing ld to find the necessary library files. To be safe, I also passed the corresponding header file paths (-I/path/to/include) to CMAKE_CXX_FLAGS.\nThis issue once again confirmed the importance of explicitly specifying paths in an HPC environment. Merely loading a module is not always sufficient for all toolchains (especially complex build systems like CMake) to work seamlessly. A developer needs to understand the entire process from compilation to linking and know how to intervene manually when necessary to \u0026ldquo;translate\u0026rdquo; environment information into a language the build tools can understand. This fix gave me a deeper appreciation for the interaction between CMake and environment modules. With both compilation and linking successful, the Reanimate executable was finally generated. Next up was the .NET part.\nFourth Failure: .NET Runtime Not Found With the C++ part successfully compiled and the .NET dotnet build also completed, generating RetinaGen.dll, I was on the verge of success. I eagerly awaited the execution of the Python script. However, when the main program main.py reached the point where it called RetinaGen, the job crashed again. The error log showed:\nYou must install .NET to run this application. App: /.../RetinaGen/bin/Debug/net6.0/RetinaGen .NET location: Not found\nThis was a very perplexing problem. I had already loaded spack load dotnet-core-sdk@6.0.25 at the beginning of the script, and the dotnet build command had executed successfully, proving that the .NET SDK was present. Why, then, could the same program, when called from a Python script via subprocess.Popen, not find the .NET runtime?\nDiagnosing this requires an understanding of how .NET is deployed on Linux and the mechanisms of subprocess environment inheritance. The RetinaGen file generated by dotnet build is actually an \u0026ldquo;AppHost\u0026rdquo; executable. It\u0026rsquo;s a small, native launcher whose primary job is to find the .NET runtime on the system, load it, and then hand over the RetinaGen.dll (the actual assembly) to the runtime for execution. When this AppHost launcher fails to find the .NET runtime, it reports the error above.\nThe cause was likely related to environment propagation. Although I had loaded the Spack environment at the top level of my Slurm script, setting variables like PATH to make the dotnet executable visible, this environment might not have been fully inherited by the subprocess. When the Python interpreter starts as a process, and then it forks a child process to execute RetinaGen, this new child process may not inherit all the environment variables from its parent process (the Slurm job\u0026rsquo;s shell), especially those dynamically set by Spack to locate the .NET runtime (like DOTNET_ROOT).\nTo solve this, I decided to use a more robust way of calling the .NET program. Instead of running the AppHost (RetinaGen) directly, I could call the dotnet CLI directly and pass the DLL file as an argument: dotnet RetinaGen.dll. The advantage of this approach is that I\u0026rsquo;m directly using the dotnet executable, which itself knows how to find its associated runtime, thus bypassing the AppHost\u0026rsquo;s environment search problem. As long as dotnet is in the PATH, this command should work.\nTo implement this change, I couldn\u0026rsquo;t directly modify the Python source code in the repository, as this would affect its portability and integrity. The best approach was to \u0026ldquo;patch\u0026rdquo; it dynamically within the Slurm script. I once again turned to sed, the powerful stream editor. I first located the Python file that calls RetinaGen, which was retinasim/vascular.py. Then, before running main.py, I wrote a sed command to replace the line cmd = [exe_path, fname] in vascular.py with cmd = ['dotnet', exe_path, fname], while also changing the definition of EXE_PATH to point to RetinaGen.dll instead of RetinaGen. For safety, I created a backup file vascular.py.bak before making the modification.\nThis solution demonstrates an advanced technique for adapting to a specific runtime environment without altering the original codebase. In a batch processing environment, the ability to non-interactively and dynamically modify code to resolve environmental issues is an extremely practical skill. It not only solved the immediate problem but also kept the codebase clean, with all modifications documented in the Slurm script, making the entire process fully reproducible. After applying this patch, the .NET part of the call finally succeeded. But just when I thought I was done, one last obstacle related to the graphical interface appeared.\nFifth Failure: Open3D Rendering Crash After resolving all compilation and dependency issues, the program finally began executing its core simulation logic. However, within the generate_lsystem function, it crashed yet again. This time, the error was a Python runtime error related to the Open3D library:\n[Open3D WARNING] GLFW Error: Failed to detect any supported platform [Open3D WARNING] GLFW initialized for headless rendering. [Open3D WARNING] GLFW Error: OSMesa: Library not found [Open3D WARNING] Failed to create window AttributeError: 'NoneType' object has no attribute 'background_color'\nThe first part of the error message consists of warnings from Open3D. It tried to initialize a graphics window (via the GLFW library) but failed because it was running on a \u0026ldquo;headless\u0026rdquo; compute node without a physical display. It then attempted to fall back to offscreen rendering using OSMesa but also failed because the corresponding library was not found in the environment. Ultimately, because it could not create a window, the vis.create_window() call likely returned None.\nThe final AttributeError confirmed this diagnosis. The next line of code, opt.background_color = np.asarray(self.bgcolor), was trying to set the background color on a None object, causing the program to crash. Analyzing the generate_lsystem function call in main.py, I found a parameter screen_grab=True. This meant that even though I hadn\u0026rsquo;t requested an interactive display, the code was still trying to initialize a rendering environment to save an image.\nFor scientific computing tasks running on an HPC, intermediate visualizations are often unnecessary and should even be avoided. The goal is to obtain the final simulation data, not debug images. Therefore, the most direct and pragmatic solution was to disable this screenshot functionality.\nI once again resorted to sed. In the Slurm script, before running main.py, I added a command to patch the main.py file: sed -i 's/screen_grab=True/screen_grab=False/g' main.py. This command finds all instances of screen_grab=True in main.py and replaces them with screen_grab=False. I also added extra modifications to change the default values in argparse, ensuring that all plotting-related behaviors were turned off by default, even without command-line arguments. This fundamentally prevented any code path that would call Open3D\u0026rsquo;s window creation functions.\nThis fix embodies an important way of thinking in research and engineering practice: prioritizing what matters most. Fixing the complex headless rendering environment on an HPC node (which might require administrator privileges to install system-level dependencies) would have been a time-consuming task that strayed from the main objective. My core goal was to reproduce the simulation. Bypassing this problem with a simple code patch allowed me to focus on the final scientific output rather than struggling in the quagmire of environment configuration.\nAfter applying this final patch, I resubmitted the job. This time, there were no more errors in the log. I saw the program\u0026rsquo;s output, executing step by step as expected: creating the L-system seed network, writing Amira files, and launching the CCO vessel generation\u0026hellip; The program was finally running completely and successfully on the Barkla2 cluster.\nThis end-to-end reproduction process was full of challenges, but every step of debugging and resolution deepened my understanding of HPC environments, multi-language project builds, and software dependency management. From simple path errors to complex compiler and linker issues, and finally to runtime environment differences, this series of obstacles is a microcosm of the universal challenges faced in migrating and reproducing scientific computing software. Through systematic analysis, bold hypotheses, careful validation, and a few scripting tricks, we can ultimately tame this complex \u0026ldquo;beast,\u0026rdquo; allowing scientific research to proceed smoothly on powerful computational resources.\n","permalink":"https://blog.tategotoazarasi.me/en/posts/reproducing-retinasim-on-hpc-cluster/","summary":"\u003cp\u003eRecently, I undertook a rather challenging task: to fully reproduce a paper titled \u0026ldquo;Physics-informed deep generative learning for quantitative assessment of the retina\u0026rdquo; on a High-Performance Computing (HPC) cluster. The core software repository for this paper is RetinaSim. The goal was not merely to run the code, but to completely replicate its complex software stack and simulation workflow in a strictly managed computational environment, one that likely differed significantly from the original developers\u0026rsquo;. This blog post will chronicle my entire journey from the initial attempt to the final successful run, focusing on my chain of thought as I diagnosed and resolved a series of tricky issues.\u003c/p\u003e","title":"Reproducing RetinaSim on an HPC Cluster"},{"content":"Recently, I had the opportunity to participate in a hackathon organized jointly by the Liverpool City Region Combined Authority (LCR CA) and Homes England. The context was both ambitious and pragmatic: the government holds a housing pipeline list containing approximately 350 sites, totalling over 57,000 new homes. However, this data was sitting quietly in Excel spreadsheets, existing merely as cold numbers.\nOur task was: How do we make this data \u0026ldquo;come alive\u0026rdquo;?\nAs developers, we know that simply plotting \u0026ldquo;here is a house\u0026rdquo; on a map is far from sufficient. Urban planning is a complex systemic engineering challenge. Where are we building? Is that area safe? What is the fuel poverty rate there? Is there social deprivation? Should we prioritize developing abandoned \u0026ldquo;Brownfield\u0026rdquo; sites?\nTo answer these questions, my team and I built a geospatial analysis and visualization system based on Python. We didn\u0026rsquo;t stop at simple data display; instead, we deeply integrated crime statistics, deprivation indices, brownfield data, and housing plans. Furthermore, we implemented a \u0026ldquo;vulnerability heatmap\u0026rdquo; with dynamic weighting using custom JavaScript.\nThis post will review the entire technical implementation process, hopefully providing some insights for those interested in GIS (Geographic Information Systems) and data visualization.\nData Silos and Bridges Before writing a single line of code, our biggest challenge was the heterogeneity of our data sources. We had five completely different formats:\nHousing Pipeline Data: The core dataset, in Excel format, containing project names, developers, postcodes, unit counts, etc. Boundaries: GeoJSON format, defining the geographic boundaries of the Liverpool City Region. LSOA Data: Lower Layer Super Output Areas (LSOA) are small geographic units defined by the UK Office for National Statistics, serving as our baseline for spatial analysis. Index of Multiple Deprivation (IMD) \u0026amp; Fuel Poverty: Excel format, providing socioeconomic indicators for each LSOA. Police Crime Data: The trickiest part. Hundreds of CSV files stored in monthly folders, containing specific latitude/longitude coordinates. Brownfield Data: GeoJSON format, containing the location and area of abandoned land. Our goal was to map these \u0026ldquo;siloed\u0026rdquo; datasets from different dimensions onto a unified geographic unit: the LSOA.\nData Cleaning and Geocoding The housing pipeline data only contained postcodes, not coordinates. This is a common issue in geospatial analysis. We used the pgeocode library to solve this. It relies on the GeoNames database, offering extremely fast queries without the need for expensive API calls like Google Maps.\nimport pgeocode import pandas as pd # Initialize UK postcode query object nomi = pgeocode.Nominatim(\u0026#34;GB\u0026#34;) def batch_geocode(df, postcode_col): # Clean postcode format clean_postcodes = df[postcode_col].astype(str).str.strip() unique_pcs = clean_postcodes.unique() # Batch query for efficiency geo_results = nomi.query_postal_code(unique_pcs) # Create mapping dictionaries pc_to_lat = dict(zip(unique_pcs, geo_results.latitude)) pc_to_lon = dict(zip(unique_pcs, geo_results.longitude)) df[\u0026#34;lat\u0026#34;] = clean_postcodes.map(pc_to_lat) df[\u0026#34;lon\u0026#34;] = clean_postcodes.map(pc_to_lon) return df.dropna(subset=[\u0026#34;lat\u0026#34;, \u0026#34;lon\u0026#34;]) This step was crucial as it converted business data (Excel) into spatial data (Points).\nHandling Massive Police Data The data provided by the police was exhaustive, covering every month of crime records for the past three years, including coordinates for \u0026ldquo;Stop \u0026amp; Search\u0026rdquo; events. There were hundreds of files.\nIf we tried to read all CSVs and plot every point on a map, the browser would crash instantly. Therefore, Spatial Aggregation was the only way forward. We needed to calculate how many crimes occurred within each LSOA.\nWe used pathlib for recursive file searching combined with Pandas for cleaning:\nfrom pathlib import Path # Recursively find all CSV files all_csvs = list(Path(\u0026#34;police_data\u0026#34;).rglob(\u0026#34;*.csv\u0026#34;)) stop_search_points = [] for csv_file in all_csvs: try: # Read only necessary coordinate columns to save memory df = pd.read_csv(csv_file, usecols=[\u0026#34;Latitude\u0026#34;, \u0026#34;Longitude\u0026#34;]) df = df.dropna() # Roughly filter points outside Liverpool to speed up processing mask = (df[\u0026#39;Latitude\u0026#39;].between(53.0, 54.0)) \u0026amp; \\ (df[\u0026#39;Longitude\u0026#39;].between(-3.5, -2.0)) stop_search_points.append(df[mask]) except: continue # Concatenate all dataframes all_stops = pd.concat(stop_search_points, ignore_index=True) Spatial Join Now we had a pile of \u0026ldquo;points\u0026rdquo; (crime locations) and a pile of \u0026ldquo;polygons\u0026rdquo; (LSOA communities). The question we needed to answer was: Which polygon does this point belong to?\nIn geopandas, the sjoin (Spatial Join) function is the ultimate tool for this. It performs a database join operation based on geometric relationships.\nimport geopandas as gpd # Convert police data to GeoDataFrame gdf_stops = gpd.GeoDataFrame( all_stops, geometry=gpd.points_from_xy(all_stops.Longitude, all_stops.Latitude), crs=\u0026#34;EPSG:4326\u0026#34; ) # Core operation: Check which polygon contains the point (predicate=\u0026#39;within\u0026#39;) # lsoa_lcr is our administrative boundary polygon data stops_with_lsoa = gpd.sjoin(gdf_stops, lsoa_lcr[[\u0026#39;LSOA21CD\u0026#39;, \u0026#39;geometry\u0026#39;]], predicate=\u0026#39;within\u0026#39;) # Aggregation: Count crimes per LSOA stop_counts = stops_with_lsoa[\u0026#39;LSOA21CD\u0026#39;].value_counts().reset_index() stop_counts.columns = [\u0026#39;LSOA21CD\u0026#39;, \u0026#39;StopSearch_Count\u0026#39;] Through this step, we transformed tens of thousands of discrete coordinate points into a single statistical metric for each community. This is the key to dimensionality reduction in spatial data.\nSimilarly, we merged Fuel Poverty data and Index of Multiple Deprivation (IMD) data into our master GeoDataFrame using the LSOA21CD code.\nConstructing a \u0026ldquo;Vulnerability Score\u0026rdquo; \u0026amp; Dynamic Normalization With the raw data in hand, we faced a new problem: the dimensions of the metrics were completely different. Crime counts might range from 0 to 500, poverty rates from 0% to 100%, and IMD is a rank from 1 to 10. To display these comprehensively on a single map, we needed Normalization.\nWe used Min-Max normalization to map all metrics to a range between 0 and 1. Notably, for IMD (where 1 represents the most deprived), we needed to invert it so that 1.0 represents \u0026ldquo;High Risk/Vulnerability\u0026rdquo;.\n# Normalize Fuel Poverty Rate f_min, f_max = master_gdf[\u0026#34;Fuel_Poverty_Rate\u0026#34;].min(), master_gdf[\u0026#34;Fuel_Poverty_Rate\u0026#34;].max() master_gdf[\u0026#34;norm_fuel\u0026#34;] = (master_gdf[\u0026#34;Fuel_Poverty_Rate\u0026#34;] - f_min) / (f_max - f_min) # Normalize IMD (Invert: 10 is least deprived, 1 is most -\u0026gt; mapped so 1.0 is high risk) master_gdf[\u0026#34;norm_imd\u0026#34;] = (11 - master_gdf[\u0026#34;IMD_Decile\u0026#34;]) / 10.0 # Normalize Crime Data (Use 95th percentile as max to prevent outliers skewing the scale) c_max = master_gdf[\u0026#34;Crime_Count\u0026#34;].quantile(0.95) master_gdf[\u0026#34;norm_crime\u0026#34;] = (master_gdf[\u0026#34;Crime_Count\u0026#34;] / c_max).clip(upper=1.0) Why use the 95th percentile? In real-world urban data, crime rates in the City Centre are often orders of magnitude higher than in the suburbs. If we normalized using the absolute maximum, the City Centre would be deep red, while everywhere else would wash out to white, making suburban variations invisible. Clipping outliers preserves contrast for the majority of the region.\nVisualization A static chart generated by matplotlib is of limited use to decision-makers. We needed interactivity. We chose Folium (based on Leaflet.js), but native Folium functionality is limited—it doesn\u0026rsquo;t support \u0026ldquo;dragging a slider to change weights in real-time.\u0026rdquo;\nTo solve this, I adopted a strategy of injecting custom JavaScript.\nLayer Z-Index Management When overlaying multiple layers on a map, occlusion is the enemy. We strictly controlled layer hierarchy by creating custom Panes and setting z-index:\nLSOA Base Layer (z=400): Bottom layer, displaying the vulnerability heatmap. Housing Project Layer (z=620): Middle layer, displaying planned new homes (bubble chart). Brownfield Layer (z=650): Top layer, displaying available abandoned land points. import folium m = folium.Map(location=[centre_lat, centre_lon], zoom_start=11, tiles=\u0026#34;CartoDB positron\u0026#34;) # Create custom panes folium.map.CustomPane(\u0026#34;poly_pane\u0026#34;, z_index=400).add_to(m) folium.map.CustomPane(\u0026#34;housing_pane\u0026#34;, z_index=620).add_to(m) folium.map.CustomPane(\u0026#34;brownfield_pane\u0026#34;, z_index=650).add_to(m) Injecting JavaScript for Dynamic Weighting This was the core innovation of the project. We didn\u0026rsquo;t want to show a rigid map; we wanted to let users (like police chiefs, urban planners, or social workers) adjust weights based on their priorities.\nWe wrote the processed GeoJSON data and frontend logic directly into the HTML.\nfrom branca.element import Element # Convert Python processed data to JSON string for frontend injection geojson_str = master_gdf.to_json() custom_ui = f\u0026#34;\u0026#34;\u0026#34; \u0026lt;div id=\u0026#34;controls\u0026#34;\u0026gt; \u0026lt;h4\u0026gt;Layer Weights (LSOA Scoring)\u0026lt;/h4\u0026gt; \u0026lt;!-- Sliders --\u0026gt; \u0026lt;div\u0026gt;\u0026lt;label\u0026gt;Fuel Poverty\u0026lt;/label\u0026gt;\u0026lt;input type=\u0026#34;range\u0026#34; id=\u0026#34;w_fuel\u0026#34; oninput=\u0026#34;updateWeights()\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;\u0026lt;label\u0026gt;Crime Density\u0026lt;/label\u0026gt;\u0026lt;input type=\u0026#34;range\u0026#34; id=\u0026#34;w_crime\u0026#34; oninput=\u0026#34;updateWeights()\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;!-- ... other sliders ... --\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script\u0026gt; var lsoaData = {geojson_str}; // Inject data // Function to calculate score dynamically function updateWeights() {{ // Get slider values var w_fuel = parseFloat(document.getElementById(\u0026#39;w_fuel\u0026#39;).value); var w_crime = parseFloat(document.getElementById(\u0026#39;w_crime\u0026#39;).value); // Iterate through features and recalculate color geojsonLayer.eachLayer(function(layer) {{ var p = layer.feature.properties; // Weighted average formula var score = (p.norm_fuel * w_fuel + p.norm_crime * w_crime) / (w_fuel + w_crime); // Set style dynamically layer.setStyle({{ fillColor: getColor(score) }}); }}); }} \u0026lt;/script\u0026gt; \u0026#34;\u0026#34;\u0026#34; m.get_root().html.add_child(Element(custom_ui)) This code implements a fully frontend-based interaction logic: once the data is processed by the Python backend, the browser handles real-time rendering and mathematical calculation. This ensures fluidity; when users drag sliders, the map colors transition smoothly, visually revealing high-risk areas under different priorities.\nVisualization Details for Housing \u0026amp; Brownfields For housing projects, we used CircleMarker.\nSize: Represents the number of planned homes (using square root scaling to keep bubble sizes sane). Fill Color: Represents development stage (Short, Medium, Long term). Border Color: Represents site area size. For Brownfields, we displayed them as small black dots on the topmost layer. Clicking them reveals details about planning permission status and prior use. This design allows planners to see at a glance: In this high-risk (red) area, are there available brownfield sites (black dots) that could be used to build new homes, thereby catalyzing urban regeneration?\n","permalink":"https://blog.tategotoazarasi.me/en/posts/building-a-data-driven-city-python-gis-visualization/","summary":"This post details how to build an interactive urban planning visualization system using Python, Geopandas, and Folium by integrating housing pipelines, crime data, and socioeconomic metrics.","title":"Building a Data-Driven City: Integrating Housing, Safety, and Deprivation Metrics with Python"},{"content":"As a student, effective time management is paramount. I\u0026rsquo;ve always wanted to integrate my university timetable into the personal calendar application I use daily, allowing me to view all my commitments in one unified place. However, the official university timetable system offers no \u0026ldquo;Export to Calendar\u0026rdquo; feature or any form of .ics subscription link. To complicate matters further, accessing the timetable webpage requires navigating a complex multi-factor authentication (MFA) process.\nInitially, this seemed like a daunting automation challenge. Any script attempting to simulate a login would be made incredibly complex and fragile by the MFA requirement. However, I soon had a crucial realization: once the timetable is set for a semester, it almost never changes. This meant I probably didn\u0026rsquo;t need a dynamic, real-time synchronization solution. A one-time, manual import process would be perfectly sufficient.\nThis insight completely reframed my approach to the problem. The challenge shifted from \u0026ldquo;how to automate a complex login flow\u0026rdquo; to \u0026ldquo;how to, after one successful login, retrieve the entire semester\u0026rsquo;s data and convert it into a standard calendar format.\u0026rdquo; This blog post will document my entire process for solving this problem, from investigating network requests and analyzing the API to finally generating an .ics file using a Python script.\nDive into Browser Developer Tools My first step was to understand how the timetable data is loaded onto the webpage. Most modern web applications are built with a front-end/back-end separation, where the front-end page is merely a shell. The necessary data is fetched from a back-end server via asynchronous API calls (usually AJAX) and then rendered dynamically. If this were the case, I could potentially find this API directly, bypassing the need for complex page scraping.\nI opened the university\u0026rsquo;s timetable page and pressed F12 (or Option + Command + I on macOS) to launch the browser\u0026rsquo;s developer tools. This is a powerful suite of tools that allows for the inspection and debugging of nearly every aspect of a webpage. I focused my attention on the \u0026ldquo;Network\u0026rdquo; tab, which records all communication between the browser and the server.\nTo trigger a data-loading event, I needed to perform an action on the page. The timetable defaults to showing the current week\u0026rsquo;s schedule. I clicked the \u0026ldquo;Next Week\u0026rdquo; button to advance to the following week\u0026rsquo;s timetable. As expected, a new network request immediately appeared in the Network panel.\nAfter filtering through the various requests, I zeroed in on one named get-events. Its URL looked something like this:\nhttps://timetable.university.ac.uk/services/get-events?start=2025-11-10T00%3A00%3A00\u0026amp;end=2025-11-15T00%3A00%3A00\u0026amp;_=1762726887267\nThis discovery was exhilarating. The request\u0026rsquo;s name (get-events) and its parameters were highly suggestive. I immediately began to dissect every detail of this request.\nAnalyzing the Request Method and URL Structure The request used the GET method, which is the standard HTTP method for a client to request data from a server. The URL structure was also quite intuitive:\nHostname: timetable.university.ac.uk, the domain of the timetable service. Path: /services/get-events, clearly indicating that this endpoint\u0026rsquo;s function is to retrieve events. Query Parameters: The part of the URL after the ?, which held the keys to unlocking the data. Deconstructing the Query Parameters I examined the query parameters closely. They were formatted as key=value pairs, separated by \u0026amp;.\nstart=2025-11-10T00%3A00%3A00: The value for the start parameter was URL-encoded. After decoding, it became 2025-11-10T00:00:00. This was clearly the start time of the first day (Monday) of the week I had requested. The T separates the date and time, which conforms to the ISO 8601 date-time standard.\nend=2025-11-15T00%3A00%3A00: Similarly, the end parameter decoded to 2025-11-15T00:00:00. Interestingly, this appeared to be the start of Saturday for that week, not the end of Friday. This suggested that the API\u0026rsquo;s time range query likely uses a left-closed, right-open interval [start, end), meaning it includes the start timestamp but excludes the end timestamp. This is a very common practice in programming for handling time ranges, as it avoids many off-by-one errors and boundary condition issues.\n_=1762726887267: This parameter looked like a Unix timestamp (in milliseconds). This is a common technique known as a \u0026ldquo;cache buster.\u0026rdquo; Browsers and proxy servers may cache the results of GET requests. By appending a unique, time-based parameter to each request, it ensures that every request is treated as new and distinct, forcing the server to return fresh data instead of a cached version. For my use case, this parameter probably wasn\u0026rsquo;t strictly necessary, but to faithfully replay the request, it was best to keep it.\nThe Importance of Request Headers Beyond the URL, the HTTP request headers also contained crucial information. I paid special attention to the following headers:\nCookie: This long, seemingly random string of characters was the cornerstone that made the entire operation possible. When I logged in through the university\u0026rsquo;s complex MFA process, the server generated a session and stored a unique session ID in my browser\u0026rsquo;s Cookie. Subsequently, the browser automatically includes this Cookie in every request sent to that domain. The server validates this Cookie to confirm my identity and returns the timetable data that belongs only to me. This meant that as long as I could get my hands on this valid Cookie, I could simulate my logged-in state in any tool capable of sending HTTP requests (like curl or a Python script) without needing to perform MFA again.\nX-Requested-With: XMLHttpRequest: This header is a de facto standard used to identify an AJAX request. Some server-side frameworks check for the presence of this header to determine the request\u0026rsquo;s origin and may alter their response behavior accordingly (e.g., an AJAX request might return JSON, while a normal page request returns full HTML). It\u0026rsquo;s good practice to include this header when simulating such requests.\nAccept: application/json, text/javascript, */*; q=0.01: This header informs the server about the response formats the client is willing to accept. Here, we are explicitly stating a preference for data in application/json format.\nAnalyzing the Response The server responded to this GET request with a 200 OK status code, indicating success. The response body was exactly what I had hoped for: JSON data. It was an array, and each object within the array represented a single class event.\nI inspected the structure of one of these event objects and found all the information I would need:\n{ \u0026#34;start\u0026#34;: \u0026#34;2025-11-10T10:00\u0026#34;, \u0026#34;end\u0026#34;: \u0026#34;2025-11-10T11:00\u0026#34;, \u0026#34;activitydesc\u0026#34;: \u0026#34;COURSE101 - Introduction to Programming [ON CAMPUS LECTURE]\u0026#34;, \u0026#34;locationdesc\u0026#34;: \u0026#34;Science Building, Lecture Theatre A\u0026#34;, \u0026#34;uniqueid\u0026#34;: \u0026#34;caf4ff456c0dba721bafcfe27f526214\u0026#34;, \u0026#34;activityname\u0026#34;: \u0026#34;COURSE101/LEC/A/02\u0026#34;, \u0026#34;staffs\u0026#34;: [ { \u0026#34;FullName\u0026#34;: \u0026#34;Dr. A. Smith\u0026#34; } ] } start and end: The precise start and end times of the event. activitydesc: The title of the event, perfect for the calendar item\u0026rsquo;s summary. locationdesc: The location of the class. uniqueid: A unique identifier that looks like a hash value. This is critical for calendar events, as it can serve as the event\u0026rsquo;s Unique ID (UID), facilitating future updates or deletions and preventing duplicate imports. activityname: The internal code for the course, which could be included as supplementary information. staffs: An array containing instructor information, which could be extracted to enrich the event\u0026rsquo;s description. At this point, the initial investigation was complete. I had a full understanding of how the data was being fetched and had identified all the necessary components: an API endpoint, two key query parameters (start and end), and a Cookie containing my authentication token.\nFetching Data for the Entire Semester My hypothesis was simple: if the start and end parameters control the time range of the request, I just needed to expand this range to cover the entire semester to get all the course data in one go.\nFirst, I needed to determine the start and end dates of the academic semester. This information is typically available in the university\u0026rsquo;s Academic Calendar. I found the relevant dates for the current semester—for instance, from September 22, 2025, to December 12, 2025.\nNext, I needed a way to replay the network request I had captured, but with modified parameters. Many tools, such as Postman or Insomnia, are excellent for this. However, for a simple one-off task, I chose to use a feature built directly into the browser\u0026rsquo;s developer tools: \u0026ldquo;Copy as cURL.\u0026rdquo;\nI right-clicked on the get-events request in the Network panel and selected \u0026ldquo;Copy\u0026rdquo; -\u0026gt; \u0026ldquo;Copy as cURL (bash).\u0026rdquo; This action copied the entire HTTP request—including the method, URL, all headers, and the crucial Cookie—to my clipboard as a command-line-runnable curl command.\nPasting it into a text editor, I had a long command that looked something like this:\ncurl \u0026#39;https://timetable.university.ac.uk/services/get-events?start=...\u0026#39; \\ -H \u0026#39;User-Agent: ...\u0026#39; \\ -H \u0026#39;Accept: application/json, ...\u0026#39; \\ -H \u0026#39;Accept-Language: ...\u0026#39; \\ --compressed \\ -H \u0026#39;X-Requested-With: XMLHttpRequest\u0026#39; \\ -H \u0026#39;Connection: keep-alive\u0026#39; \\ -H \u0026#39;Referer: https://timetable.university.ac.uk/\u0026#39; \\ -H \u0026#39;Cookie: session_auth_cookie=...\u0026#39; \\ -H \u0026#39;Sec-Fetch-Dest: empty\u0026#39; \\ -H \u0026#39;Sec-Fetch-Mode: cors\u0026#39; \\ -H \u0026#39;Sec-Fetch-Site: same-origin\u0026#39; Now, I only needed to make two small modifications to this command:\nChange the value of the start parameter to the semester\u0026rsquo;s start date, e.g., 2025-09-22T00:00:00. Change the value of the end parameter to the semester\u0026rsquo;s end date, e.g., 2025-12-13T00:00:00 (I set it to the day after the last day of classes to ensure all events on December 12th were included). The modified URL portion looked like this:\n'https://timetable.university.ac.uk/services/get-events?start=2025-09-22T00%3A00%3A00\u0026amp;end=2025-12-13T00%3A00%3A00\u0026amp;_=...\nThen, I appended \u0026gt; timetable.json to the end of the command. This would redirect the output of the curl command (the JSON data returned by the server) and save it directly into a file named timetable.json.\nI pasted the complete, modified command into my terminal and pressed Enter. After a few seconds, the command completed without any errors. I checked my current directory, and there it was: a file named timetable.json. Opening it revealed a massive array of JSON data, containing every single course event for the entire semester.\nThis was the breakthrough moment of the project. I had successfully extracted the data I needed, in its entirety, from a protected web system that offered no export functionality, and saved it as a structured local file. Now, the only remaining task was to convert this data into a format that my calendar application could understand.\nUnderstanding the iCalendar (.ics) Format My goal was to generate an .ics file, the standard file extension for the iCalendar format. iCalendar is a widely supported open standard (RFC 5545), and virtually all calendar applications—whether it\u0026rsquo;s Google Calendar, Apple Calendar, or Outlook—can recognize and import it.\nBefore I could start writing code, I had to gain a solid understanding of the syntax and structure of an .ics file. It\u0026rsquo;s essentially a plain text file with content that follows a specific key-value pair format.\nThe basic structure of an .ics file is as follows:\nBEGIN:VCALENDAR VERSION:2.0 PRODID:-//My App//EN ... BEGIN:VEVENT ... END:VEVENT BEGIN:VEVENT ... END:VEVENT ... END:VCALENDAR The entire file is wrapped in BEGIN:VCALENDAR and END:VCALENDAR, defining a calendar object. VERSION:2.0 is a required property that specifies the iCalendar version. PRODID is a product identifier, indicating which software generated the file. I could set this to a custom value. A calendar can contain one or more events, each wrapped in BEGIN:VEVENT and END:VEVENT blocks. The Role of Time Zones Handling time is one of the most complex and error-prone aspects of calendar formats. My timetable is based on the local time in the UK, which observes both British Summer Time (BST) and Greenwich Mean Time (GMT). To ensure that the event times are correct after importing them into a calendar, I must explicitly define the time zone information within the .ics file.\nThe iCalendar specification allows for the definition of a complete time zone rule set using a VTIMEZONE component. This includes the standard time offset, the daylight saving time offset, and the rules for when the transitions occur. For the Europe/London time zone, a standard VTIMEZONE definition looks like this:\nBEGIN:VTIMEZONE TZID:Europe/London BEGIN:DAYLIGHT TZOFFSETFROM:+0000 TZOFFSETTO:+0100 TZNAME:BST DTSTART:19700329T010000 RRULE:FREQ=YEARLY;BYMONTH=3;BYDAY=-1SU END:DAYLIGHT BEGIN:STANDARD TZOFFSETFROM:+0100 TZOFFSETTO:+0000 TZNAME:GMT DTSTART:19701025T020000 RRULE:FREQ=YEARLY;BYMONTH=10;BYDAY=-1SU END:STANDARD END:VTIMEZONE TZID defines the name of the time zone. The inner DAYLIGHT and STANDARD blocks define the rules for daylight saving time and standard time, respectively. TZOFFSETFROM and TZOFFSETTO define the offset from UTC. RRULE defines the recurrence rule for when the time changes start (e.g., BYDAY=-1SU means \u0026ldquo;the last Sunday\u0026rdquo;). After defining the time zone, I can reference its TZID in each event\u0026rsquo;s timestamp to ensure time accuracy.\nDeconstructing the VEVENT Each VEVENT block contains several properties that describe a specific event. Based on the information I extracted from the JSON data, I needed to focus on these key properties:\nDTSTART and DTEND: The start and end times of the event. To bind them to the time zone I defined, the format should be DTSTART;TZID=Europe/London:YYYYMMDDTHHMMSS. Note that the date-time format here, YYYYMMDDTHHMMSS, has no separators.\nSUMMARY: The title of the event. This corresponds to activitydesc in the JSON.\nLOCATION: The location of the event. This corresponds to locationdesc.\nDESCRIPTION: A detailed description of the event. I can place additional information here, such as the course code (activityname) and instructor names (staffs). In the .ics format, newlines must be represented as \\n.\nUID: A unique identifier for the event. This is crucial for calendar applications to distinguish between different events, handle updates, and prevent duplicate imports. I will use the uniqueid field from the JSON directly, appending a custom domain to ensure it is globally unique, like uniqueid@my-university-timetable.\nDTSTAMP: A timestamp indicating when this event object was created or last modified, formatted as a UTC time.\nWith a clear understanding of the .ics format, I was now ready to write a Python script to \u0026ldquo;translate\u0026rdquo; the data from timetable.json into this format.\nThe Python Script Implementation I chose Python for this conversion task due to its excellent capabilities for handling JSON and text files, along with its powerful standard library. The script\u0026rsquo;s logic is very straightforward: read the JSON file, iterate over each event, format and concatenate strings according to the .ics specification, and finally, write the result to a new file.\nDesigning the Core Function I encapsulated all the logic within a single function, create_ics_file, which takes the JSON data and an output filename as its arguments.\nThe first part of the function constructs the static header of the .ics file, including BEGIN:VCALENDAR, PRODID, VERSION, and the VTIMEZONE block I prepared earlier. I stored these lines in a list of strings, which would make it easy to join them together later.\ndef create_ics_file(json_data, output_filename): ics_content = [ \u0026#34;BEGIN:VCALENDAR\u0026#34;, \u0026#34;PRODID:-//[Your Name]//Timetable//EN\u0026#34;, \u0026#34;VERSION:2.0\u0026#34;, # ... other header info ... \u0026#34;X-WR-TIMEZONE:Europe/London\u0026#34;, # ... VTIMEZONE block ... \u0026#34;END:VTIMEZONE\u0026#34; ] Iteration and Data Extraction Next came the core of the script: a loop to iterate through the list of events loaded from timetable.json. Inside the loop, I needed to safely extract the required data from each event dictionary.\nUsing the dictionary\u0026rsquo;s .get() method is a best practice here. Unlike direct access with [], .get() returns a default value (which is None by default) if a key doesn\u0026rsquo;t exist, rather than raising a KeyError. This makes the code more robust.\nfor event_data in json_data: summary = event_data.get(\u0026#39;activitydesc\u0026#39;, \u0026#39;No Title\u0026#39;) start_time_str = event_data.get(\u0026#39;start\u0026#39;) end_time_str = event_data.get(\u0026#39;end\u0026#39;) location = event_data.get(\u0026#39;locationdesc\u0026#39;, \u0026#39;\u0026#39;) unique_id = event_data.get(\u0026#39;uniqueid\u0026#39;, \u0026#39;\u0026#39;) Handling Instructor Info and Description The instructor information was nested within the staffs list. I needed to iterate through this list, extract the FullName, and join them into a single string. A list comprehension is an elegant way to accomplish this.\nstaff_list = [ staff[\u0026#39;FullName\u0026#39;] for staff in event_data.get(\u0026#39;staffs\u0026#39;, []) if staff.get(\u0026#39;FullName\u0026#39;) ] I then combined the course code and the list of instructors into a multi-line description string. The iCalendar spec requires a literal \\n for newlines, so in my Python string, I needed to use \\\\n to escape the backslash.\ndescription_parts = [] if activity_name: description_parts.append(activity_name) if staff_list: description_parts.append(\u0026#34;Staff: \u0026#34; + \u0026#34;, \u0026#34;.join(staff_list)) description = \u0026#34;\\\\n\u0026#34;.join(description_parts) Formatting Dates and Times This was the step that required the most attention to detail. The time format in the JSON was ISO 8601 (YYYY-MM-DDTHH:MM), while the .ics file required the YYYYMMDDTHHMMSS format. Python\u0026rsquo;s datetime module is the perfect tool for this job.\nFirst, I used datetime.fromisoformat() to parse the string into a datetime object. Then, I used the .strftime() method to format this object into the target string.\nstart_dt = datetime.fromisoformat(start_time_str).strftime(\u0026#39;%Y%m%dT%H%M%S\u0026#39;) end_dt = datetime.fromisoformat(end_time_str).strftime(\u0026#39;%Y%m%dT%H%M%S\u0026#39;) Assembling the VEVENT Block With all the information extracted and correctly formatted, I could now assemble it into a complete VEVENT block as a series of strings and append them to my ics_content list. F-strings are a modern and efficient way to format strings in Python.\nics_content.extend([ \u0026#34;BEGIN:VEVENT\u0026#34;, f\u0026#34;DTSTART;TZID=Europe/London:{start_dt}\u0026#34;, f\u0026#34;DTEND;TZID=Europe/London:{end_dt}\u0026#34;, f\u0026#34;DTSTAMP:{dtstamp}\u0026#34;, f\u0026#34;UID:{unique_id}@university-timetable\u0026#34;, f\u0026#34;SUMMARY:{summary}\u0026#34;, f\u0026#34;DESCRIPTION:{description}\u0026#34;, f\u0026#34;LOCATION:{location}\u0026#34;, \u0026#34;END:VEVENT\u0026#34; ]) The Final Result After the loop finished, the ics_content list contained the complete calendar data. I added the final END:VCALENDAR marker, then used \u0026quot;\\n\u0026quot;.join() to concatenate all lines in the list into a single string, which I then wrote to the specified output file.\nUsing a with open(...) statement for file I/O is a good practice, as it ensures the file is properly closed, even if errors occur during the write process. Additionally, explicitly specifying encoding='utf-8' prevents potential encoding issues when dealing with non-ASCII characters (such as in some instructor or location names).\nics_content.append(\u0026#34;END:VCALENDAR\u0026#34;) with open(output_filename, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: f.write(\u0026#34;\\n\u0026#34;.join(ics_content)) Finally, I added an if __name__ == \u0026quot;__main__\u0026quot;: block, which is the standard entry point for a Python script. This allows the file to be both imported as a module in other scripts and run directly from the command line as the main program. Within this block, I included the logic for reading the timetable.json file and added error handling for cases like the file not being found or containing invalid JSON.\nI placed the timetable.json file and my completed Python script in the same directory. Then, I ran python your_script_name.py in my terminal. The script executed in an instant, and a timetable.ics file was generated.\nThe final step was to verify the result. I opened my calendar application (Google Calendar), selected the \u0026ldquo;Import\u0026rdquo; function, and uploaded the timetable.ics file I had just created.\nIt worked perfectly. My entire semester\u0026rsquo;s timetable, with the exact time, location, course name, and instructor for every class, appeared neatly on my calendar. I could now customize colors, set reminders, and manage my academic schedule just like any other event.\n","permalink":"https://blog.tategotoazarasi.me/en/posts/reverse-engineering-a-university-timetable-api-to-generate-an-ics-file-with-python/","summary":"Learn how to reverse-engineer a university\u0026rsquo;s private timetable API using browser developer tools and write a Python script to convert the JSON data into a universally importable iCalendar (.ics) file for your personal calendar.","title":"Importing My University Timetable into a Personal Calendar: A Hands-On Journey Through Manual API Exploration and iCalendar Generation"},{"content":"In modern web development, we often encounter tasks that seem simple on the surface but are fraught with hidden challenges. I recently faced such a task: downloading a video from a specific website. This was no simple static page; it heavily utilized JavaScript for dynamic content loading and was fortified with rather sophisticated anti-scraping and content protection mechanisms. This blog post will chronicle the entire journey of how I built a robust download tool from scratch using Node.js, TypeScript, and Playwright, step-by-step, to overcome these challenges.\nThe Initial Idea and Technology Selection The mission began with a clear objective: given a webpage URL, I needed to download the main video on that page to my local machine. My first instinct was to inspect the page\u0026rsquo;s source code, hoping to find a direct link to an .mp4 file. However, reality quickly set in. Upon opening the browser\u0026rsquo;s developer tools, I discovered that the initial HTML document contained no direct URLs to any video files. The video player, its source information, and everything related to it were dynamically generated and injected into the page only after a series of complex JavaScript scripts had finished executing.\nThis immediately ruled out using traditional HTTP clients like curl or wget, or even simple Node.js libraries like axios or node-fetch. These tools can only retrieve the initial HTML skeleton, which is devoid of any video information. They cannot emulate a real user environment, execute JavaScript, and therefore, can never \u0026ldquo;see\u0026rdquo; the dynamically generated video player.\nI knew at once that the key to solving this problem was to simulate a complete browser environment. I needed a tool that could not only load a webpage but also possess a JavaScript engine to execute all the scripts on the page, render the DOM, and respond to user interactions, just like Chrome or Firefox.\nMy technology selection process quickly honed in on a few key components:\nNode.js: As the backend runtime, its powerful file system capabilities and rich ecosystem make it the ideal platform for building this kind of automation tool.\nTypeScript: I chose TypeScript over native JavaScript primarily for the long-term maintainability and robustness of the project. TypeScript\u0026rsquo;s static type checking can catch a vast number of potential errors during the development phase, such as typos and type mismatches. For a project dealing with complex browser APIs and network data streams, type safety dramatically improves development efficiency and code quality. This was not just a personal preference, but a sound engineering decision.\nPlaywright: In the realm of browser automation, two main contenders stand out: Puppeteer and Playwright. While both are excellent, I ultimately opted for Playwright. As a more modern tool from Microsoft, it offers superior cross-browser support (Chromium, Firefox, WebKit), a cleaner and more powerful API, and provides incredibly flexible and reliable interfaces for handling waits, interactions, and network interception. I believed that for a site with potentially complex anti-scraping measures, Playwright\u0026rsquo;s advanced network control capabilities would be crucial.\nWith the tech stack decided, I began setting up the project. This process itself is a showcase of an engineer\u0026rsquo;s foundational skills. I created a new project directory, initialized a package.json with npm init -y, and installed the core dependency playwright along with development dependencies like typescript, ts-node, and @types/node. Next, I generated a tsconfig.json file using npx tsc --init.\nI meticulously configured the tsconfig.json file to align with modern Node.js best practices. For instance, I set the target to ES2020 to leverage modern syntax like async/await, configured the module to CommonJS (our initial choice on this journey, which would later evolve), and clearly defined rootDir and outDir to separate source code from compiled output. These seemingly minor configurations are the first step toward a well-structured and professional project.\nHeadless Browser and a Simulated Click With the project skeleton in place, I started writing the core logic. The core idea of my first version was straightforward: \u0026ldquo;emulate the user, listen to the network.\u0026rdquo;\nI began by designing a VideoDownloader class to encapsulate all related operations. This is a good engineering practice that leads to a cleaner, more extensible, and maintainable code structure.\nMy first step was to implement the most basic user behavior simulation. I wrote several core private methods: initialize to launch Playwright and create a new browser page instance; navigateToPage to navigate to the user-provided target URL; and clickPlayButton to find and click the play button on the page.\nThe key assumption here was that the real video URL would only begin to load after the user clicks the play button. Therefore, my primary task was to locate that button. By inspecting the page\u0026rsquo;s DOM, I identified the CSS selector for the play button, which was a button element with specific class names.\nBut simply clicking the button wasn\u0026rsquo;t enough. The click would trigger a network request for the video file, and I needed to capture it. Playwright\u0026rsquo;s powerful network listening capabilities came into play here. In the initialize method, immediately after creating the page instance, I registered a network response listener:\n// Core Snippet - Registering the response listener this.page.on(\u0026#39;response\u0026#39;, this.handleResponse.bind(this)); This listener would capture every single network response generated by the page. The handleResponse method was where I implemented the core filtering and data processing logic.\nInside this method, I had to accurately identify the video segments from potentially hundreds of network requests. By observing the network panel in the browser\u0026rsquo;s developer tools, I discovered several key characteristics of the video requests:\nFirst, the request URL contained a specific file extension, such as .mp4. Second, the video was typically loaded in chunks to support seeking and save bandwidth. This chunked loading is implemented in the HTTP protocol via the Range request header and the status code 206 Partial Content. The server\u0026rsquo;s response headers would include a Content-Range field, which explicitly states the segment\u0026rsquo;s position within the entire video file and the total file size.\nBased on these observations, I established my filtering logic: I would only process responses whose URLs contained a specific keyword (like .mp4) and had an HTTP status code of 206.\nThe next challenge was how to handle these captured video chunks, which could arrive out of order. Simply pushing them into an array would likely result in a corrupted final file. The correct approach was to store them based on their position in the complete video. The Content-Range header provided this crucial piece of information, for example, bytes 0-5242879/906098800, indicating a data block starting at byte 0.\nConsequently, I decided to use a Map data structure to store the segments, using the starting byte of the chunk as the key and the Buffer containing the binary data as the value.\n// Core Snippet - Storing chunks using a Map private videoChunks: Map\u0026lt;number, Buffer\u0026gt; = new Map(); // Inside handleResponse const rangeMatch = contentRange.match(/bytes (\\d+)-(\\d+)\\/(\\d+)/); if (rangeMatch) { const start = Number.parseInt(rangeMatch[1], 10); const buffer = await response.body(); // This was an early mistake, as we\u0026#39;ll see this.videoChunks.set(start, buffer); } The advantage of this approach is that regardless of the order in which the chunks arrive, I can place them precisely where they belong.\nThe final step was file assembly. Once all chunks were downloaded, I needed to write them to a local file in the correct order. For efficiency and memory conservation, I opted against concatenating all chunks into one giant Buffer in memory. Instead, I leveraged Node.js\u0026rsquo;s Stream API. I created an fs.createWriteStream, then iterated over the sorted keys of my Map (the chunk start bytes), and sequentially fetched the corresponding Buffer for each key to write to the stream. This stream-based writing approach has a very low memory footprint and can handle massive files, even tens of gigabytes, with ease.\nWith that, the logic for my first version was complete. It seemed perfect: simulate a user click, listen to the network, filter for video chunks, store them in order, and write them efficiently. I ran the script with confidence.\nTimeouts and the networkidle Trap After running the program, the terminal output stalled at \u0026ldquo;Navigating to page\u0026hellip;\u0026rdquo;, and a minute later, a bright red error message announced the failure of my first attempt: page.goto: Timeout 60000ms exceeded. waiting until \u0026quot;networkidle\u0026quot;.\nA timeout error. This is an extremely common issue in automation and web scraping. I immediately reviewed my navigateToPage method:\n// Early, incorrect code await this.page.goto(this.url, { waitUntil: \u0026#39;networkidle\u0026#39;, timeout: 60000 }); The problem was waitUntil: 'networkidle'. This option tells Playwright to consider the navigation successful only when the page has loaded and there has been no new network activity for at least 500 milliseconds.\nThis option is useful for simple, static pages that become \u0026ldquo;quiet\u0026rdquo; after loading. However, for the modern, dynamic video site I was dealing with, this was an almost impossible condition to meet. The frontend applications of such sites are highly complex and engage in continuous background network activity:\nSending user behavior analytics and tracking data. Periodically refreshing ad content. Maintaining connections with the server via long-polling or WebSockets for real-time updates. Pre-loading images or other media resources. In this environment, the page\u0026rsquo;s network activity almost never goes \u0026ldquo;idle\u0026rdquo; for more than 500 milliseconds. My program waited patiently for the full 60 seconds, only to inevitably time out.\nThis failure made me realize that my waiting strategy had to be more precise. I didn\u0026rsquo;t need to wait for the entire page to fall silent; I only needed to wait until the key element I intended to interact with—the play button—was present and available.\nI quickly adjusted my strategy. First, I relaxed the page.goto waiting condition from networkidle to domcontentloaded. This option tells Playwright to proceed as soon as the core HTML document is loaded and parsed, without waiting for all images, stylesheets, and async scripts to finish. This made the navigation step fast and reliable.\nThen, I placed the real, precise waiting logic inside the clickPlayButton method. This method already contained a call to await this.page.waitForSelector(playButtonSelector, ...). This was the correct form of \u0026ldquo;waiting,\u0026rdquo; as it targeted a specific goal rather than an ambiguous and unattainable network state.\nIn the process of fixing this issue, I made an additional optimization. I realized that much of the continuous background network activity was related to ad and tracking scripts. These requests not only slowed down the page load and increased the risk of networkidle timeouts but were also completely useless for my download task. So, I decided to set up a request interceptor before navigation to proactively block these unnecessary requests.\n// Core Snippet - Blocking unnecessary requests await this.page.route(\u0026#39;**/*\u0026#39;, (route) =\u0026gt; { const url = route.request().url(); if (url.includes(\u0026#39;google-analytics.com\u0026#39;) || url.includes(\u0026#39;doubleclick.net\u0026#39;)) { return route.abort(); } return route.continue(); }); This small change not only made the page load faster but also made my automation script\u0026rsquo;s environment cleaner and more predictable. It reflected an engineer\u0026rsquo;s mindset shift from just \u0026ldquo;making it work\u0026rdquo; to \u0026ldquo;making it work better.\u0026rdquo;\nA Race Condition After resolving the navigation timeout, I ran the program again. This time, the logs showed the page loaded successfully, and the play button was clicked. I could almost hear the trumpets of victory. However, a few seconds later, a new, more bizarre error appeared: Protocol error (Network.getResponseBody): No data found for resource with given identifier.\nThe program had successfully captured the video chunk\u0026rsquo;s response (response object), but it failed when attempting to retrieve its content (await response.body()). The error message was very low-level, pointing directly at the Chrome DevTools Protocol (CDP) layer.\nI spent a significant amount of time debugging and understanding this error. At first, I suspected a Playwright bug or some special encryption on the website\u0026rsquo;s part. But after repeated experiments and research, I finally identified the root cause: a classic and tricky \u0026ldquo;Race Condition.\u0026rdquo;\nMy code and the browser\u0026rsquo;s internal media processing pipeline were in a high-speed race, and I was losing.\nHere\u0026rsquo;s what was happening:\nThe browser initiates a request for a video chunk. The server returns data. Playwright\u0026rsquo;s backend detects this response via the CDP and notifies my Node.js process. The Node.js event loop places a 'response' event onto its event queue. Meanwhile, the browser, a highly optimized C++ application, doesn\u0026rsquo;t wait for my Node.js script. Its network stack receives the video data and may immediately send it via \u0026ldquo;zero-copy\u0026rdquo; or similar mechanisms directly to the GPU or media decoder for playback. For maximum performance, it clears this data from memory as soon as this operation is complete, assuming the data has been \u0026ldquo;consumed.\u0026rdquo; When the Node.js event loop finally gets around to processing my 'response' event, my handleResponse callback function begins to execute. When the code reaches await response.body(), it sends a command back to the browser via CDP: \u0026ldquo;Please give me the data for that response you just told me about.\u0026rdquo; The browser replies: \u0026ldquo;Sorry, the thing you\u0026rsquo;re asking for? I already processed and discarded it. The data is gone.\u0026rdquo; And thus, the No data found for resource error was thrown. The delay in this process might be mere milliseconds or even microseconds, but in the face of a high-performance browser kernel, that\u0026rsquo;s more than enough time for the data to vanish. I realized that any form of \u0026ldquo;post-mortem\u0026rdquo; listening—any approach that tries to get the data after the response has already happened—was inherently at risk of failing.\nTo win this race, I needed a more proactive, more invasive method. I turned once again to page.route. This time, I wouldn\u0026rsquo;t just use it to abort() requests; I would use it to completely \u0026ldquo;proxy\u0026rdquo; the video requests. My new plan was:\nIntercept an outgoing video request. Instead of letting the browser make the request, I would call route.fetch(), which instructs Playwright\u0026rsquo;s backend to perform the request on the browser\u0026rsquo;s behalf. After getting the APIResponse object back from route.fetch(), I could safely await response.body() from it, because this was my request, and the data was fully preserved. After capturing and storing the data in my videoChunks Map, I would then call route.fulfill({ response }) to pass a \u0026ldquo;forged\u0026rdquo; copy of this response back to the browser, making the page\u0026rsquo;s video player think everything was normal. This solution was, in theory, perfect. It transformed passive listening into active proxying, fundamentally solving the race condition. I refactored the code and ran it again, full of hope.\nIt worked! I successfully captured the first, and then the second, chunk of data! However, the joy was short-lived. After downloading a few chunks (about 9MB), the network requests stopped dead. The program timed out after waiting 20 seconds, leaving me with an incomplete file.\nWhy did the download chain break? Had my route.fulfill() call failed to properly \u0026ldquo;deceive\u0026rdquo; the player?\nTo find the truth, I added extremely verbose logging to my interceptor, printing the URL, method, type of every intercepted request, and the complete headers of every video response.\nThe logs quickly revealed the first stunning discovery: after the play button was clicked, the player initiated concurrent requests for multiple different video resolutions! For example, 720P.mp4, 1440P.mp4, and 2048P.mp4. It seemed to be probing to determine the best bitrate for the current network speed.\nMy code had a critical flaw: I was using a single activeVideoUrl variable to lock onto the first video stream I downloaded. When the request for 720P.mp4 arrived first, my program \u0026ldquo;decided\u0026rdquo; this was the target and ruthlessly ignored all subsequent requests for 1440P and 2048P. However, the player, after its brief probing, may have ultimately decided to continue streaming the 2048P version. Because my program didn\u0026rsquo;t correctly fulfill the 2048P requests (since it ignored them), the player\u0026rsquo;s state machine was broken, and it ceased all subsequent requests.\nI immediately corrected this logic. I stopped being \u0026ldquo;first-come, first-served\u0026rdquo; and instead maintained a list of all candidate streams. After the download finished, I would then select the \u0026ldquo;best\u0026rdquo; stream to assemble. My initial criterion for \u0026ldquo;best\u0026rdquo; was \u0026ldquo;the one with the most chunks.\u0026rdquo;\nYet, upon running it again, the problem persisted. The logs showed that every resolution stream was requested only for 2 chunks, and then they all stopped. My \u0026ldquo;select the most chunks\u0026rdquo; strategy degenerated into \u0026ldquo;select the first one\u0026rdquo; in this scenario, which was still wrong.\nIt was then that I finally grasped the true essence of the problem. It wasn\u0026rsquo;t my selection logic that was flawed; it was that route.fulfill() itself was the problem. The APIResponse object I got from route.fetch(), while containing the data, was ultimately a \u0026ldquo;clone.\u0026rdquo; When passed back to the browser via route.fulfill(), it may have lost certain low-level connection handles, timing information, or other metadata critical to the player. This \u0026ldquo;imperfect\u0026rdquo; response, while providing the initial few chunks of data, was enough to corrupt the player\u0026rsquo;s delicate internal state, causing it to refuse to request subsequent segments.\nActive Control over Passive Listening After the failures of both the page.on('response') race condition and the route.fulfill() state corruption, I was at an impasse. I seemed to be caught in a dilemma: either be too slow, or be too intrusive.\nIt was at that moment that a completely new idea took shape in my mind. All my previous attempts had revolved around a central theme: how to \u0026ldquo;steal\u0026rdquo; data from requests initiated by the browser. Whether listening or proxying, I was a \u0026ldquo;parasite\u0026rdquo; attached to the browser\u0026rsquo;s behavior.\nWhy not flip the script? Why couldn\u0026rsquo;t I be the one driving the download process?\nThis idea completely transformed my architecture. The new, final solution was born, divided into two distinct phases: Reconnaissance and Takeover.\nReconnaissance I still used page.route to intercept network requests. But this time, its purpose was incredibly simple: look, don\u0026rsquo;t touch. When a video chunk request was intercepted, I did nothing but immediately call route.continue(), giving the browser an unobstructed path to conduct its own network communications. This ensured the player\u0026rsquo;s state remained absolutely pure and continuous. Simultaneously, in the background, I quietly used page.waitForResponse() to await the completion of the very request I had just allowed to pass. When the response arrived, I parsed from it all the information I needed: the full URL (including the dynamically generated token), the total video size, and the resolution. I stored this information in my discoveredStreams Map. I set a 5-10 second reconnaissance window. During this time, the player would probe all the resolution streams as usual. My recon detector would gather information on all of them. At the end of the window, I would examine the discoveredStreams Map and select the stream with the highest resolution. With that, I had all the critical intelligence needed to download the full, high-quality video.\nTakeover With the recon mission complete, I no longer cared about the browser\u0026rsquo;s subsequent network activities. I initiated a brand new download loop, one that I was in complete control of. In this loop, I would execute the most core and elegant operation of the entire solution: making the browser work for me.\nI used the page.evaluate() function. This function can execute arbitrary JavaScript code within the context of the browser page. From my Node.js side, I calculated the range of the next chunk I wanted to download, for example, bytes=0-5242879, and then passed this range along with the high-resolution URL from the recon phase as arguments to page.evaluate.\n// Core Snippet - Initiating a fetch request inside the browser const chunkData = await this.page.evaluate(async ({ url, range }) =\u0026gt; { const response = await fetch(url, { headers: { \u0026#39;Range\u0026#39;: range } }); if (!response.ok) throw new Error(`Fetch failed: ${response.status}`); const buffer = await response.arrayBuffer(); return { data: Array.from(new Uint8Array(buffer)) }; }, { url, range }); The magic of this code lies in the fact that fetch(url, ...) is executed inside the browser. This means the request:\nAutomatically carries all the current page\u0026rsquo;s cookies and session information. Originates from the same IP address and browser fingerprint. Has completely passed any Cloudflare or other JavaScript-based human verification checks the site might have deployed. It was a perfect, legitimate request originating from a real user session. The server could not distinguish it from the player\u0026rsquo;s own requests.\nThe response from fetch is an ArrayBuffer, an object that cannot be directly passed from the browser back to Node.js. Therefore, I cleverly converted it into a plain JavaScript array of numbers using Array.from(new Uint8Array(buffer)), a data structure that can be serialized as JSON and passed across process boundaries.\nBack in my Node.js environment, I received this array of numbers, converted it back into a Node.js Buffer object with Buffer.from(chunkData.data), and then steadily wrote it to my file stream.\nI repeated this process in a for loop, sequentially requesting the entire video file chunk by chunk, according to the size I defined, until every last byte was downloaded.\nThis solution proved to be unassailable. It completely sidestepped the race condition because it was no longer passively listening. It also entirely avoided interfering with the player\u0026rsquo;s state, because it let the browser\u0026rsquo;s business be the browser\u0026rsquo;s, and the download\u0026rsquo;s be mine. I was simply borrowing the browser\u0026rsquo;s \u0026ldquo;identity\u0026rdquo; to make my own requests.\nPolishing the Product Having solved the core download challenge, I didn\u0026rsquo;t stop there. A great engineer doesn\u0026rsquo;t just solve problems; they deliver a great tool. I began to add a series of professional features to the script.\nCommand-Line Argument Support I removed the hardcoded TARGET_URL and switched to using Node.js\u0026rsquo;s process.argv to parse command-line arguments. Now, a user could simply append any video URL they wanted to download after npm start, vastly improving the tool\u0026rsquo;s flexibility. I also added parameter checking; if no URL was provided, the program would print a usage hint and exit gracefully.\nRich Progress Display A long download process without any feedback is a terrible user experience. I created a separate DownloadTracker helper class dedicated to handling download statistics. It records a timestamp at the start of the download and updates the number of downloaded bytes each time a data chunk is received. To avoid spamming the console with frequent updates, I used a simple throttling technique: the progress information would update at most once per second. With each update, it would calculate:\nAverage Download Speed (Mbps): (Total Bytes Downloaded * 8) / Seconds Elapsed / 1024 / 1024 Time Remaining (seconds): (Total Size - Bytes Downloaded) / Average Speed Estimated Time of Arrival (ETA): Current Time + Time Remaining I then formatted this information into a single, clear, continuously refreshing status line, such as: Progress: 25.13% | 216.05MB / 860.00MB | Speed: 123.45 Mbps | ETA: 19:30:45 (55s remaining) This provided excellent real-time feedback to the user. Download Retry Mechanism Networks are unstable. During the download of a large file, the failure of a single chunk due to a temporary network glitch should not terminate the entire task. I added a while loop-based retry mechanism to the downloadChunkWithRetries logic. If a fetch failed, it wouldn\u0026rsquo;t give up immediately. It would enter a catch block where I would increment a retry counter and calculate a wait time using an \u0026ldquo;exponential backoff\u0026rdquo; strategy (e.g., wait 2s on the 1st retry, 4s on the 2nd, 8s on the 3rd\u0026hellip;). After a short wait, it would attempt to download the exact same chunk again. Only after reaching the maximum number of retries (e.g., 5) would the program finally give up and abort the entire download. This mechanism dramatically enhanced the program\u0026rsquo;s robustness in unstable network environments.\nIntelligent File Naming Finally, I implemented a feature you might have seen in another one of my projects: extracting a meaningful title from the page to use as the filename. I wrote an extractVideoTitle method that would search for specific elements on the page in order of priority (first p[lang=\u0026quot;ja\u0026quot;], then h2.mt-16). If neither was found, it would fall back to using a part of the URL. I also cleaned the extracted title, removing any characters that are illegal in filenames. In the end, the downloaded file was named [Video Title]-[Resolution].mp4, making it instantly identifiable.\n","permalink":"https://blog.tategotoazarasi.me/en/posts/building-a-resilient-typescript-video-downloader-against-advanced-anti-scraping/","summary":"How to build a robust video downloader from scratch with TypeScript, Node.js, and Playwright, capable of handling complex anti-scraping mechanisms, dynamic content loading, and network race conditions.","title":"Building a TypeScript Video Downloader for Complex, Anti-Scraping Websites"},{"content":"Weak Vertices The Problem In graph theory, the structural integrity of a network can often be analyzed by identifying fundamental shapes within it, such as triangles. A triangle provides rigidity and is a common motif in many applications. This problem asks us to identify vertices that are not part of any triangle. A vertex i is defined as being part of a triangle if it has two distinct neighbors, j and k, which are also neighbors of each other. Our task is to find all vertices that do not satisfy this condition, which the problem statement refers to as \u0026ldquo;weak vertices.\u0026rdquo; The graph is given to us in the form of an adjacency matrix.\nMy Code My approach is a direct simulation of the definition. For each vertex, I iterate through all pairs of its neighbors and check if they are connected.\n// Iterate through each vertex `i` from 0 to n-1 to check if it\u0026#39;s weak. for(int i=0; i\u0026lt;n; i++) { // A flag to track if vertex `i` is part of any triangle. // We initialize it to true, assuming `i` is weak until proven otherwise. bool flag = true; // Iterate through all neighbors of `i`. // The outer loop picks the first neighbor, `j`. for(int j=0; j\u0026lt;siblings[i].size(); j++) { // The inner loop picks the second neighbor, `k`. // We start from j+1 to ensure that we consider each pair of neighbors only once. for(int k=j+1; k\u0026lt;siblings[i].size(); k++) { // Check if the two neighbors of `i`, namely `siblings[i][j]` and `siblings[i][k]`, // are connected to each other. if(graph[siblings[i][j]][siblings[i][k]]) { // If they are connected, then vertices `i`, `j`, and `k` form a triangle. // Thus, vertex `i` is not weak. We set the flag to false. flag = false; // Since we have found a triangle involving `i`, we can stop checking for this vertex. break; } } // If the flag has been set to false, we can break out of the outer neighbor loop as well. if(!flag) { break; } } // If, after checking all pairs of neighbors, the flag is still true, // it means no triangle was found involving `i`. Thus, `i` is a weak vertex. if(flag) { cout\u0026lt;\u0026lt;i\u0026lt;\u0026lt;\u0026#39; \u0026#39;; } } My Solution The problem requires us to identify all vertices in a given undirected graph that are not part of any triangle. A vertex is considered \u0026ldquo;weak\u0026rdquo; if this condition holds. The graph\u0026rsquo;s structure is provided via an $n \\times n$ adjacency matrix, where $n$ is the number of vertices.\nMy solution directly implements the definition provided. The core of the logic revolves around exhaustively checking, for each vertex, whether the condition for being part of a triangle is met. The algorithm proceeds vertex by vertex, from $0$ to $n-1$. For each vertex, which we can call $i$, we adopt the hypothesis that it is a weak vertex. We then try to disprove this hypothesis by searching for evidence of a triangle involving $i$. A triangle involving vertex $i$ would consist of $i$ and two of its neighbors, say $j$ and $k$, where $j$ and $k$ are also connected by an edge.\nTo perform this check, my algorithm first identifies all neighbors of the vertex $i$. This is done by reading the adjacency matrix during the input phase and storing the neighbors of each vertex in an adjacency list representation, which I\u0026rsquo;ve called siblings. This pre-processing step simplifies the neighbor lookup process. For a given vertex $i$, siblings[i] contains a list of all vertices $j$ such that there is an edge between $i$ and $j$.\nWith the list of neighbors for vertex $i$ at hand, the next step is to examine all possible pairs of these neighbors. If vertex $i$ has $d_i$ neighbors, there are $\\binom{d_i}{2}$ such pairs. I use a pair of nested loops to iterate through every unique pair of neighbors. Let the neighbors be indexed from $0$ to $d_i-1$ in the siblings[i] list. The outer loop selects a neighbor at index $j$, and the inner loop selects another neighbor at index $k$, where $k \u0026gt; j$. This ensures that each pair is considered exactly once and avoids redundant checks (e.g., checking both $(j, k)$ and $(k, j)$) and self-comparisons (where $j=k$).\nFor each pair of neighbors, say vertex $u$ (from siblings[i][j]) and vertex $v$ (from siblings[i][k]), the algorithm checks if there is an edge connecting $u$ and $v$. This check is efficiently performed by looking up the entry in the adjacency matrix, graph[u][v]. If this entry is true (or 1), it signifies that the edge $(u, v)$ exists. The existence of edges $(i, u)$, $(i, v)$, and $(u, v)$ confirms the presence of a triangle ${i, u, v}$.\nA boolean variable, flag, is used to maintain the status of the current vertex $i$. It is initialized to true, representing the initial assumption that $i$ is weak. As soon as the algorithm finds a single pair of neighbors of $i$ that are connected to each other, it has found a triangle involving $i$. At this point, the hypothesis that $i$ is weak is disproven. The flag is set to false, and since we only need to find one triangle to disqualify a vertex from being weak, the search for this vertex $i$ can be terminated immediately. The break statements are used to exit the inner and outer loops prematurely to improve efficiency.\nIf the nested loops complete their execution without the flag ever being set to false, it means that for every pair of neighbors of $i$, no direct edge exists between them. This exhaustively proves that there are no triangles involving vertex $i$. Therefore, the initial hypothesis holds, and vertex $i$ is indeed a weak vertex. In this case, the algorithm prints the index $i$. This entire process is repeated for all vertices in the graph, ensuring that every vertex is correctly classified.\nTo formally prove the correctness of this algorithm, let us use the language of predicate logic. Let $G=(V, E)$ be the graph, where $V={0, 1, \\dots, n-1}$ is the set of vertices and $E$ is the set of edges. Let $A$ be the adjacency matrix representation of $G$, such that $A_{uv} = 1$ if $(u,v) \\in E$, and $A_{uv} = 0$ otherwise. A vertex $i \\in V$ is defined as being part of a triangle, let\u0026rsquo;s denote this property as $T(i)$, if and only if: $T(i) \\iff \\exists j, k \\in V \\text{ such that } (j \\neq i) \\land (k \\neq i) \\land (j \\neq k) \\land (A_{ij}=1) \\land (A_{ik}=1) \\land (A_{jk}=1)$. A vertex $i$ is weak, let\u0026rsquo;s denote this as $W(i)$, if it is not part of any triangle: $W(i) \\iff \\neg T(i) \\iff \\neg (\\exists j, k \\in V : (j \\neq i) \\land (k \\neq i) \\land (j \\neq k) \\land A_{ij}=1 \\land A_{ik}=1 \\land A_{jk}=1)$. By De Morgan\u0026rsquo;s laws, this is equivalent to: $W(i) \\iff \\forall j, k \\in V, \\neg ((j \\neq i) \\land (k \\neq i) \\land (j \\neq k) \\land A_{ij}=1 \\land A_{ik}=1 \\land A_{jk}=1)$. This simplifies to: $W(i) \\iff \\forall j, k \\in V, ((A_{ij}=1) \\land (A_{ik}=1)) \\implies (A_{jk}=0 \\lor i=j \\lor i=k \\lor j=k)$. Let $N(i)$ be the set of neighbors of $i$, i.e., $N(i) = {j \\in V \\mid A_{ij}=1}$. The condition for being in a triangle can be restated as: $T(i) \\iff \\exists j, k \\in N(i) \\text{ such that } (j \\neq k) \\land (A_{jk}=1)$. Consequently, the condition for being a weak vertex is: $W(i) \\iff \\forall j, k \\in N(i), (j=k) \\lor (A_{jk}=0)$. Since we are only interested in pairs of distinct neighbors, this is equivalent to: $W(i) \\iff \\forall j, k \\in N(i) \\text{ with } j \\neq k, \\text{ it holds that } A_{jk}=0$. My algorithm\u0026rsquo;s logic is a direct implementation of this final statement. The outer for loop ensures that every vertex $i \\in V$ is considered. For each $i$, the nested loops iterate through all unique pairs of distinct neighbors ${j, k} \\subseteq N(i)$. The condition if(graph[siblings[i][j]][siblings[i][k]]) is a direct check for whether $A_{jk}=1$. If this condition is ever true, the flag is set to false, correctly concluding that $\\neg W(i)$ is true. If the loops complete without this condition ever being met, it means that for all pairs of distinct neighbors ${j, k}$, $A_{jk}=0$, which is the definition of $W(i)$. Thus, the algorithm correctly identifies and prints precisely the set of all weak vertices. The correctness is therefore established.\nTime and Space Complexity The analysis of the algorithm\u0026rsquo;s complexity is crucial for understanding its performance, especially as the size of the graph grows.\nTime Complexity: The dominant part of the algorithm is the set of nested loops that execute for each vertex. Let\u0026rsquo;s analyze the work done. The main process is enclosed in a for loop that iterates from $i=0$ to $n-1$, where $n$ is the number of vertices. This loop runs $n$ times. Inside this loop, for each vertex $i$, we perform a check for triangle formation. This check involves iterating through pairs of neighbors of $i$. Let $d_i$ denote the degree of vertex $i$, which is the number of neighbors it has (siblings[i].size()). The two nested loops are designed to select every unique pair of these $d_i$ neighbors. The number of such pairs is given by the binomial coefficient $\\binom{d_i}{2} = \\frac{d_i(d_i-1)}{2}$. For each pair, a constant time lookup is performed in the adjacency matrix. Therefore, the total number of operations for a single vertex $i$ is proportional to $d_i^2$. Summing this over all vertices, the total time complexity $T(n)$ is given by: $T(n) = \\sum_{i=0}^{n-1} O(d_i^2) = O(\\sum_{i=0}^{n-1} d_i^2)$. In the worst-case scenario, the graph is a complete graph ($K_n$), where every vertex is connected to every other vertex. In this case, the degree of every vertex is $d_i = n-1$. The time complexity becomes: $T(n) = O(\\sum_{i=0}^{n-1} (n-1)^2) = O(n \\cdot (n-1)^2) = O(n^3)$. This cubic complexity indicates that the algorithm is well-suited for graphs with a small number of vertices, as specified by the problem constraints ($n \\le 20$), where $20^3 = 8000$, which is computationally very feasible.\nTo prove this formally, let $C$ be the maximum cost of a single check inside the innermost loop (a constant). The total number of operations, $T_{ops}(n)$, can be bounded by: $T_{ops}(n) \\le \\sum_{i=0}^{n-1} C \\cdot \\frac{d_i(d_i-1)}{2}$. Since $d_i \\le n-1$ for any vertex $i$, we can establish an upper bound: $T_{ops}(n) \\le \\sum_{i=0}^{n-1} C \\cdot \\frac{(n-1)(n-2)}{2} = n \\cdot C \\cdot \\frac{(n-1)(n-2)}{2}$. This expression is a polynomial of degree 3 in $n$. According to the definition of Big-O notation, there must exist constants $c \u0026gt; 0$ and $n_0$ such that for all $n \\ge n_0$, $T_{ops}(n) \\le c \\cdot n^3$. Our derived upper bound $n \\cdot C \\cdot \\frac{(n-1)(n-2)}{2}$ clearly satisfies this, so the time complexity is formally $O(n^3)$.\nSpace Complexity: The space complexity is determined by the amount of memory required to store the graph structure. My solution uses two primary data structures for this purpose:\nAn adjacency matrix graph, which is an $n \\times n$ matrix of booleans. This requires $O(n^2)$ space. An adjacency list siblings, which is a vector of vectors. In the worst case of a dense graph, the total number of entries across all lists is $2|E|$, where $|E|$ is the number of edges. For a complete graph, $|E| = \\binom{n}{2} = O(n^2)$. Thus, the adjacency list also requires $O(n^2)$ space in the worst case. Other variables, such as loop counters and the boolean flag, use only a constant amount of additional space, $O(1)$. Therefore, the total space complexity is dominated by the graph representations, making it $O(n^2)$. Formally, let $S(n)$ be the total space required. $S(n) = \\text{space}(\\text{graph}) + \\text{space}(\\text{siblings}) + \\text{space}(\\text{other})$. $\\text{space}(\\text{graph}) = n \\times n \\times \\text{sizeof(bool)} = \\Theta(n^2)$. $\\text{space}(\\text{siblings}) = \\sum_{i=0}^{n-1} d_i \\times \\text{sizeof(int)} = 2|E| \\times \\text{sizeof(int)} = O(|E|)$. Since $|E| \\le n^2$, we have $\\text{space}(\\text{siblings}) = O(n^2)$. $\\text{space}(\\text{other}) = O(1)$. So, $S(n) = \\Theta(n^2) + O(n^2) + O(1) = O(n^2)$. This concludes the formal complexity analysis.\nWhere\u0026rsquo;s my Internet? The Problem This problem presents a scenario of houses in a new town that need to be connected to the internet. We are given $N$ houses, numbered 1 to $N$, and $M$ existing network cable connections between pairs of houses. House 1 is the source of the internet connection for the entire town. A house is considered connected to the internet if it is house 1, or if it is connected by a cable to another house that is already connected. The task is to identify and list all the houses that are not yet connected to the internet. If all houses are connected, we should report that.\nMy Code This problem is about connectivity in a graph. I recognized it as a perfect use case for a Disjoint Set Union (DSU), also known as a Union-Find data structure. I model the houses as nodes in a graph and the cables as edges. All houses that can reach each other form a connected component, which the DSU can track efficiently.\n/// \\brief Disjoint Set Union (Union-Find) class class UnionFind { private: vector\u0026lt;int\u0026gt; parent; // parent[i] stores the parent of element i vector\u0026lt;int\u0026gt; rank; // Used for union by rank optimization public: // Constructor initializes n elements, each in its own set. explicit UnionFind(int n) { parent.resize(n); rank.resize(n, 0); // Initially, each element is its own parent. for(int i = 0; i \u0026lt; n; i++) { parent[i] = i; } } // Finds the representative (root) of the set containing x, with path compression. int find(int x) { if(parent[x] != x) { parent[x] = find(parent[x]); // Path compression } return parent[x]; } // Unites the sets containing x and y, using union by rank. void unite(int x, int y) { int rootX = find(x); int rootY = find(y); if(rootX != rootY) { // Only unite if they are in different sets if(rank[rootX] \u0026lt; rank[rootY]) { parent[rootX] = rootY; } else { parent[rootY] = rootX; if(rank[rootX] == rank[rootY]) { rank[rootX]++; } } } } // Checks if x and y are in the same set. bool same(int x, int y) { return find(x) == find(y); } }; // Main logic int N,M; cin \u0026gt;\u0026gt; N \u0026gt;\u0026gt; M; // Create a DSU structure for N+1 houses (using 1-based indexing). UnionFind uf = UnionFind(N+1); while(M--) { int a, b; cin \u0026gt;\u0026gt; a \u0026gt;\u0026gt; b; // Each cable connection means we unite the sets of the two houses. uf.unite(a,b); } bool flag = false; // Flag to check if any house is unconnected. // Iterate through all houses from 2 to N. for(int i=2; i\u0026lt;=N; i++) { // A house `i` is connected to the internet if it\u0026#39;s in the same set as house 1. if(!uf.same(i,1)) { cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; endl; // If not, print its number. flag = true; } } // If the flag was never set, all houses were connected. if(!flag) { cout \u0026lt;\u0026lt; \u0026#34;Connected\u0026#34;; } My Solution The problem asks us to determine which houses, in a network of $N$ houses and $M$ cable connections, are not connected to the internet. The internet originates from house 1. Connectivity is transitive: if house A is connected to house B, and B is connected to C, then A, B, and C are all mutually connected. A house has internet if it is part of the same connected block as house 1.\nThis problem can be modeled as finding the connected components of a graph. The houses are the vertices, and the cables are the edges. All houses in the same connected component as vertex 1 will have internet access. All houses in other components will not. My chosen data structure to solve this is the Disjoint Set Union (DSU) or Union-Find data structure. This structure is specifically designed to keep track of a partition of a set of elements into a number of disjoint, non-overlapping subsets. In our case, the set of elements is the set of houses, and each subset will represent a connected component.\nInitially, I treat each of the $N$ houses as being in its own separate component. The DSU is initialized with $N+1$ elements (to allow for 1-based indexing from 1 to $N$), where each element is its own \u0026ldquo;parent\u0026rdquo; or \u0026ldquo;representative,\u0026rdquo; signifying $N$ disjoint sets.\nThen, I process the $M$ cable connections one by one. Each connection between two houses, say house a and house b, signifies that these two houses are directly connected. In the context of our connected components, this means that the component containing a and the component containing b should now be considered a single, larger component. The unite(a, b) operation in my DSU class accomplishes this. It finds the representatives of the sets containing a and b. If they are not already the same, it merges the two sets. To keep the structure efficient, I employ two standard optimizations: union by rank and path compression.\nUnion by Rank: When merging two sets, the representative of the set with a smaller \u0026ldquo;rank\u0026rdquo; (a rough measure of the tree\u0026rsquo;s depth) is attached to the representative of the set with a larger rank. This helps to keep the trees representing the sets from becoming too tall and unbalanced, which would slow down the find operation. Path Compression: During a find(x) operation, which traverses the path from element x to its root representative, we make every node on that path point directly to the root. This dramatically flattens the tree structure over time, making subsequent find operations on any of those elements (or their descendants) nearly constant time. After processing all $M$ cable connections, the DSU structure accurately reflects the connected components of the housing network. Each set in the DSU corresponds to one connected component. The final step is to determine which houses are connected to the internet. A house i has internet if and only if it is in the same connected component as house 1. The DSU provides a highly efficient way to check this: the same(i, 1) function. This function returns true if i and 1 belong to the same set (i.e., they have the same root representative) and false otherwise.\nMy code iterates through all houses from 2 to $N$. For each house i, it calls uf.same(i, 1). If this returns false, it means house i is not in the same component as house 1 and therefore lacks an internet connection. The number i is then printed. If the loop completes and no such houses are found, it means every house from 2 to $N$ is in the same component as house 1. In this case, the program prints \u0026ldquo;Connected\u0026rdquo;.\nThe formal proof of correctness for this approach rests on the DSU\u0026rsquo;s ability to correctly model equivalence relations. The \u0026ldquo;is connected to\u0026rdquo; relation in a graph is an equivalence relation:\nReflexive: Any vertex v is connected to itself (by a path of length 0). Symmetric: If u is connected to v, then v is connected to u (since edges are undirected). Transitive: If u is connected to v and v is connected to w, then u is connected to w. An equivalence relation partitions a set into disjoint equivalence classes. In graph theory, these equivalence classes are precisely the connected components. The DSU data structure is designed to maintain such a partition. Let\u0026rsquo;s prove that after processing all edges, two vertices $u, v$ are in the same set in the DSU if and only if there is a path between them in the graph. Proof by Induction on the number of unite operations (edges processed): Base Case: Before any edges are processed ($m=0$), each vertex is in its own set. This is correct, as there are no paths between distinct vertices. Inductive Hypothesis: Assume that after processing $k$ edges, the DSU correctly represents the connected components of the graph formed by these $k$ edges. Inductive Step: Consider processing the $(k+1)$-th edge, $(u, v)$. The unite(u, v) operation merges the sets containing $u$ and $v$. In the graph, this new edge creates a path between any node in $u$\u0026rsquo;s old component and any node in $v$\u0026rsquo;s old component. Therefore, these two components should merge into one, which is exactly what the unite operation does. Any two nodes that were in the same component before remain so. Any two nodes that were in different components and are not in the newly merged component also remain in different components. Thus, the DSU partition remains correct after processing the $(k+1)$-th edge. By induction, after all $M$ edges are processed, the DSU correctly partitions the vertices into the graph\u0026rsquo;s connected components. A house i has internet if and only if there is a path from i to 1. This is equivalent to i and 1 being in the same connected component. My algorithm checks this using uf.same(i, 1). Therefore, the logic is correct. Time and Space Complexity Time Complexity: The overall time complexity is determined by three parts: initialization of the DSU, processing the $M$ cable connections, and the final check for connectivity.\nInitialization: The UnionFind constructor initializes the parent and rank arrays for $N+1$ elements. This involves a single loop that runs $N+1$ times. The complexity is $O(N)$. Processing Connections: We loop $M$ times. In each iteration, we perform two find operations and one unite operation. With the crucial optimizations of path compression and union by rank (or size), the amortized time complexity of a single find or unite operation is nearly constant. It is more formally expressed using the inverse Ackermann function, $\\alpha(N)$, which is a very slowly growing function. For all practical purposes in computing, $\\alpha(N)$ is less than 5. Thus, the cost of one operation can be considered $O(\\alpha(N))$. The total time for processing all $M$ connections is therefore $O(M \\cdot \\alpha(N))$. Final Check: We loop from house 2 to $N$, which is $N-1$ iterations. In each iteration, we perform a same(i, 1) call, which involves two find operations. The total time for this part is $O(N \\cdot \\alpha(N))$. Combining these parts, the total time complexity is $O(N + M \\cdot \\alpha(N) + N \\cdot \\alpha(N))$, which simplifies to $O((N+M)\\alpha(N))$. Given the constraints ($N, M \\le 200,000$), this is highly efficient.\nTo formally state this, let\u0026rsquo;s denote the cost of a DSU operation as $T_{dsu}(N)$. With both path compression and union by rank/size, it is known that a sequence of $m$ operations on $n$ elements takes $O(m \\cdot \\alpha(n))$ time. In our case, the initialization takes $T_{init}(N) = c_1 N$ time. The main loop performs $M$ unite operations, each of which calls find twice. This is a sequence of $3M$ operations. The final loop performs $N-1$ same operations, each calling find twice, for a total of $2(N-1)$ operations. The total number of DSU operations is $m = 3M + 2(N-1)$. The total time is $T(N, M) = T_{init}(N) + T_{ops}(N, M) = c_1 N + O((3M + 2(N-1))\\alpha(N)) = O((N+M)\\alpha(N))$. This confirms the complexity.\nSpace Complexity: The memory usage is primarily determined by the storage required for the DSU data structure.\nThe parent vector stores an integer for each of the $N+1$ houses. This requires $O(N)$ space. The rank vector also stores an integer for each of the $N+1$ houses, also requiring $O(N)$ space. The other variables are of constant size. Therefore, the total space complexity of my solution is $O(N)$. Formally, $S(N) = \\text{space}(\\text{parent}) + \\text{space}(\\text{rank}) = c_1 (N+1) + c_2 (N+1) = O(N)$.\nGrid The Problem This problem challenges us to find the shortest path on a grid. We are given an $n \\times m$ grid where each cell contains a single digit, say $k$. From a cell containing the digit $k$, we can make a \u0026ldquo;move\u0026rdquo; by jumping exactly $k$ squares in one of the four cardinal directions (up, down, left, or right). We are not allowed to jump off the grid. The goal is to find the minimum number of moves required to travel from the top-left corner (cell (0, 0)) to the bottom-right corner (cell (n-1, m-1)). If the destination is unreachable, we should report -1.\nMy Code This problem is a shortest path problem on an unweighted graph. The grid cells can be thought of as nodes, and the possible jumps as edges. A Breadth-First Search (BFS) is the classic algorithm for finding the shortest path in an unweighted graph. My implementation uses a priority queue, which makes it resemble Dijkstra\u0026rsquo;s algorithm. However, since all \u0026ldquo;edge weights\u0026rdquo; are implicitly 1 (each move counts as one step), this approach functions identically to a standard BFS and correctly finds the shortest path.\n// A struct to hold the state: coordinates (x, y) and the number of steps taken. struct state { public: int x; int y; int step; }; // Main logic int n, m; cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; m; // Read the grid from input. vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; grid(n, vector\u0026lt;int\u0026gt;(m)); for(int i = 0; i \u0026lt; n; i++) { string line; cin \u0026gt;\u0026gt; line; for(int j = 0; j \u0026lt; m; j++) { grid[i][j] = line[j] - \u0026#39;0\u0026#39;; } } // A set to keep track of visited grid cells to avoid cycles and redundant computations. unordered_set\u0026lt;pair\u0026lt;int, int\u0026gt;, pair_hash, pair_equal\u0026gt; visited{}; // A priority queue to manage states to visit. While a simple queue is sufficient // for a standard BFS, a priority queue also works correctly here because all // edge weights are uniform (1). It will explore layer by layer. // My custom comparator is not strictly necessary but was added for experimentation. auto comp = [](const state \u0026amp;a, const state \u0026amp;b) { return a.step \u0026gt; b.step; }; priority_queue\u0026lt;state, vector\u0026lt;state\u0026gt;, decltype(comp)\u0026gt; pq{}; pq.push(state{0, 0, 0}); // Start at (0,0) with 0 steps. while(!pq.empty()) { auto top = pq.top(); pq.pop(); int x = top.x; int y = top.y; // If we have already processed this cell, skip it. if(visited.count({x, y})) { continue; } visited.insert(make_pair(x, y)); // Get the jump distance from the current cell. int jump_dist = grid[x][y]; // Define the four possible moves. pair\u0026lt;int, int\u0026gt; next_moves[4] = { make_pair(x + jump_dist, y), // Down make_pair(x - jump_dist, y), // Up make_pair(x, y + jump_dist), // Right make_pair(x, y - jump_dist) // Left }; for(auto [next_x, next_y]: next_moves) { // Check if the destination is reached. if(next_x == n - 1 \u0026amp;\u0026amp; next_y == m - 1) { cout \u0026lt;\u0026lt; top.step + 1; // Found the shortest path. return 0; } // Check if the move is valid (within grid bounds) and not to a zero-jump cell. if(next_x \u0026gt;= 0 \u0026amp;\u0026amp; next_x \u0026lt; n \u0026amp;\u0026amp; next_y \u0026gt;= 0 \u0026amp;\u0026amp; next_y \u0026lt; m \u0026amp;\u0026amp; grid[next_x][next_y]!=0) { // Push the new state to the queue. pq.push(state{next_x, next_y, top.step + 1}); } } } // If the queue becomes empty and the destination was not reached, it\u0026#39;s impossible. cout \u0026lt;\u0026lt; -1; My Solution This problem asks for the minimum number of moves to get from a starting cell to a target cell in a grid. This is a classic example of a shortest path problem on a graph. The graph in question can be constructed by considering each cell $(i, j)$ of the grid as a vertex. An edge exists from vertex $(i, j)$ to another vertex $(i\u0026rsquo;, j\u0026rsquo;)$ if we can legally jump from the first cell to the second in a single move. Since each move counts as 1, the graph is unweighted. The problem is then reduced to finding the shortest path from the vertex corresponding to cell $(0, 0)$ to the vertex for cell $(n-1, m-1)$.\nThe canonical algorithm for finding the shortest path in an unweighted graph is the Breadth-First Search (BFS). BFS systematically explores the graph layer by layer, starting from the source vertex. It discovers all vertices at distance 1, then all vertices at distance 2, and so on. This property guarantees that the first time BFS reaches the destination vertex, it will have done so via a shortest path.\nMy solution implements this BFS-like traversal. I use a data structure, in this case a priority queue, to store the \u0026ldquo;frontier\u0026rdquo; of the search—the set of cells that have been reached but whose neighbors have not yet been explored. A state struct is used to store the coordinates of a cell and the number of moves (step) taken to reach it. The search starts by placing the initial state, {0, 0, 0}, into the queue.\nThe main part of the algorithm is a loop that continues as long as the queue is not empty. In each iteration, it extracts a state from the queue. Let\u0026rsquo;s say we extract the state for cell $(x, y)$, reached in top.step moves. To prevent infinite loops in case of cycles and to avoid redundant computations, I use a visited set. If the cell $(x, y)$ has already been visited, we simply discard the current state and proceed to the next one. Otherwise, we mark $(x, y)$ as visited.\nNext, the algorithm determines the jump distance, $k = \\text{grid}[x][y]$, from the current cell. It then calculates the coordinates of the four potential destination cells by adding or subtracting $k$ from the current coordinates. For each potential move to a cell $(next_x, next_y)$:\nIt first checks if this new cell is the target destination, $(n-1, m-1)$. If it is, we have found a path. Since BFS explores paths in increasing order of length, this must be a shortest path. The length is the current path length (top.step) plus one more move. The program prints this number and terminates. If it\u0026rsquo;s not the destination, the algorithm verifies that the move is valid: the new coordinates must be within the grid\u0026rsquo;s boundaries ($0 \\le next_x \u0026lt; n$ and $0 \\le next_y \u0026lt; m$). I also added a check to ensure the jump value at the destination cell is not zero, which would create a dead end. If the move is valid, a new state {next_x, next_y, top.step + 1} is created and added to the queue for future exploration. If the main loop finishes (i.e., the queue becomes empty) and the destination has not been reached, it means that the destination cell is in a part of the graph that is not reachable from the starting cell. In this case, the algorithm prints -1.\nNow, let\u0026rsquo;s provide a formal proof of correctness. The algorithm is a variant of Dijkstra\u0026rsquo;s algorithm, which, on a graph with uniform positive edge weights (all equal to 1 in this case), behaves identically to BFS. Let\u0026rsquo;s prove that it finds the shortest path. Let $d(v)$ be the length of the shortest path from the source vertex $s$ (cell $(0,0)$) to any vertex $v$. We want to prove that when the algorithm terminates upon reaching the destination $t$ (cell $(n-1, m-1))$, the path length it found, step+1, is equal to $d(t)$. Let\u0026rsquo;s prove by induction that when a vertex $v$ is extracted from the priority queue, the value v.step is equal to $d(v)$.\nBase Case: The first vertex extracted is the source $s$, with s.step = 0. The shortest path from a source to itself is indeed 0. So, the base case holds. Inductive Hypothesis: Assume for all vertices $u$ extracted from the queue before a vertex $v$, it holds that u.step = $d(u)$. Inductive Step: Let $v$ be the next vertex to be extracted from the queue. Let $u$ be the vertex that caused $v$ to be added to the queue. This means there is an edge $(u, v)$ and v.step = u.step + 1. By the inductive hypothesis, u.step = d(u). Therefore, v.step = d(u) + 1. This means there is a path to $v$ of length v.step. We now need to show this is the shortest path. Consider any path from $s$ to $v$. Let this path be $s=v_0, v_1, \\dots, v_k=v$. Let $v_i$ be the first vertex on this path that has not yet been extracted from the queue. All its predecessors $v_0, \\dots, v_{i-1}$ have been extracted. Let $v_{i-1}$ be the immediate predecessor. When $v_{i-1}$ was processed, the state for $v_i$ would have been pushed to the queue with a step count of v_{i-1}.step + 1. By the inductive hypothesis, v_{i-1}.step = d(v_{i-1}) = i-1. So, $v_i$ is in the queue with a step value of $i$. Since my priority queue (and a standard BFS queue) processes nodes in non-decreasing order of their step value, $v$ could not have been extracted before $v_i$ if v.step \u0026gt; v_i.step. Since the path $s, \\dots, v_k=v$ has length $k$, we know $d(v) \\le k$. The vertex $v_i$ on this path is in the queue with step value $i$. The length of the path to $v$ is $k$. Any path to $v$ must pass through a frontier node. The algorithm always expands the node on the frontier with the smallest step count. Therefore, when it reaches $v$, it must have done so via a path of length $d(v)$. So v.step = d(v). Since the algorithm terminates as soon as it reaches the destination $t$, the step count at that point will be $d(t)$, the shortest path length. If $t$ is never reached, the queue will become empty, correctly indicating no path exists. Time and Space Complexity Time Complexity: The time complexity of this search algorithm depends on the number of vertices and edges in the graph. In our grid, the number of vertices $|V|$ is $n \\times m$. Each vertex has at most 4 outgoing edges. So, the number of edges $|E|$ is at most $4 \\times n \\times m$, which is $O(nm)$. The main loop runs as long as the priority queue is not empty. Each vertex (cell) is added to the queue at most once because of the visited set. When we process a vertex, we do a constant number of operations (checking its 4 neighbors). The operations on the priority queue (push and pop) take logarithmic time in the size of the queue. The maximum size of the queue is $|V|=nm$. So, the complexity is dominated by the queue operations for each vertex and edge. The total time complexity is $O(|V| \\log |V| + |E|) = O(nm \\log(nm) + 4nm) = O(nm \\log(nm))$. Note: If a standard queue were used for a pure BFS, the enqueue and dequeue operations would be $O(1)$, leading to a time complexity of $O(|V|+|E|) = O(nm)$. Since my solution uses a priority queue, the logarithmic factor is included in the analysis.\nLet\u0026rsquo;s formalize this. Let $N = n \\times m$. The number of vertices is $N$. The number of edges is at most $4N$. The algorithm is essentially Dijkstra\u0026rsquo;s.\nPushing the initial state: $O(\\log N)$. The while loop runs at most $N$ times (once per vertex). Inside the loop: pq.pop(): $O(\\log N)$. visited.count() and visited.insert() on an unordered set are on average $O(1)$. The inner loop runs 4 times. Inside the inner loop, pq.push(): $O(\\log N)$. Total time: $N \\times (O(\\log N) + O(1) + 4 \\times O(\\log N)) = N \\times O(\\log N) = O(nm \\log(nm))$. Space Complexity: The space required is determined by the data structures used to store the grid, the visited set, and the priority queue.\nGrid: The grid itself is an $n \\times m$ matrix, requiring $O(nm)$ space. Visited Set: In the worst case, the visited set can store a pair of coordinates for every cell in the grid. This requires $O(nm)$ space. Priority Queue: The priority queue can also, in the worst case, hold a state for every cell in the grid. This also requires $O(nm)$ space. Summing these up, the total space complexity is $O(nm) + O(nm) + O(nm) = O(nm)$. Formally, $S(n, m) = \\text{space}(\\text{grid}) + \\text{space}(\\text{visited}) + \\text{space}(\\text{pq})$. Let $N=nm$. $S(n, m) = c_1 N + c_2 N + c_3 N = O(N) = O(nm)$.\nOddities The Problem This is a fundamental programming exercise focused on the concept of parity. We are asked to determine whether a given integer is odd or even. An integer $n$ is defined as even if it is a multiple of two (i.e., can be expressed as $n=2k$ for some integer $k$), and odd otherwise. The program needs to handle multiple test cases, reading an integer and printing whether it is odd or even.\nMy Code The solution is straightforward and relies on the modulo operator (%), which gives the remainder of a division. A number is even if its remainder when divided by 2 is 0, and odd if the remainder is 1. I also handle negative numbers by taking the absolute value first.\n// The number of test cases `n` is read first. int n; cin\u0026gt;\u0026gt;n; // Loop `n` times to process each test case. while(n--) { int x; cin\u0026gt;\u0026gt;x; // To correctly handle negative numbers like -5, we first take the absolute value. // Then, we use the modulo operator (%) to find the remainder when divided by 2. // If abs(x) % 2 is 1, the number is odd. // If abs(x) % 2 is 0, the number is even. // The ternary operator `(condition ? value_if_true : value_if_false)` // is used for a compact output string. cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; \u0026#34; is \u0026#34; \u0026lt;\u0026lt; ((abs(x) % 2 == 1) ? \u0026#34;odd\u0026#34; : \u0026#34;even\u0026#34;) \u0026lt;\u0026lt; endl; } My Solution The problem is to classify a given integer $x$ as either \u0026ldquo;even\u0026rdquo; or \u0026ldquo;odd\u0026rdquo;. The mathematical definition of parity is central to the solution. An integer $z$ is called even if it can be written as $z=2k$ for some integer $k$. An integer is called odd if it can be written as $z=2k+1$ for some integer $k$. This definition is a direct consequence of the Division Algorithm from number theory, which states that for any integer $a$ and any positive integer $d$, there exist unique integers $q$ (quotient) and $r$ (remainder) such that $a = dq + r$ and $0 \\le r \u0026lt; d$.\nWhen we apply the Division Algorithm with the divisor $d=2$, we find that for any integer $x$, there exist unique integers $q$ and $r$ such that $x = 2q + r$ and $0 \\le r \u0026lt; 2$. This means the only possible values for the remainder $r$ are 0 or 1.\nIf $r=0$, then $x=2q$. By definition, $x$ is an even number. If $r=1$, then $x=2q+1$. By definition, $x$ is an odd number. This provides a clear and unambiguous method for determining the parity of any integer: we just need to find its remainder when divided by 2. The modulo operator (%) in C++ and many other programming languages is designed to compute this remainder. So, x % 2 will yield the remainder of $x$ divided by 2.\nA small subtlety arises with negative numbers. The behavior of the modulo operator with negative operands can differ across programming languages. In C++, the result of a % n has the same sign as a. For example, -5 % 2 results in -1. To ensure a consistent and simple check, my solution first takes the absolute value of the input integer x using abs(x). This maps both positive and negative integers to their non-negative counterparts. For any non-negative integer, abs(x) % 2 will be either 0 (for even numbers) or 1 (for odd numbers). This approach works because the parity of an integer $x$ is the same as the parity of its absolute value, $|x|$. If $x = 2k$, then $|x| = |2k| = 2|k|$, which is even. If $x = 2k+1$, then $|x|=|2k+1|$. If $k \\ge 0$, $|x|=2k+1$, which is odd. If $k \u0026lt; 0$, say $k\u0026rsquo; = -k \u0026gt; 0$, then $x = -2k\u0026rsquo;+1$. $|x| = |-(2k\u0026rsquo;-1)| = 2k\u0026rsquo;-1 = 2(k\u0026rsquo;-1)+1$, which is also odd. Thus, checking abs(x) % 2 is a robust method.\nMy code implements this logic concisely. It reads an integer $x$, computes abs(x) % 2, and uses a ternary operator to select the correct string (\u0026ldquo;odd\u0026rdquo; or \u0026ldquo;even\u0026rdquo;) based on whether the result is 1 or 0.\nThe formal proof of correctness is grounded in the Division Algorithm as explained above. Let $P(x)$ be the proposition \u0026ldquo;the algorithm correctly determines the parity of integer $x$\u0026rdquo;. We need to prove $\\forall x \\in \\mathbb{Z}, P(x)$. Case 1: $x$ is even. By definition, $\\exists k \\in \\mathbb{Z}$ such that $x = 2k$. Then $|x| = |2k| = 2|k|$. The expression abs(x) % 2 in the code computes $|x| \\pmod 2$. $|x| \\pmod 2 = (2|k|) \\pmod 2 = 0$. The code checks if this value is 1. It is not. The ternary operator (0 == 1 ? \u0026quot;odd\u0026quot; : \u0026quot;even\u0026quot;) evaluates to \u0026ldquo;even\u0026rdquo;. The algorithm correctly outputs that $x$ is even. Case 2: $x$ is odd. By definition, $\\exists k \\in \\mathbb{Z}$ such that $x = 2k+1$. Then $|x| = |2k+1|$. The expression abs(x) % 2 computes $|x| \\pmod 2$. We know $|x| \\pmod 2 = ((2k+1) \\pmod 2 \\text{ if } 2k+1 \\ge 0) \\lor ((-(2k+1)) \\pmod 2 \\text{ if } 2k+1 \u0026lt; 0)$. In the first subcase, $(2k+1) \\pmod 2 = 1$. In the second subcase, let $k\u0026rsquo; = -k-1$. Then $-(2k+1) = -2k-1 = -2(k\u0026rsquo;+1)-1 = 2(-k\u0026rsquo;-1)+1$. This is of the form $2q+1$, so its remainder when divided by 2 is 1. In all subcases, if $x$ is odd, $|x| \\pmod 2 = 1$. The code checks if this value is 1. It is. The ternary operator (1 == 1 ? \u0026quot;odd\u0026quot; : \u0026quot;even\u0026quot;) evaluates to \u0026ldquo;odd\u0026rdquo;. The algorithm correctly outputs that $x$ is odd. Since the algorithm is correct for both even and odd integers, it is correct for all integers.\nTime and Space Complexity Time Complexity: The algorithm processes each of the $n$ test cases independently inside a while loop. For each test case, the work done is constant. It involves:\nReading an integer (cin). Calling abs(), which is a constant-time operation. Performing a modulo operation (%), which is a constant-time operation for native integer types. Performing a comparison (==). Printing the result (cout). All these steps take a small, constant amount of time, let\u0026rsquo;s call it $C$. The total time complexity is therefore the number of test cases $n$ multiplied by this constant time, which is $O(n)$. Formally, let $T(n)$ be the total time for $n$ test cases. $T(n) = \\sum_{i=1}^{n} C_i$, where $C_i$ is the constant time taken for the $i$-th test case. So, $T(n) = n \\cdot C$. By the definition of Big-O notation, this is $O(n)$.\nSpace Complexity: The algorithm uses only a few variables to store the number of test cases (n) and the integer for the current test case (x). The amount of memory used does not depend on the magnitude of the input numbers (within the limits of the int type) or the number of test cases. Therefore, the space complexity is constant, $O(1)$.\nFormally, $S(n) = \\text{space}(n) + \\text{space}(x) = \\text{sizeof(int)} + \\text{sizeof(int)} = C\u0026rsquo;$, where $C\u0026rsquo;$ is a constant. This is $O(1)$.\nCounting Chocolate The Problem This problem is a variation of the classic Partition Problem. We are given a collection of $n$ boxes of chocolate, each containing a certain number of pieces. We need to determine if it\u0026rsquo;s possible to divide these boxes between two people, John and Sam, such that the total number of chocolate pieces each person receives is exactly the same. All boxes must be distributed.\nMy Code This problem can be solved by checking if there exists a subset of the boxes whose total number of pieces is exactly half of the grand total. If such a subset exists, we can give it to John, and the remaining boxes will automatically sum to the same amount for Sam. This is a subset sum problem. Given the small constraints, I opted for a recursive (backtracking) solution to explore all possible subsets.\n// Recursive function to solve the subset sum problem. // a: the vector of chocolate pieces in each box. // john: current sum of pieces for John. // sam: current sum of pieces for Sam. // sp: the starting point (index) in the vector `a` for the current recursive call. // target: the target sum for each person (total_sum / 2). bool solve(const vector\u0026lt;int\u0026gt;\u0026amp; a, int john, int sam, int sp, int target) { // Base case: if we have considered all boxes. if(sp == a.size()) { // If we reached the end, a solution is only found if one person hit the target exactly. // However, the checks below make this redundant. return false; } // Success condition: if either person\u0026#39;s sum has reached the target. if(john == target || sam == target) { return true; } // Pruning: if either person\u0026#39;s sum has exceeded the target, this path is invalid. if(john \u0026gt; target || sam \u0026gt; target) { return false; } // Recursive step: explore two choices for the box at index `sp`. // Choice 1: Give the current box to John. // Choice 2: Give the current box to Sam. // The `||` (OR) means we return true if either choice leads to a solution. return solve(a, john + a[sp], sam, sp + 1, target) || solve(a, john, sam + a[sp], sp + 1, target); } // Main logic int n; cin\u0026gt;\u0026gt;n; vector\u0026lt;int\u0026gt; a = vector\u0026lt;int\u0026gt;(n); int sum = 0; for(int i=0; i\u0026lt;n; i++) { cin\u0026gt;\u0026gt;a[i]; sum += a[i]; } // A crucial initial check: If the total sum of chocolates is odd, // it\u0026#39;s impossible to divide it into two equal integer halves. if(sum % 2 == 1) { cout \u0026lt;\u0026lt; \u0026#34;NO\u0026#34;; return 0; } // Call the recursive solver. We start with both sums at 0, from the first box (index 0), // with the target being half the total sum. if(solve(a, 0, 0, 0, sum / 2)) { cout \u0026lt;\u0026lt; \u0026#34;YES\u0026#34;; } else { cout \u0026lt;\u0026lt; \u0026#34;NO\u0026#34;; } My Solution The problem asks if a given set of integers (the number of chocolates in each box) can be partitioned into two subsets with equal sums. Let the set of chocolate counts be $S = {a_1, a_2, \\dots, a_n}$. We are looking for a partition of $S$ into two disjoint subsets, $S_1$ and $S_2$, such that $S_1 \\cup S_2 = S$, $S_1 \\cap S_2 = \\emptyset$, and $\\sum_{x \\in S_1} x = \\sum_{y \\in S_2} y$.\nFirst, a simple but powerful observation can be made. If the two subsets have equal sums, say $K$, then the total sum of all chocolates, $\\sum_{z \\in S} z$, must be $K+K = 2K$. This implies that the total sum must be an even number. If the total sum is odd, it\u0026rsquo;s mathematically impossible to partition it into two equal integer sums. My algorithm begins with this check: it calculates the total sum of all pieces, and if this sum is odd, it immediately prints \u0026ldquo;NO\u0026rdquo; and terminates. This is a valid and efficient pruning of the entire problem space.\nIf the total sum is even, let it be $2K$. Our goal is now to find a subset $S_1 \\subseteq S$ whose elements sum up to exactly $K$. If we can find such a subset, then the elements in the remaining subset $S_2 = S \\setminus S_1$ will have a sum of $2K - K = K$, satisfying the condition. So, the problem is transformed into the Subset Sum Problem: given a set of integers, is there a non-empty subset whose sum is equal to a given integer $K$?\nThe Subset Sum Problem is a classic NP-complete problem. This means that there is no known algorithm that can solve it in polynomial time for all inputs. However, for small inputs, we can solve it using methods like dynamic programming or, as I have chosen, recursion with backtracking. My solve function embodies this backtracking approach.\nThe function solve(a, john, sam, sp, target) explores the possibilities. The parameters represent the current state of the decision-making process: john and sam are the current sums for the two people, sp is the index of the current box we are considering, and target is the desired sum $K$.\nThe logic of the recursion is as follows: For each box a[sp], we have two choices:\nGive the box to John. Give the box to Sam. The function explores both of these choices through recursive calls. In the first call, solve(a, john + a[sp], sam, sp + 1, target), we add the current box\u0026rsquo;s value to John\u0026rsquo;s total and move on to consider the next box (sp + 1). In the second call, solve(a, john, sam + a[sp], sp + 1, target), we explore the alternative where the box is given to Sam. The || (logical OR) operator combines the results. If either of these branches of exploration eventually leads to a solution, the function returns true.\nThe recursion has several base cases and pruning conditions:\nSuccess: If at any point john == target (or sam == target), we have successfully found a subset that sums to the target. We can immediately return true. This true value will propagate up the call stack. Pruning: If john \u0026gt; target or sam \u0026gt; target, it means the current path has already overshot the target sum. Since all chocolate counts are positive, this sum will only increase. This branch cannot lead to a solution, so we can prune it by returning false. Failure: If sp == a.size(), it means we have considered every box. If we have not yet returned true, it means this particular distribution of boxes did not result in either person having a sum of exactly $K$. We return false. The initial call to the function is solve(a, 0, 0, 0, sum/2). This starts the process with empty sums for both people, considering the first box, with the target being half of the total sum.\nThe correctness of this backtracking algorithm stems from the fact that it explores the entire search space of all possible partitions. The decision for each box (a[sp]) to go to John or Sam creates a binary decision tree. The leaves of this tree represent all $2^n$ possible ways to distribute the $n$ boxes. The algorithm systematically traverses this tree. The pruning conditions (john \u0026gt; target, sam \u0026gt; target) help to cut off branches that are guaranteed not to lead to a solution, but they do not affect correctness because they only discard invalid paths. Since the algorithm explores every valid possibility, if a solution exists, it is guaranteed to be found. If the function returns false, it means the exhaustive search completed without finding any valid partition.\nLet $P(S, K)$ be the problem of finding if a subset of $S$ sums to $K$. My recursive function, let\u0026rsquo;s call it $R(i, j_s, s_s)$, where $i$ is the index of the item to consider, and $j_s, s_s$ are the current sums, computes whether a partition is possible. The correctness can be formally proven by induction on the number of items remaining, $k = n-i$.\nBase Case ($k=0$, i.e., $i=n$): No items are left. A solution exists if and only if $j_s=K$ or $s_s=K$. My function\u0026rsquo;s base cases cover this. Inductive Hypothesis: Assume $R(i+1, \\dots)$ correctly solves the problem for the sub-array starting at $i+1$. Inductive Step: To solve for $R(i, j_s, s_s)$, we consider item $a_i$. A solution exists if (a) we give $a_i$ to John and a solution exists for the rest, i.e., $R(i+1, j_s+a_i, s_s)$ is true, OR (b) we give $a_i$ to Sam and a solution exists for the rest, i.e., $R(i+1, j_s, s_s+a_i)$ is true. This is precisely what the line return solve(...) || solve(...) does. By the inductive hypothesis, the recursive calls are correct. Therefore, the logic for step $i$ is also correct. Time and Space Complexity Time Complexity: The recursive algorithm, in its pure form, explores a binary tree of decisions. For each of the $n$ boxes, there are two choices. This leads to a total of $2^n$ possible distributions (leaves in the decision tree). The algorithm, in the worst case, might have to explore all of them. At each step of the recursion, a constant amount of work is done. Therefore, the time complexity is exponential, $O(2^n)$. The pruning I included can reduce the search space in practice, but the worst-case complexity remains exponential. Given the problem constraints ($n \\le 1000$), this solution would be too slow. However, for typical competitive programming constraints where such an approach is intended to pass, $n$ is usually much smaller (e.g., $n \\le 20$). There appears to be a mismatch between the provided solution\u0026rsquo;s complexity and the problem\u0026rsquo;s stated constraints. Assuming the constraints are looser in practice or the test cases are weak, this is the complexity of the code as written. For the stated constraints, a dynamic programming approach with complexity $O(n \\cdot \\text{sum})$ would be required.\nFormally, let $T(i)$ be the time to solve the problem for the subarray from index $i$ to $n-1$. $T(i) = T(i+1) + T(i+1) + C = 2T(i+1) + C$. This recurrence relation unfolds to $T(0) = O(2^n)$.\nSpace Complexity: The space complexity is determined by the maximum depth of the recursion call stack. The recursion goes from index sp=0 to sp=n. This means the maximum depth of the call stack will be $n$. Each call on the stack stores its parameters and some local variables, which takes a constant amount of space. Therefore, the space complexity is proportional to the number of boxes, $O(n)$.\nFormally, let $S(n)$ be the space required. The recursion depth is $n$. Each stack frame requires constant space $C$. So, $S(n) = n \\cdot C = O(n)$. The vector a also takes $O(n)$ space. Total space is $O(n)$.\nMaximize Sum of Squares of Digits The Problem This LeetCode problem asks us to construct a positive integer n with a specific number of digits, num, and a specific sum of those digits, sum. Among all \u0026ldquo;good\u0026rdquo; integers that satisfy these two conditions, we need to find the one that has the maximum possible \u0026ldquo;score,\u0026rdquo; where the score is defined as the sum of the squares of its digits. If multiple numbers yield the same maximum score, we should return the largest one. If no such number exists, we return an empty string.\nMy Code To maximize the sum of squares of digits for a fixed sum, the digits should be as \u0026ldquo;uneven\u0026rdquo; as possible. For example, for a sum of 10, the digits 9 and 1 give a score of $9^2+1^2=82$, whereas 5 and 5 give $5^2+5^2=50$. This suggests a greedy approach: use as many 9s as possible. To make the resulting number as large as possible, these 9s should be placed in the most significant positions.\nstring Solution::maxSumOfSquares(int num, int sum) { // Create an output string stream for easy string building. ostringstream oss = {}; // A basic check for impossibility: the maximum possible sum for `num` digits // is `9 * num`. If the required `sum` is greater, no solution exists. if(9 * num \u0026lt; sum) { return \u0026#34;\u0026#34;; } // `cnt` tracks how many digits we have placed so far. int cnt = 0; // Greedily place as many \u0026#39;9\u0026#39;s as possible. while(sum \u0026gt; 9) { oss \u0026lt;\u0026lt; 9; // Add a \u0026#39;9\u0026#39; to our number. cnt++; // Increment the digit count. sum -= 9; // Decrease the remaining sum we need to achieve. } // After the loop, `sum` is between 0 and 9 (inclusive). // This `sum` becomes the next digit. oss \u0026lt;\u0026lt; sum; cnt++; // If we haven\u0026#39;t reached the required `num` digits yet, // the remaining digits must be \u0026#39;0\u0026#39;s. while(num - cnt \u0026gt; 0) { cnt++; oss \u0026lt;\u0026lt; 0; } return oss.str(); } My Solution The problem has two objectives layered on top of each other: first, maximize the sum of the squares of the digits, and second, among those with the maximal score, find the largest number. The constraints are a fixed number of digits (num) and a fixed sum of those digits (sum).\nLet the digits of the number be $d_1, d_2, \\dots, d_{\\text{num}}$. We are given:\n$\\sum_{i=1}^{\\text{num}} d_i = \\text{sum}$ $0 \\le d_i \\le 9$ for all $i$. We want to maximize the score $S = \\sum_{i=1}^{\\text{num}} d_i^2$.\nThe core mathematical insight here is that for a fixed sum, the sum of squares is maximized when the numbers are as far apart as possible. Consider two digits $a$ and $b$ with a sum $a+b=C$. If we change them to $a-1$ and $b+1$ (assuming $a\u0026gt;0, b\u0026lt;9$), the new sum is still $C$, but the new sum of squares is $(a-1)^2 + (b+1)^2 = a^2 - 2a + 1 + b^2 + 2b + 1 = (a^2+b^2) + 2(b-a+1)$. If $b \\ge a$, this change increases the sum of squares. This \u0026ldquo;spreading out\u0026rdquo; of values increases the sum of squares. We can generalize this: to maximize $\\sum d_i^2$ subject to $\\sum d_i = C$, we should make some $d_i$ as large as possible.\nThis insight leads to a greedy strategy. To maximize the score, we should make the digits as large as possible. The largest possible digit is 9. Therefore, our strategy should be to use as many 9s as we can. For each 9 we use, we satisfy 9 of the required sum and use up one of the num available digit slots.\nMy algorithm implements this greedy choice. It repeatedly appends the digit \u0026lsquo;9\u0026rsquo; to the result, subtracting 9 from the remaining sum each time, until the remaining sum is 9 or less. At this point, the remaining sum (which is between 0 and 9) must be used as the next digit. After placing this digit, if we still have not used up all num digit slots, the remaining slots must be filled with \u0026lsquo;0\u0026rsquo;s, as using any other digit would alter the sum.\nNow, let\u0026rsquo;s consider the second objective: returning the largest number among those with the maximum score. A number is larger if its most significant digits (the ones on the left) are larger. My greedy approach of placing the 9s first, followed by the next largest digit, and then the 0s, naturally constructs the number in descending order of its digits. This ensures that the largest possible digits (the 9s) occupy the most significant positions, thus creating the lexicographically largest and numerically greatest number possible for that set of digits.\nFor example, if num = 3 and sum = 18, my algorithm would generate 9, then 9, then the remaining sum is 0. The digits are {9, 9, 0}. The number constructed is \u0026ldquo;990\u0026rdquo;. This number has the maximum possible score ($9^2+9^2+0^2 = 162$) and is the largest possible permutation of these digits.\nThere is an initial check for feasibility: if (9 * num \u0026lt; sum). The maximum possible sum for num digits is achieved if all digits are 9, which gives a sum of $9 \\times \\text{num}$. If the required sum exceeds this, it is impossible to form such a number, so the function correctly returns an empty string. The problem statement also implies a lower bound check is needed. The sum of digits must be at least 1 for a positive integer, which is covered by the constraints.\nThe formal proof of correctness for this greedy algorithm relies on an \u0026ldquo;exchange argument\u0026rdquo;. Let our greedy solution produce a sequence of digits $G = (g_1, g_2, \\dots, g_n)$, where $n=\\text{num}$. Let $O = (o_1, o_2, \\dots, o_n)$ be any other valid sequence of digits for an optimal solution. We want to show that the score of $G$ is at least as high as the score of $O$, and that the number formed by $G$ is at least as large as the number formed by $O$ if the scores are equal. Assume the digits in both sequences are sorted in descending order for score analysis: $g_1\u0026rsquo; \\ge g_2\u0026rsquo; \\ge \\dots$ and $o_1\u0026rsquo; \\ge o_2\u0026rsquo; \\ge \\dots$. Our greedy algorithm produces the lexicographically largest possible sequence of digits that maximizes the sum of squares. Let\u0026rsquo;s find the first index $i$ where $g_i\u0026rsquo; \\neq o_i\u0026rsquo;$. Since our algorithm picks the largest possible digits first, it must be that $g_i\u0026rsquo; \u0026gt; o_i\u0026rsquo;$. Since both sequences have the same sum, there must be some other index $j \u0026gt; i$ where $o_j\u0026rsquo; \u0026gt; g_j\u0026rsquo;$. Consider the digits ${o_i\u0026rsquo;, o_j\u0026rsquo;}$. We have $o_i\u0026rsquo; \u0026lt; g_i\u0026rsquo; \\le 9$ and $o_j\u0026rsquo; \u0026gt; g_j\u0026rsquo; \\ge 0$. Let\u0026rsquo;s modify the optimal solution $O$ by changing the pair $(o_i\u0026rsquo;, o_j\u0026rsquo;)$ to $(o_i\u0026rsquo;+1, o_j\u0026rsquo;-1)$. The sum of digits remains the same. The new sum of squares is $(o_i\u0026rsquo;+1)^2 + (o_j\u0026rsquo;-1)^2 = o_i\u0026rsquo;^2 + 2o_i\u0026rsquo; + 1 + o_j\u0026rsquo;^2 - 2o_j\u0026rsquo; + 1 = (o_i\u0026rsquo;^2 + o_j\u0026rsquo;^2) + 2(o_i\u0026rsquo; - o_j\u0026rsquo; + 1)$. Since $o_i\u0026rsquo; \u0026lt; o_j\u0026rsquo;$ (because the sequence is sorted), $o_i\u0026rsquo; - o_j\u0026rsquo; + 1 \\le 0$. This transformation might not increase the score. Let\u0026rsquo;s re-evaluate the core property. The function $f(x)=x^2$ is a convex function. For any set of numbers $d_i$ with $\\sum d_i = C$, the sum $\\sum d_i^2$ is maximized when the values are at the boundaries of their domain. So, we should use as many 9s and 0s as possible. My greedy choice of using 9s is correct. It makes some digits as large as possible (9), forcing others to be as small as possible (0s, and one remainder digit). This configuration maximizes the sum of squares. Any other configuration could be transformed into our greedy configuration by repeatedly taking 1 from a digit $d_i \u0026lt; 9$ and adding it to a digit $d_j \u0026lt; 9$ to make one of them larger. This \u0026ldquo;robbing Peter to pay Paul\u0026rdquo; strategy increases the sum of squares. So, the set of digits produced by the greedy algorithm is optimal for the score. My algorithm also arranges these digits in descending order, which by definition produces the largest number. Thus, the algorithm is correct.\nTime and Space Complexity Time Complexity: The algorithm\u0026rsquo;s runtime is determined by the number of digits num and the sum.\nThe while(sum \u0026gt; 9) loop runs at most sum / 9 times. In each iteration, it performs a constant number of operations (append, decrement, increment). So this part is $O(\\text{sum})$. The while(num - cnt \u0026gt; 0) loop runs at most num times. This part is $O(\\text{num})$. The total time complexity is therefore $O(\\text{sum} + \\text{num})$. Formally, let $T(\\text{num}, \\text{sum})$ be the running time. The first loop runs $k_1 = \\lfloor (\\text{sum}-1)/9 \\rfloor$ times. The second loop runs $k_2 = \\text{num} - k_1 - 1$ times. The total work is proportional to $k_1 + k_2 = \\text{num}-1$. However, building the string can take longer. If we assume appending to an ostringstream is amortized constant time, the logic holds. If it\u0026rsquo;s proportional to the string length, it would be a sum of lengths from 1 to num, resulting in $O(\\text{num}^2)$. But ostringstream is optimized. A simpler analysis bounds the total number of appends by num. The number of loops is bounded by sum and num. So $O(\\text{sum} + \\text{num})$ is a safe upper bound.\nSpace Complexity: The space used is primarily for the output string being built. The final string will have num digits. The ostringstream will use space proportional to the number of digits. Therefore, the space complexity is $O(\\text{num})$.\nFormally, $S(\\text{num}, \\text{sum})$ is the space for the ostringstream. Its final size is num characters. So $S(\\text{num}, \\text{sum}) = O(\\text{num})$.\nMinimum Operations to Transform Array The Problem This problem was presented as minOperations which takes two integer vectors, nums1 and nums2. Based on the code, it seems to be a custom transformation problem. The goal is to calculate a \u0026ldquo;cost\u0026rdquo; or \u0026ldquo;number of operations\u0026rdquo;. The calculation involves a base cost, which is the sum of element-wise absolute differences between nums1 and drevantor, and an additional cost related to a special value extra, which is the last element of nums2.\nMy Code The code I wrote calculates a value based on a specific formula. It first computes a base step count by summing up all the abs(nums1[i] - nums2[i]) differences. Then, it determines an additional cost related to the extra value. It finds the element in either nums1 or nums2 that is \u0026ldquo;nearest\u0026rdquo; to extra. An additional operation seems to be required, and its cost depends on whether extra falls between any (nums1[i], nums2[i]) pair.\nlong long Solution::minOperations(vector\u0026lt;int\u0026gt; \u0026amp;nums1, vector\u0026lt;int\u0026gt; \u0026amp;nums2) { // The \u0026#39;extra\u0026#39; value is defined as the last element of the second array. int extra = nums2.back(); long long step_cnt = 0; // `nearest` will store the index of the element closest to `extra`. int nearest = 0; // `nearest_d` stores the minimum distance found so far. int nearest_d = INT_MAX; // `flag` will be true if `extra` is \u0026#34;covered\u0026#34; by any range [nums1[i], nums2[i]]. bool flag = false; // `status` indicates whether the nearest element was found in nums1 (1) or nums2 (2). int status = 1; // Iterate through both arrays simultaneously. for(int i = 0; i \u0026lt; nums1.size(); i++) { // Find the element in either array that is closest to `extra`. if(abs(nums1[i] - extra) \u0026lt; nearest_d) { nearest = i; status = 1; nearest_d = abs(nums1[i] - extra); } if(abs(nums2[i] - extra) \u0026lt; nearest_d) { nearest = i; status = 2; nearest_d = abs(nums2[i] - extra); } // Check if `extra` lies within the closed interval defined by nums1[i] and nums2[i]. if((nums1[i] \u0026lt;= extra \u0026amp;\u0026amp; extra \u0026lt;= nums2[i]) || (nums2[i] \u0026lt;= extra \u0026amp;\u0026amp; extra \u0026lt;= nums1[i])) { flag = true; } // Accumulate the base cost, which is the sum of element-wise differences. step_cnt += abs(nums1[i] - nums2[i]); } // Determine the final cost based on the `flag` and `status`. if(flag) { // If `extra` was covered, the cost is the base cost plus 1. return step_cnt + 1; } else if(status == 1) { // If not covered, and the nearest element was in nums1, add 1 and the distance. return step_cnt + 1 + abs(nums1[nearest] - extra); } else { // status == 2 // If not covered, and the nearest element was in nums2, add 1 and the distance. return step_cnt + 1 + abs(nums2[nearest] - extra); } } My Solution This problem appears to define a unique cost function for transforming nums1 into nums2 with a special consideration for an extra value. My solution meticulously implements the calculation of this cost as defined by the underlying (unseen) problem logic. Let\u0026rsquo;s break down the components of the cost and rationalize the logic.\nThe core of the problem seems to be about transforming nums1 to nums2. A common way to measure the \u0026ldquo;distance\u0026rdquo; or \u0026ldquo;cost of transformation\u0026rdquo; between two arrays is the sum of absolute differences, also known as the Manhattan distance or $L_1$ norm of the difference vector. The line step_cnt += abs(nums1[i] - nums2[i]); calculates this base cost. This can be interpreted as the minimum number of increment/decrement operations needed to make nums1[i] equal to nums2[i] for all i.\nThe complexity arises from the special role of extra = nums2.back(). The final cost is not just step_cnt. There is an additional component. My algorithm\u0026rsquo;s logic suggests this is a \u0026ldquo;connection cost\u0026rdquo; to the extra value.\nThe code iterates through the arrays to determine two things regarding extra:\nCoverage (flag): It checks if extra is contained within any of the intervals [min(nums1[i], nums2[i]), max(nums1[i], nums2[i])]. If it is, the boolean flag is set to true. This suggests that if extra can be \u0026ldquo;formed\u0026rdquo; by a value between nums1[i] and nums2[i], the connection cost is minimal. Proximity (nearest, nearest_d, status): Concurrently, it finds the element across both nums1 and nums2 that is numerically closest to extra. It records the index (nearest), the minimum distance (nearest_d), and which array the element came from (status). This seems to be a fallback plan for calculating the connection cost if the \u0026ldquo;coverage\u0026rdquo; condition is not met. The final return statement synthesizes these findings:\nif (flag): If extra was \u0026ldquo;covered\u0026rdquo; by at least one interval, the total cost is step_cnt + 1. This can be interpreted as: the base transformation cost, plus a single operation to \u0026ldquo;activate\u0026rdquo; the connection to extra, which is cheap because it\u0026rsquo;s already within range. else: If extra was not covered, the cost is higher. It is step_cnt + 1 + nearest_d. This implies: the base transformation cost, plus one activation operation, plus an additional cost equal to the minimum distance to extra. This additional cost represents the effort needed to \u0026ldquo;pull\u0026rdquo; the nearest existing value towards extra to establish the connection. The correctness of this solution is predicated on it being a correct implementation of the specific, custom cost function defined by the problem. Assuming the problem is to calculate this exact cost, my algorithm is correct because it systematically computes each component of the formula.\nIt correctly accumulates the sum of absolute differences. It correctly iterates through all elements to check for the coverage condition and updates the flag appropriately. It correctly maintains the minimum distance to extra and the location of the element that achieves this minimum distance. It uses a conditional structure that correctly applies the final formula based on the computed flag and status. Let\u0026rsquo;s formalize the cost function $C$ that the algorithm computes. Let $S = \\sum_{i=0}^{n-1} |nums1_i - nums2_i|$. Let $E = nums2_{n-1}$. Let the \u0026ldquo;coverage\u0026rdquo; predicate be $P = \\exists i \\in [0, n-1] \\text{ s.t. } (E \\ge \\min(nums1_i, nums2_i)) \\land (E \\le \\max(nums1_i, nums2_i))$. Let $d_{min} = \\min_{i \\in [0, n-1]} (\\min(|nums1_i - E|, |nums2_i - E|))$. The algorithm computes the cost $C$ as: $C = \\begin{cases} S + 1 \u0026amp; \\text{if } P \\text{ is true} \\ S + 1 + d_{min} \u0026amp; \\text{if } P \\text{ is false} \\end{cases}$ My code is a direct and correct implementation of this function. The loops ensure that all $i$ are checked for the existence condition $P$ and that the minimum distance $d_{min}$ is found. The final if-else block correctly applies the two cases of the piecewise function.\nTime and Space Complexity Time Complexity: The algorithm is dominated by a single for loop that iterates through the arrays from i = 0 to nums1.size() - 1. Let $n$ be the size of the arrays. The loop runs $n$ times. Inside the loop, all operations are constant time:\nAbsolute value calculations. Comparisons. Assignments. Arithmetic operations. Therefore, the work done inside the loop is $O(1)$. The total time complexity is $n$ times this constant work, which is $O(n)$. Formally, let $T(n)$ be the running time. $T(n) = \\sum_{i=0}^{n-1} C$, where $C$ is the constant cost of the operations inside the loop. $T(n) = n \\cdot C = O(n)$.\nSpace Complexity: The algorithm uses a handful of variables (extra, step_cnt, nearest, nearest_d, flag, status) to store intermediate values. The amount of memory used is constant and does not depend on the size of the input arrays. Therefore, the space complexity is $O(1)$ (not including the space for the input arrays themselves).\n","permalink":"https://blog.tategotoazarasi.me/en/posts/uol-2025-wk3/","summary":"\u003ch2 id=\"weak-vertices\"\u003eWeak Vertices\u003c/h2\u003e\n\u003ch3 id=\"the-problem\"\u003eThe Problem\u003c/h3\u003e\n\u003cp\u003eIn graph theory, the structural integrity of a network can often be analyzed by identifying fundamental shapes within it, such as triangles. A triangle provides rigidity and is a common motif in many applications. This problem asks us to identify vertices that are \u003cem\u003enot\u003c/em\u003e part of any triangle. A vertex \u003ccode\u003ei\u003c/code\u003e is defined as being part of a triangle if it has two distinct neighbors, \u003ccode\u003ej\u003c/code\u003e and \u003ccode\u003ek\u003c/code\u003e, which are also neighbors of each other. Our task is to find all vertices that do not satisfy this condition, which the problem statement refers to as \u0026ldquo;weak vertices.\u0026rdquo; The graph is given to us in the form of an adjacency matrix.\u003c/p\u003e","title":"Uol 2025 Wk3 \u0026\u0026 LeetCode Biweekly Contest 168 Solutions"},{"content":"Recount Problem The recent schoolboard elections were hotly contested: a proposal to swap school start times for elementary and high school students, a controversial new dress code proposal that bans athletic clothes in school, and a proposal to raise real-estate taxes to pay for a new football practice facility, and the list goes on and on. It is now hours after the polls have closed and a winner has yet to emerge!\nIn their desperation, the election officials turn to you and ask you to write a program to count the vote!\nInput The input consists of a single test case, which is a list of votes cast. Each line in the input contains the name of a candidate for whom a vote was cast. A name may consist of multiple words, separated by spaces. Words contain letters or hyphens, but no other punctuation characters. There will be at least 2 votes on the list. The list of votes ends with a single line containing the characters ***. This line should not be counted. There can be up to 100 000 valid votes.\nOutput If a candidate obtained a simple or absolute majority of all votes cast (that is, more than any other candidate), output the name of this candidate! If no candidate obtained a simple majority, output: “Runoff!” (don’t forget to include the exclamation mark!)\nCode ##include \u0026lt;iostream\u0026gt; ##include \u0026lt;string\u0026gt; ##include \u0026lt;unordered_map\u0026gt; ##include \u0026lt;algorithm\u0026gt; // using namespace std; // For brevity in a single file solution namespace recount { int main(istream \u0026amp;cin, ostream \u0026amp;cout) { // Use an unordered_map to store vote counts for each candidate. // The key is the candidate\u0026#39;s name (string), and the value is their vote count (unsigned long). std::unordered_map\u0026lt;std::string, unsigned long\u0026gt; m = std::unordered_map\u0026lt;std::string, unsigned long\u0026gt;(); std::string line; // Read votes line by line until the sentinel value \u0026#34;***\u0026#34; is encountered. while(std::getline(cin, line)) { if(line == \u0026#34;***\u0026#34;) { break; } // Increment the vote count for the candidate named in the current line. // If the candidate is not yet in the map, they are added with a count of 1. m[line]++; } // Initialize a string to hold the winner\u0026#39;s name and a variable for the maximum vote count. std::string ans = \u0026#34;***\u0026#34;; // Sentinel value to check for ties. unsigned long max_vote = 0; // First pass: find the highest vote count among all candidates. for(const auto \u0026amp;[k, v]: m) { max_vote = std::max(max_vote, v); } // Second pass: find the candidate(s) who achieved the maximum vote count. for(const auto \u0026amp;[k, v]: m) { if(v == max_vote) { // If \u0026#39;ans\u0026#39; is no longer the sentinel value, it means we have already found // one winner. Finding another one means there is a tie. if(ans != \u0026#34;***\u0026#34;) { cout \u0026lt;\u0026lt; \u0026#34;Runoff!\u0026#34;; return 0; // Exit after printing the result for a tie. } // This is the first candidate found with the maximum vote count. ans = k; } } // If the loop completes and \u0026#39;ans\u0026#39; has been updated exactly once, print the winner\u0026#39;s name. cout \u0026lt;\u0026lt; ans; return 0; } } Solution This problem asks us to process a list of votes, where each vote is a string representing a candidate\u0026rsquo;s name. We need to find the candidate with the most votes. If there is a single candidate with the highest vote count, we print their name. If two or more candidates are tied for the highest count, we must declare a \u0026ldquo;Runoff!\u0026rdquo;. The list of votes is terminated by a special string, ***.\nTo solve this, we need an efficient way to store and count votes for potentially many different candidates. A hash map ( or dictionary) is the ideal data structure for this task. In C++, this is implemented as std::unordered_map. We can map each candidate\u0026rsquo;s name (a std::string) to their total vote count (an integer type like unsigned long).\nThe overall algorithm proceeds in three main stages:\nFirst, we read the input and count the votes. We iterate through each line of the input. For each line, which represents a single vote, we check if it is the terminator string ***. If it is, we stop reading. Otherwise, we use the candidate\u0026rsquo;s name as a key in our unordered_map and increment the corresponding value. The [] operator of std::unordered_map is very convenient here: if the key (the name) doesn\u0026rsquo;t exist in the map, it is automatically inserted with a default-constructed value (0 for integers), and then the increment operation ++ brings it to 1. If the key already exists, its value is simply incremented.\nSecond, after processing all votes, we need to determine the highest number of votes received by any candidate. We can achieve this by iterating through all the key-value pairs in our map and keeping track of the maximum value (vote count) seen so far. Let\u0026rsquo;s call this maximum value max_vote.\nThird, we must identify the winner or detect a tie. A single pass through the map is not sufficient to do this reliably while also finding the maximum value. Therefore, a second pass through the map is the simplest and clearest approach. In this second iteration, we compare each candidate\u0026rsquo;s vote count with the max_vote we found in the previous step. We use a string variable, say winner_name, initialized to a special sentinel value (like the *** from the input, as it cannot be a valid candidate name). When we find the first candidate whose vote count equals max_vote, we store their name in winner_name. If we then encounter another candidate whose vote count also equals max_vote, we know there is a tie. At this point, we can immediately print \u0026ldquo;Runoff!\u0026rdquo; and terminate the program. If the second loop completes without finding a second candidate with max_vote, it means there is a unique winner, whose name is now stored in our winner_name variable. We then print this name.\nComplexity Analysis Let N be the total number of votes cast, and C be the number of unique candidates. Let L be the maximum length of a candidate\u0026rsquo;s name.\nTime Complexity The process can be broken down into three parts.\nReading votes and populating the map: We loop N times. Inside the loop, std::getline takes O(L) time. Accessing the unordered_map with a string key involves hashing the string, which takes O(L) time on average. Therefore, this phase has an average-case time complexity of O(N * L). Finding the maximum vote count: We iterate through the C unique candidates stored in the map. This takes O(C) time. Identifying the winner or a tie: We again iterate through the C unique candidates, which takes O(C) time. The total time complexity is the sum of these parts: O(N * L + C + C) = O(N * L + C). Since the number of unique candidates C cannot exceed the total number of votes N (i.e., C ≤ N), the complexity is dominated by the first phase, resulting in a final average-case time complexity of O(N * L).\nSpace Complexity The primary space usage comes from the unordered_map. In the worst-case scenario, every vote is for a different candidate, meaning we would store N unique names. The space required for the map is proportional to the number of unique candidates (C) and the sum of the lengths of their names. In the worst case, this is O(N * L), where we store N names of average length L. Thus, the space complexity is O(N * L).\nSet Problem Set is a card game designed by Marsha Falco in 1974 which is marketed by Set Enterprises, Inc. It also appears in syndicated form on the website of the New York Times. The player is shown 12 cards, each of which contains 1, 2, or 3 symbols. The symbols are either diamonds, squiggles, or ovals. Symbols are drawn using either a solid, striped, or open fill style. Each symbol’s color is either red, green, or purple. On a given card, all symbols are of the same type, same color, and have the same fill style.\nTo make a set, you must select three cards for which all 4 characteristics are either the same or pairwise different. For instance, 3 cards where the first shows 2 striped red ovals, the second shows 3 striped green squiggles, and the third shows 1 striped purple diamond form a set. They show 2, 3, and 1 symbols (each has a different number); they show ovals, squiggles, and diamonds (each shows a different shape); they use colors red, green, and purple (3 different colors); and lastly, they all share the same fill style: striped.\nWrite a program that finds all sets for 12 provided cards!\nInput The input to your program will consist of 4 lines, each containing 3 strings representing 3 cards, each consisting of four characters ABCD where\nA ∈ {1, 2, 3}, corresponding to the number of symbols.\nB ∈ {D, S, O}, corresponding to diamonds (D), squiggles (S), and ovals (O).\nC ∈ {S, T, O}, corresponding to solid (S), striped (T), and open (O) fill styles.\nD ∈ {R, G, P}, corresponding to red (R), green (G), and purple (P).\nThink of the cards as being arranged in the input as follows:\n+\u0026mdash;\u0026mdash;\u0026mdash;-+ | 1 2 3 | | 4 5 6 | | 7 8 9 | | 10 11 12 | +\u0026mdash;\u0026mdash;\u0026mdash;-+\nOutput Output all sets you can find, one per line. For each set, output the numbers of the card in the set in sorted order. The sets should be listed in sorted order using the number of their first card, breaking ties using the numbers of the second and third card in the set. If no sets can be formed, output “no sets”.\nCode #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026lt;sstream\u0026gt; #include \u0026lt;algorithm\u0026gt; #include \u0026lt;unordered_set\u0026gt; namespace set { std::vector\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt; ans = {}; class card { public: int id; char f[4]; card(int id, std::string s); }; class cardset { public: unsigned short mask = -1; int cnt = 0; std::unordered_set\u0026lt;card *\u0026gt; cards{}; cardset() = default; cardset(card *c); void insert(card *c); }; cardset::cardset(card *c) { this-\u0026gt;cards.insert(c); this-\u0026gt;cnt = 1; } unsigned short calc_mask(card *c1, card *c2) { unsigned short mask = 0; for(int i = 0; i \u0026lt; 4; i++) { mask \u0026lt;\u0026lt;= 1; mask |= c1-\u0026gt;f[i] == c2-\u0026gt;f[i]; } return mask; } card::card(int id, std::string s) { this-\u0026gt;id = id; std::istringstream iss(s); iss \u0026gt;\u0026gt; this-\u0026gt;f[0] \u0026gt;\u0026gt; this-\u0026gt;f[1] \u0026gt;\u0026gt; this-\u0026gt;f[2] \u0026gt;\u0026gt; this-\u0026gt;f[3]; } void cardset::insert(card *c) { if(this-\u0026gt;mask == (unsigned short) (-1)) { this-\u0026gt;mask = calc_mask(c, *this-\u0026gt;cards.begin()); } this-\u0026gt;cards.insert(c); this-\u0026gt;cnt++; if(this-\u0026gt;cnt == 3) { std::vector\u0026lt;int\u0026gt; vec = {}; for(auto \u0026amp;card_ptr: this-\u0026gt;cards) { vec.emplace_back(card_ptr-\u0026gt;id); } std::sort(vec.begin(), vec.end()); ans.emplace_back(vec); } } bool fit(card *c, const cardset *s) { if(s-\u0026gt;mask == (unsigned short) (-1)) { return true; } for(auto \u0026amp;sc: s-\u0026gt;cards) { if(calc_mask(sc, c) != s-\u0026gt;mask) { return false; } } return true; } int main(std::istream \u0026amp;cin, std::ostream \u0026amp;cout) { cardset sets[1 \u0026lt;\u0026lt; 10] = {}; int sets_cnt = 0; std::string input; for(int i = 1; i \u0026lt;= 12; i++) { cin \u0026gt;\u0026gt; input; card *newcard = new card(i, input); for(int j = 0; j \u0026lt; sets_cnt; j++) { if(fit(newcard, \u0026amp;sets[j])) { cardset newset = sets[j]; newset.insert(newcard); sets[sets_cnt++] = (newset); } } cardset newset = cardset(newcard); sets[sets_cnt++] = (newset); } if(ans.size() == 0) { cout \u0026lt;\u0026lt; \u0026#34;no sets\u0026#34;; return 0; } std::sort(ans.begin(), ans.end(), [](const std::vector\u0026lt;int\u0026gt; \u0026amp;a, const std::vector\u0026lt;int\u0026gt; \u0026amp;b) { if(a[0] != b[0]) { return a[0] \u0026lt; b[0]; } else if(a[1] != b[1]) { return a[1] \u0026lt; b[1]; } else { return a[2] \u0026lt; b[2]; } }); for(const auto \u0026amp;s: ans) { cout \u0026lt;\u0026lt; s[0] \u0026lt;\u0026lt; \u0026#39; \u0026#39; \u0026lt;\u0026lt; s[1] \u0026lt;\u0026lt; \u0026#39; \u0026#39; \u0026lt;\u0026lt; s[2] \u0026lt;\u0026lt; std::endl; } return 0; } } Solution The problem asks us to find all valid \u0026ldquo;sets\u0026rdquo; from a given collection of 12 cards. A set consists of three cards where for each of their four features, the feature values are either all identical or all pairwise different.\nThe provided C++ code implements a constructive or incremental algorithm to find these sets. Instead of checking every possible combination of three cards (brute-force), it builds up potential sets by adding one card at a time.\nThe core logic revolves around a clever use of bitmasking to represent the relationship between two cards. The function calc_mask(card *c1, card *c2) generates a 4-bit integer. Each bit corresponds to one of the four features. The bit is set to 1 if the feature is the same on both cards, and 0 if it\u0026rsquo;s different. This \u0026ldquo;similarity mask\u0026rdquo; compactly describes how two cards relate to each other.\nThe rule for a set of three cards (A, B, C) can be rephrased using this mask: the similarity mask between A and B must be identical to the similarity mask between A and C, and also identical to the similarity mask between B and C. This ensures the \u0026ldquo;all same or all different\u0026rdquo; property for every feature.\nThe main algorithm proceeds as follows:\nIt processes cards one by one, from card 1 to card 12. It maintains an array, sets, which stores cardset objects. A cardset is a potential set, which can contain one or two cards.\nFor each new card newcard that is read:\nIt iterates through all cardset objects already created (sets[j]). A cardset can be of size 1 (a single card) or size 2 (a pair of cards). The function fit(newcard, \u0026amp;sets[j]) checks if newcard can be validly added to the existing cardset. If sets[j] contains only one card, any newcard can \u0026ldquo;fit\u0026rdquo; to form a pair. A new cardset of size 2 is created from this pair. Its mask member is now calculated and stored, representing the similarity between these two cards. If sets[j] contains two cards, fit checks if newcard\u0026rsquo;s similarity mask with both cards in sets[j] matches the mask already stored in sets[j]. If it does, a valid set of three has been found. A new cardset of size 3 is created, and its card IDs are added to the global ans vector. After attempting to extend all existing cardsets, a new cardset containing only the newcard is created and added to the list. This allows newcard to start new potential sets with subsequently processed cards. After all 12 cards have been processed, the ans vector contains all found sets. The code then checks if any sets were found. If not, it prints \u0026ldquo;no sets\u0026rdquo;. Otherwise, it sorts the list of sets lexicographically as required by the problem statement and prints each set on a new line.\nComplexity Analysis: Let N be the number of cards (N=12).\nTime Complexity The outer loop runs N times. The inner loop iterates through sets_cnt, which is the number of existing cardsets. After processing i cards, the number of cardsets of size 1 is i and the number of size 2 is i*(i-1)/2. So, sets_cnt grows quadratically, O(i^2). The total work is approximately the sum of i^2 from i=1 to N-1, which results in a time complexity of O(N^3). For N=12, this is very efficient.\nSpace Complexity The sets array stores all cardset objects. The number of these objects is O(N^2). Each cardset stores pointers, so the space is dominated by the array itself, leading to O(N^2) space complexity.\nPlanting Trees Problem Farmer Jon has recently bought n tree seedlings that he wants to plant in his yard. It takes 1 day for Jon to plant a seedling, and for each tree Jon knows exactly in how many days after planting it grows to full maturity. Jon would also like to throw a party for his farmer friends, but in order to impress them he would like to organize the party only after all the trees have grown. More precisely, the party can be organized at earliest on the next day after the last tree has grown up.\nHelp Jon to find out when is the earliest day when the party can take place. Jon can choose the order of planting the trees as he likes, so he wants to plant the trees in such a way that the party will be as soon as possible.\nInput The input consists of two lines. The first line contains a single integer N (1 ≤ N ≤ 100 000) denoting the number of seedlings. Then a line with N integers t_i follows (1 ≤ t_i ≤ 1 000 000), where t_i denotes the number of days it takes for the i-th tree to grow.\nOutput You program should output exactly one line containing one integer, denoting the earliest day when the party can be organized. The days are numbered 1, 2, 3, \u0026hellip; beginning from the current moment.\nCode ##include \u0026lt;iostream\u0026gt; ##include \u0026lt;vector\u0026gt; ##include \u0026lt;algorithm\u0026gt; namespace plantingtrees { int main(std::istream \u0026amp;cin, std::ostream \u0026amp;cout) { int n; cin \u0026gt;\u0026gt; n; std::vector\u0026lt;int\u0026gt; vec(n); for(int i = 0; i \u0026lt; n; i++) { cin \u0026gt;\u0026gt; vec[i]; } // Sort the tree growth times in descending order. // The rbegin() and rend() iterators are used for reverse sorting. std::sort(vec.rbegin(), vec.rend()); int ans = 0; // Iterate through the trees in the chosen planting order. // The tree at index \u0026#39;i\u0026#39; is planted on day \u0026#39;i + 1\u0026#39;. for(int i = 0; i \u0026lt; n; i++) { // Day of planting: i + 1 // Growth time: vec[i] // Maturity day: (i + 1) + vec[i] // Party day must be after all trees mature, so we find the maximum maturity day. // The party is on the day AFTER the last tree matures. // The value `i + vec[i] + 2` corresponds to `(i + 1) + vec[i] + 1`, which is the earliest possible party day // if this tree is the last one to mature. ans = std::max(ans, i + vec[i] + 2); } cout \u0026lt;\u0026lt; ans; return 0; } } Solution The problem asks for the earliest possible day to hold a party, which must be the day after all planted trees have matured. We have N seedlings, and we know the time t_i each seedling takes to mature after being planted. Planting one seedling takes one day. We can decide the order of planting. The goal is to find a planting order that minimizes the final party day.\nLet\u0026rsquo;s analyze the timeline. If we decide on a planting order, the first tree is planted on day 1, the second on day 2, and so on, with the i-th tree in the sequence being planted on day i. If this i-th tree has a maturity time of t_i, it will be fully grown on day i + t_i. The party can only happen after all trees are mature. This means we need to find the latest maturity day among all trees. The party can be held on the day immediately following this latest maturity day. So, for a given planting sequence p_1, p_2, ..., p_N with corresponding growth times t_{p_1}, t_{p_2}, ..., t_{p_N}, the party day will be 1 + max(1 + t_{p_1}, 2 + t_{p_2}, ..., N + t_{p_N}). Our task is to find an ordering (a permutation p) of the trees that minimizes this value.\nThis problem can be solved using a greedy approach. The intuition is that trees requiring a longer time to grow should be planted as early as possible. This gives them a \u0026ldquo;head start\u0026rdquo; on their long maturation period. Conversely, trees that grow quickly can be planted later without significantly pushing back the final completion date.\nLet\u0026rsquo;s prove this greedy strategy is optimal. The strategy is: sort the trees in descending order of their growth times ( t_i) and plant them in that order. Consider any optimal planting schedule. If this schedule is not sorted by growth time in descending order, there must be at least one pair of adjacent trees in the planting sequence, say at day i and i+1, where the tree planted on day i (let\u0026rsquo;s call it tree A with growth time t_A) has a shorter growth time than the tree planted on day i+1 (tree B with growth time t_B). So, t_A \u0026lt; t_B.\nThe maturity days for these two trees in this schedule are:\nMaturity of A: i + t_A\nMaturity of B: (i + 1) + t_B\nAll other trees in the sequence are unaffected by what we do with A and B. The latest maturity day for the schedule is max(..., i + t_A, (i + 1) + t_B, ...).\nNow, let\u0026rsquo;s swap the planting order of A and B. We plant B on day i and A on day i+1. The new maturity days are:\nNew Maturity of B: i + t_B\nNew Maturity of A: (i + 1) + t_A\nLet\u0026rsquo;s compare the latest maturity day of just this pair before and after the swap. Before swap, the latest is max(i + t_A, i + 1 + t_B). Since t_A \u0026lt; t_B, it implies t_A \u0026lt;= t_B - 1. So, i + t_A \u0026lt; i + t_B - 1 \u0026lt; i + 1 + t_B. The maximum is i + 1 + t_B. After swap, the latest is max(i + t_B, i + 1 + t_A). Since t_A \u0026lt; t_B, it implies i + 1 + t_A \u0026lt; i + 1 + t_B. And also i + t_B is greater than i + 1 + t_A if t_B - t_A \u0026gt; 1. Regardless, the maximum is i + t_B.\nComparing the maximums: (i + t_B) (after swap) vs. (i + 1 + t_B) (before swap). Clearly, i + t_B \u0026lt; i + 1 + t_B. The swap has reduced the latest maturity day for this pair. Since all other trees' maturity days remain unchanged, the overall latest maturity day for the entire schedule can only decrease or stay the same. It cannot increase. This \u0026ldquo;exchange argument\u0026rdquo; shows that we can always improve or maintain an unsorted schedule by moving longer-growth-time trees earlier. By repeatedly applying this logic, we can transform any optimal schedule into one that is sorted by growth time descending, without making the result worse. Therefore, the greedy strategy of planting trees with longer growth times first is indeed optimal.\nThe implementation is straightforward:\nRead N and all the growth times t_i into a vector. Sort the vector in descending order. Initialize a variable max_party_day to 0. Iterate through the sorted vector from i = 0 to N-1. The tree t_i is planted on day i+1. Its maturity day is (i+1) + t_i. The earliest party day considering this tree would be (i+1) + t_i + 1. We update our max_party_day with the maximum of its current value and this new calculated day. After the loop, max_party_day will hold the final answer. Complexity Analysis Let N be the number of seedlings.\nTime Complexity The dominant operation is sorting the growth times. Standard sorting algorithms take O(N log N) time. Reading the input takes O(N), and the final loop to calculate the maximum party day also takes O(N). Therefore, the total time complexity is O(N log N).\nSpace Complexity We need to store the N growth times in a vector, which requires O(N) space.\n","permalink":"https://blog.tategotoazarasi.me/en/posts/uol-2025-wk2/","summary":"\u003ch2 id=\"recount\"\u003eRecount\u003c/h2\u003e\n\u003ch3 id=\"problem\"\u003eProblem\u003c/h3\u003e\n\u003cp\u003eThe recent schoolboard elections were hotly contested: a proposal to swap school start times for elementary and high\nschool students, a controversial new dress code proposal that bans athletic clothes in school, and a proposal to raise\nreal-estate taxes to pay for a new football practice facility, and the list goes on and on. It is now hours after the\npolls have closed and a winner has yet to emerge!\u003c/p\u003e","title":"Uol 2025 Wk2 Solutions"},{"content":"The story begins, as many do, quite mundanely. I needed to install a piece of software called SwashbucklerDiary, which was only officially available as a .deb package. For an Arch Linux user, this is hardly a problem; the debtap utility was made for exactly this scenario.\nAs is my usual practice, I created a temporary directory to handle the conversion, ensuring I wouldn\u0026rsquo;t clutter my main Downloads folder.\n\u0026gt; pwd /home/myusername/Downloads/tmp \u0026gt; ls SwashbucklerDiary-1.17.0-linux-x64.deb Everything looked normal. The directory contained only the .deb file I had just downloaded. I first ran the standard debtap command, and it successfully generated the pkg.tar.zst package I needed.\n\u0026gt; debtap SwashbucklerDiary-1.17.0-linux-x64.deb ... (conversion process output omitted) ... ==\u0026gt; Package successfully created! ==\u0026gt; Removing leftover files... \u0026gt; ls -alh total 106M drwxr-xr-x 2 myusername myusername 116 Aug 6 12:34 . drwxr-xr-x 4 myusername myusername 41 Aug 6 12:34 .. -rw-r--r-- 1 myusername root 58M Aug 6 12:34 com.yucore.swashbucklerdiary-1.17.0-1-x86_64.pkg.tar.zst -rw-r--r-- 1 myusername myusername 49M Aug 4 18:14 SwashbucklerDiary-1.17.0-linux-x64.deb Perfect. The converted package and the original .deb file were both present. But then, driven by curiosity and a desire to learn, I decided I wanted to see what the PKGBUILD file generated by debtap looked like. The tool provides the -p or -P flag for this purpose. So, I deleted the newly created package and ran the command again, this time with the -p flag.\n\u0026gt; debtap -p SwashbucklerDiary-1.17.0-linux-x64.deb ... (same interactive prompts) ... ==\u0026gt; Package successfully created! ==\u0026gt; Generating PKGBUILD file... mv: cannot stat \u0026#39;PKGBUILD\u0026#39;: No such file or directory ==\u0026gt; PKGBUILD is now located in \u0026#34;/home/myusername/Downloads/tmp\u0026#34; and ready to be edited ==\u0026gt; Removing leftover files... The output seemed a bit odd. There was an error message: mv: cannot stat 'PKGBUILD': No such file or directory. But the final line still confidently informed me that the PKGBUILD had been generated in the current directory. I didn\u0026rsquo;t think much of it and habitually typed ls -alh.\nThen, I saw something that sent a chill down my spine.\n\u0026gt; ls -alh total 0 Completely empty.\nMy first reaction was shock. Not only was the expected PKGBUILD directory missing, but the original .deb file had also vanished! The entire tmp directory had been wiped clean.\nStranger things were yet to come. I tried to cd .. and then cd tmp back into the directory. My shell prompt (I use Oh My Zsh with Powerlevel10k) displayed some bizarre artifacts, as if the directory\u0026rsquo;s metadata itself had been corrupted. When I ran ls -alh again, I was greeted with an even more bewildering output:\n\u0026gt; ls -alh total 0 drwxr-xr-x 2 myusername myusername 6 Aug 6 12:35 . drwxr-xr-x 4 myusername myusername 41 Aug 6 12:35 .. Look at the size of the . directory: 6 bytes. A normal, freshly emptied directory on my XFS filesystem should be 4096 bytes. This was highly unusual.\nMy mind started racing. My /home directory is on a RAID0 array of two SSDs, formatted with XFS. My first thought was, \u0026ldquo;It\u0026rsquo;s over. Did my RAID0 array just fail? Did one of the drives die?\u0026rdquo; The high performance of RAID0 comes at the cost of zero redundancy; the failure of a single drive means the loss of all data on the array. I immediately started checking dmesg and system logs, but I found no signs of I/O errors or filesystem corruption.\nAfter ruling out hardware and filesystem issues, I calmed down and started to suspect debtap itself. Since the first run without -p was normal, and the second run with -p caused the disaster, the problem had to be linked to that specific flag.\nI decided to reproduce the issue, but this time in a completely safe environment. I created a new test directory, populated it with a few harmless dummy files, and a copy of the .deb package.\nmkdir ~/safe-test cd ~/safe-test touch fileA.txt fileB.log cp ~/Downloads/SwashbucklerDiary-1.17.0-linux-x64.deb . ls -l # total 49264 # -rw-r--r-- 1 myusername myusername 50442386 Aug 4 18:14 SwashbucklerDiary-1.17.0-linux-x64.deb # -rw-r--r-- 1 myusername myusername 0 Aug 6 13:00 fileA.txt # -rw-r--r-- 1 myusername myusername 0 Aug 6 13:00 fileB.log Then, holding my breath, I executed the \u0026ldquo;demonic\u0026rdquo; command once more:\ndebtap -p SwashbucklerDiary-1.17.0-linux-x64.deb After the process finished, I ran ls.\nls -l # total 0 The result was identical. The directory was wiped clean.\nAt this point, the case was clear. This was no paranormal event or hardware failure. It was an extremely dangerous bug in debtap that, when used with the -p or -P flag, would delete all files in the current working directory.\nWith the problem identified, the next step was to find the cause. debtap is a shell script, which makes source code analysis very straightforward. I opened /usr/bin/debtap, version 3.6.2. It was a massive script, over three thousand lines long, so a full read-through was out of the question.\nMy investigation had a clear focus:\nThe bug is strongly correlated with the -p/-P flags. The function of these flags is to generate a PKGBUILD. The final result is the deletion of the current directory. Therefore, I needed to find the code block in the script that handled the -p/-P flags and was responsible for generating and moving the PKGBUILD file. I searched the code for the keyword pkgbuild.\nNear the end of the script, I quickly found the logic for handling the PKGBUILD creation.\n# ... (code for generating PKGBUILD content) ... # Moving PKGBUILD (and .INSTALL, if it exists) and announcing its creation pkgname=\u0026#34;$(grep \u0026#39;^pkgname=\u0026#39; PKGBUILD | sed s\u0026#39;/^pkgname=//\u0026#39;)\u0026#34; if [[ $output == set ]]; then pkgbuild_location=\u0026#34;$(dirname \u0026#34;$outputdirectory/$pkgname-PKGBUILD\u0026#34;)\u0026#34; rm -rf \u0026#34;$pkgbuild_location\u0026#34; 2\u0026gt; /dev/null mkdir \u0026#34;$pkgbuild_location\u0026#34; 2\u0026gt; /dev/null # ... (error handling and file moving code) ... else pkgbuild_location=\u0026#34;$(dirname \u0026#34;\u0026#34;$(dirname \u0026#34;$package_with_full_path\u0026#34;)\u0026#34;/$pkgname-PKGBUILD\u0026#34;)\u0026#34; rm -rf \u0026#34;$pkgbuild_location\u0026#34; 2\u0026gt; /dev/null mkdir \u0026#34;$pkgbuild_location\u0026#34; 2\u0026gt; /dev/null # ... (error handling and file moving code) ... fi My eyes were immediately drawn to the line rm -rf \u0026quot;$pkgbuild_location\u0026quot;. This was, without a doubt, the prime suspect. The script was executing a forced, recursive delete command. The question now was: what was the actual value of the $pkgbuild_location variable?\nLet\u0026rsquo;s focus on the key line in the else block, which is where execution goes since I didn\u0026rsquo;t use the -o output directory option:\npkgbuild_location=\u0026#34;$(dirname \u0026#34;\u0026#34;$(dirname \u0026#34;$package_with_full_path\u0026#34;)\u0026#34;/$pkgname-PKGBUILD\u0026#34;)\u0026#34; This line looks a bit complex, with two nested dirname commands. Let\u0026rsquo;s dissect it and analyze its execution step by step.\ndirname is a basic shell command that strips the filename from a path, returning only the directory path. For example:\ndirname /usr/bin/ls returns /usr/bin dirname /home/user/file.txt returns /home/user Now, let\u0026rsquo;s substitute the actual variable values from my session.\n$package_with_full_path: This variable is defined at the beginning of the script as the absolute path to the input .deb file. In my case, its value was /home/myusername/Downloads/tmp/SwashbucklerDiary-1.17.0-linux-x64.deb. $pkgname: This variable is extracted from the temporarily generated PKGBUILD file. According to my logs, the converted package name was com.yucore.swashbucklerdiary-1.17.0-1. Now, let\u0026rsquo;s trace the nested command:\nStep 1: Execute the inner dirname\n\u0026#34;$(dirname \u0026#34;$package_with_full_path\u0026#34;)\u0026#34; # Becomes: \u0026#34;$(dirname \u0026#34;/home/myusername/Downloads/tmp/SwashbucklerDiary-1.17.0-linux-x64.deb\u0026#34;)\u0026#34; The output of this step is the directory containing the .deb file: /home/myusername/Downloads/tmp.\nStep 2: Concatenate the string\nThe result from the previous step is then concatenated with the rest of the string, forming a longer path:\n\u0026#34;/home/myusername/Downloads/tmp/$pkgname-PKGBUILD\u0026#34; # Substituting $pkgname: \u0026#34;/home/myusername/Downloads/tmp/com.yucore.swashbucklerdiary-1.17.0-1-PKGBUILD\u0026#34; This string represents a path\u0026hellip; wait, this looks like a file path, not a directory. The author\u0026rsquo;s intent was likely to create a directory named packagename-PKGBUILD to place the PKGBUILD file into.\nStep 3: Execute the outer dirname\nNow for the most critical step. The script takes the entire string generated in Step 2 and runs the outer dirname on it:\n\u0026#34;$(dirname \u0026#34;/home/myusername/Downloads/tmp/com.yucore.swashbucklerdiary-1.17.0-1-PKGBUILD\u0026#34;)\u0026#34; And what is the output of this command? It is /home/myusername/Downloads/tmp!\nThe Truth is Revealed\nAfter these three steps, we have the final value of the pkgbuild_location variable: /home/myusername/Downloads/tmp, which was the current working directory where I ran the debtap command.\nNow let\u0026rsquo;s look back at those fatal lines of code:\npkgbuild_location=\u0026#34;/home/myusername/Downloads/tmp\u0026#34; rm -rf \u0026#34;$pkgbuild_location\u0026#34; # Effectively becomes: rm -rf \u0026#34;/home/myusername/Downloads/tmp\u0026#34; mkdir \u0026#34;$pkgbuild_location\u0026#34; # Effectively becomes: mkdir \u0026#34;/home/myusername/Downloads/tmp\u0026#34; The mystery was solved. The script first calculated the wrong path—the current working directory—and then, without hesitation, executed rm -rf, deleting the directory and everything inside it (including my original .deb file). Immediately after, the mkdir command recreated the directory, which is why I was left with an empty tmp directory whose metadata appeared to have been \u0026ldquo;initialized.\u0026rdquo;\nThis was a classic yet terrifying logical error. The author likely intended to ensure the target directory was clean by deleting and recreating it. However, the incorrect use of a double dirname caused the deletion target to shift from the \u0026ldquo;intended subdirectory\u0026rdquo; to the \u0026ldquo;entire current directory.\u0026rdquo;\nAfter discovering the root cause, a new thought occurred to me: a bug this severe couldn\u0026rsquo;t have been in debtap for long, or it would have been discovered ages ago. It must have been introduced recently.\nI decided to do some \u0026ldquo;code archeology\u0026rdquo; in the debtap GitHub repository to uncover the bug\u0026rsquo;s history. Using git blame and browsing the commit history, I quickly zeroed in on a suspicious commit: commit 27a9ff5.\nThe commit message was a simple code update. Let\u0026rsquo;s look at its diff:\ndiff --git a/debtap b/debtap index 4518a7a..71aea20 100755 --- a/debtap +++ b/debtap @@ -3458,8 +3458,8 @@ if [[ $output == set ]]; then fi else pkgbuild_location=\u0026#34;$(dirname \u0026#34;\u0026#34;$(dirname \u0026#34;$package_with_full_path\u0026#34;)\u0026#34;/$pkgname-PKGBUILD\u0026#34;)\u0026#34; - rm -rf \u0026#34;$pkgbuilt_location\u0026#34; 2\u0026gt; /dev/null - mkdir \u0026#34;$pkgbuilt_location\u0026#34; 2\u0026gt; /dev/null + rm -rf \u0026#34;$pkgbuild_location\u0026#34; 2\u0026gt; /dev/null + mkdir \u0026#34;$pkgbuild_location\u0026#34; 2\u0026gt; /dev/null if [[ $? != 0 ]]; then echo -e \u0026#34;${red}Error: Cannot create PKGBUILD directory to the same directory as .deb package, permission denied. Removing leftover files and exiting...${NC}\u0026#34; rm -rf \u0026#34;$working_directory\u0026#34; Seeing this, it all clicked, and I didn\u0026rsquo;t know whether to laugh or cry.\nBefore this commit, the code was:\nrm -rf \u0026#34;$pkgbuilt_location\u0026#34; 2\u0026gt; /dev/null mkdir \u0026#34;$pkgbuilt_location\u0026#34; 2\u0026gt; /dev/null Notice the variable name: pkgbuilt_location. But the variable defined above was named pkgbuild_location. It was a * typo*!\nIn shell scripting, referencing a non-existent variable (due to a typo, for instance) causes it to expand to an empty string. Therefore, before commit 27a9ff5, the commands being executed were effectively:\nrm -rf \u0026#34;\u0026#34; 2\u0026gt; /dev/null mkdir \u0026#34;\u0026#34; 2\u0026gt; /dev/null rm -rf \u0026quot;\u0026quot; and mkdir \u0026quot;\u0026quot; do nothing and produce no errors. Thus, although the flawed dirname logic was calculating the wrong path, this typo prevented it from ever being used in the rm -rf command. The typo acted like a safety fuse, unintentionally protecting countless users\u0026rsquo; data by a strange twist of fate.\nThe author of commit 27a9ff5 likely spotted this typo during a code review and, with the good intention of \u0026ldquo;fixing the code,\u0026rdquo; changed pkgbuilt_location to the correct pkgbuild_location. He \u0026ldquo;fixed\u0026rdquo; the typo, but in doing so, he unwittingly armed the bomb.\nIt\u0026rsquo;s a textbook case of how a seemingly trivial, well-intentioned change can lead to catastrophic consequences if the full context and potential impact are not understood.\nHaving unraveled the entire story, I knew I had to report this to the project maintainer immediately to prevent more users from falling victim. I quickly created a new issue on the debtap GitHub repository.\nThe issue got a swift response from the community. Other users confirmed they had encountered the same problem, with one user expressing relief that they hadn\u0026rsquo;t run the command in their $HOME directory—a comment that further underscored the bug\u0026rsquo;s severity.\nThe project maintainer, helixarch, took notice quickly and released a fix a few days later. Let\u0026rsquo;s look at the core diff that fixed the bug:\n--- a/debtap +++ b/debtap @@ -3486,7 +3486,7 @@ if [[ $output == set ]]; then echo -e \u0026#34;${lightgreen}==\u0026gt;${NC} ${bold}PKGBUILD is now located in${normal} ${lightblue}\\\u0026#34;$pkgbuild_location\\\u0026#34;${NC} ${bold}and ready to be edited${normal}\u0026#34; fi else - pkgbuild_location=\u0026#34;$(dirname \u0026#34;\u0026#34;$(dirname \u0026#34;$package_with_full_path\u0026#34;)\u0026#34;/$pkgname-PKGBUILD\u0026#34;)\u0026#34; + pkgbuild_location=\u0026#34;\u0026#34;$(dirname \u0026#34;$package_with_full_path\u0026#34;)\u0026#34;/$pkgname-PKGBUILD\u0026#34; rm -rf \u0026#34;$pkgbuild_location\u0026#34; 2\u0026gt; /dev/null mkdir \u0026#34;$pkgbuild_location\u0026#34; 2\u0026gt; /dev/null if [[ $? != 0 ]]; then The fix was direct and elegant. The maintainer removed the outer dirname.\nNow, the calculation for pkgbuild_location became:\npkgbuild_location=\u0026#34;\u0026#34;$(dirname \u0026#34;$package_with_full_path\u0026#34;)\u0026#34;/$pkgname-PKGBUILD\u0026#34; Let\u0026rsquo;s trace this new logic:\ndirname \u0026quot;$package_with_full_path\u0026quot; is still /home/myusername/Downloads/tmp. The concatenated string is /home/myusername/Downloads/tmp/com.yucore.swashbucklerdiary-1.17.0-1-PKGBUILD. This value is now directly assigned to pkgbuild_location. Consequently, the subsequent commands become:\nrm -rf \u0026#34;/home/myusername/Downloads/tmp/com.yucore.swashbucklerdiary-1.17.0-1-PKGBUILD\u0026#34; mkdir \u0026#34;/home/myusername/Downloads/tmp/com.yucore.swashbucklerdiary-1.17.0-1-PKGBUILD\u0026#34; This is exactly the behavior we want! The script now correctly creates a new, clean subdirectory within the current directory to store the PKGBUILD file, without posing any threat to the current directory itself.\nWith the release of debtap 3.6.3, this heart-stopping bug was finally squashed.\n","permalink":"https://blog.tategotoazarasi.me/en/posts/discovering-a-catastrophic-rm-rf-bug-in-debtap/","summary":"A deep-dive investigation into the Arch Linux tool \u003ccode\u003edebtap\u003c/code\u003e reveals how a well-intentioned typo fix accidentally activated a catastrophic \u003ccode\u003erm -rf\u003c/code\u003e bug that deleted all files in the current working directory.","title":"Discovering a Delete-Your-Files-and-Run Level Bug in debtap"},{"content":"It\u0026rsquo;s a familiar story for many Linux enthusiasts: the thrill of unboxing a shiny new laptop, the eager anticipation of installing your favorite distribution, and then\u0026hellip; the little papercuts. Sometimes it\u0026rsquo;s Wi-Fi, sometimes suspend/resume, and very often, it\u0026rsquo;s the audio, particularly the microphone. My recent acquisition, a Lenovo ThinkBook 16 G7+ ASP powered by an AMD Ryzen AI 9 365 (part of the \u0026ldquo;Strix Point\u0026rdquo; family, for those keeping score), running CachyOS (an Arch-based distribution) with kernel 6.14.8-2-cachyos, decided to give me the silent treatment from its built-in digital microphone array.\nIf you\u0026rsquo;re facing a similar issue, especially on recent AMD hardware, I hope my odyssey provides some clues, or at least, solidarity.\nIs This Thing On? The first step in any troubleshooting saga is to gather information. What does the system think it has?\nThe Kernel\u0026rsquo;s Perspective (ALSA) At the lowest level accessible to most user-space tools, we have ALSA (Advanced Linux Sound Architecture). The command arecord -l lists capture hardware devices as ALSA sees them:\n**** List of CAPTURE Hardware Devices **** card 1: Generic_1 [HD-Audio Generic], device 0: ALC257 Analog [ALC257 Analog] Subdevices: 1/1 Subdevice #0: subdevice #0 card 2: acppdmmach [acp-pdm-mach], device 0: DMIC capture dmic-hifi-0 [] Subdevices: 1/1 Subdevice #0: subdevice #0 This was interesting and somewhat promising. The output showed two main entries. The first, card 1: Generic_1 [...] ALC257 Analog, was identified as our standard analog audio codec, a Realtek ALC257. This component would typically handle headphone jacks and, if the laptop had one, an analog microphone input, though many modern devices exclusively use digital arrays. The second entry, card 2: acppdmmach [...] DMIC capture, immediately looked like our target. The \u0026ldquo;DMIC\u0026rdquo; clearly stands for Digital Microphone, and \u0026ldquo;acp-pdm-mach\u0026rdquo; suggested its connection via AMD\u0026rsquo;s Audio Co-Processor (ACP) using Pulse Density Modulation (PDM), a common interface for digital microphones. So, ALSA seemed to be aware of a digital microphone. That\u0026rsquo;s a good start.\nFor completeness, aplay -l shows playback devices:\n**** List of PLAYBACK Hardware Devices **** card 0: Generic [HD-Audio Generic], device 3: HDMI 0 [HDMI 0] ... (other HDMI outputs) ... card 1: Generic_1 [HD-Audio Generic], device 0: ALC257 Analog [ALC257 Analog] Subdevices: 0/1 Subdevice #0: subdevice #0 Card 0 is the HDMI audio output from the AMD GPU, and Card 1 is the analog output via the ALC257 (speakers, headphones). This all seemed normal.\nThe Sound Server\u0026rsquo;s Perspective (PipeWire) This was interesting and somewhat promising.\nModern Linux desktops predominantly use PipeWire, often with WirePlumber as the session manager, to handle audio and video streams. This system provides compatibility layers for PulseAudio and JACK applications. To understand PipeWire\u0026rsquo;s perspective, I used the pactl list cards command.\nThe output revealed a couple of important \u0026ldquo;cards\u0026rdquo; as seen by PipeWire. The first, Card #42: alsa_card.pci-0000_65_00.1, was named HD-Audio Generic (its alsa.card_name) and more specifically identified by its device.product.name as Rembrandt Radeon High Definition Audio Controller. This clearly corresponded to ALSA\u0026rsquo;s card 0. It listed various HDMI outputs but, notably, had sources: 0, which is logical as HDMI audio is typically an output-only path.\nThe second entry from PipeWire, Card #43: alsa_card.pci-0000_65_00.6, was also designated as HD-Audio Generic by its alsa.card_name. However, its device.product.name was Family 17h/19h/1ah HD Audio Controller, and its alsa.mixer_name was Realtek ALC257. This card matched ALSA\u0026rsquo;s card 1. Its active profile was reported as HiFi (Mic1, Mic2, Speaker). Delving into its Ports section, PipeWire listed an [Out] Speaker and an [Out] Headphones port, the latter being not available unless headphones were plugged in. For inputs, it showed an [In] Mic2: Stereo Microphone, possibly associated with the headphone jack and also not available unless a device was connected, and, crucially, an [In] Mic1: Digital Microphone whose availability was marked as unknown.\nThe presence of \u0026ldquo;Mic1: Digital Microphone\u0026rdquo; under this ALC257-associated card (Card #43) was initially a bit perplexing. It wasn\u0026rsquo;t immediately clear if the DMIC was routed through the ALC257 codec or if this was simply how PipeWire and WirePlumber decided to group these functionalities based on ALSA Use Case Manager (UCM) profiles. What stood out was that the acppdmmach device, which ALSA identified as card 2 and the likely candidate for the DMIC, wasn\u0026rsquo;t directly listed as a distinct top-level \u0026ldquo;Card\u0026rdquo; in the pactl list cards output. This was a significant flag, suggesting that while ALSA might expose the device, PipeWire might not be initializing or interpreting it correctly to present it as a fully independent audio card.\nPCI Device Identification To get a clearer picture of the underlying hardware, I used lspci | grep Audio. This command confirmed the audio-related PCI devices present in the system:\n65:00.1 Audio device: Advanced Micro Devices, Inc. [AMD/ATI] Rembrandt Radeon High Definition Audio Controller 65:00.5 Multimedia controller: Advanced Micro Devices, Inc. [AMD] ACP/ACP3X/ACP6x Audio Coprocessor (rev 70) 65:00.6 Audio device: Advanced Micro Devices, Inc. [AMD] Family 17h/19h/1ah HD Audio Controller The output broke down as follows: the device at PCI address 65:00.1 was identified as the HDMI audio controller, part of the AMD Radeon graphics. The device at 65:00.5 was the AMD Audio Co-Processor (ACP), specifically revision 70; this is the component primarily responsible for handling the digital microphone (DMIC). Finally, the device at 65:00.6 was the analog audio controller, which interfaces with the Realtek ALC257 codec for speakers and headphone jacks. This information aligned perfectly with what arecord -l and pactl list cards were suggesting: the DMIC\u0026rsquo;s functionality was undeniably tied to the ACP.\nSystem Software Check A quick check of installed packages confirmed I had the usual suspects for a modern Linux audio setup. The core PipeWire stack, including pipewire, pipewire-alsa, pipewire-pulse, and wireplumber, was present. The ALSA essentials, such as alsa-lib, alsa-utils, and alsa-card-profiles, were also installed. Crucially, I had fairly recent versions of the necessary firmware blobs: linux-firmware (version 20250508) and sof-firmware (Sound Open Firmware, version 2025.01.1). The sof-firmware package is particularly important for modern Intel and AMD audio hardware, especially for devices connected via coprocessors like AMD\u0026rsquo;s ACP.\nAt this stage, the initial reconnaissance suggested that ALSA was aware of a DMIC device. PipeWire seemed to acknowledge a \u0026ldquo;Digital Microphone\u0026rdquo; port in its configuration, but it wasn\u0026rsquo;t entirely clear if this port was properly associated with the ACP\u0026rsquo;s dedicated acppdmmach device. The hardware components were clearly present, and the core audio software and firmware were installed. Despite all this, the internal microphone remained stubbornly silent.\nLogs and Configurations Time to get our hands dirty with logs and deeper configuration details.\nKernel Messages (dmesg) Initially, I tried sudo dmesg | grep -iE 'acp|dmic|snd_pci_acp|snd_sof_amd' but got no output. This was puzzling. dmesg should always have something. This might have been an artifact of how I was filtering or perhaps the relevant messages had scrolled out of the buffer quickly after boot. I made a mental note to try broader dmesg searches later or check the full journalctl -k. The absence of explicit error messages here was, in itself, a piece of information – no obvious driver crashes or failures to load for these specific terms, at least not that grep caught initially.\nALSA Use Case Manager (UCM) ALSA UCM files describe how devices, ports, and profiles are meant to be used. They are essential for PipeWire/WirePlumber to make sense of complex audio hardware. Since pactl list cards associated \u0026ldquo;Mic1: Digital Microphone\u0026rdquo; with Card #43 (ALSA card 1, the ALC257), I dumped its UCM: alsaucm -c hw:1 dump text (where hw:1 refers to ALSA card 1).\nThe output contained this interesting snippet under Verb.HiFi:\nDevice.Mic1 { Comment \u0026#34;Digital Microphone\u0026#34; Values { CaptureCTL \u0026#34;_ucm0001.hw:Generic_1\u0026#34; CaptureMixerElem \u0026#34;Mic ACP LED\u0026#34; CapturePCM \u0026#34;_ucm0001.hw:acppdmmach\u0026#34; // BINGO! CapturePriority 100 CaptureSwitch \u0026#34;Mic ACP LED Capture Switch\u0026#34; PlaybackCTL \u0026#34;_ucm0001.hw:Generic_1\u0026#34; TQ HiFi } } The line CapturePCM \u0026quot;_ucm0001.hw:acppdmmach\u0026quot; was key. It explicitly states that the UCM profile\u0026rsquo;s \u0026ldquo;Mic1\u0026rdquo; (Digital Microphone) expects to use the ALSA PCM device named acppdmmach. This device was listed by arecord -l as card 2. So, the UCM config for the ALC257 (card 1) references the ACP\u0026rsquo;s DMIC (card 2) for its \u0026ldquo;Digital Microphone\u0026rdquo; input. This explained why \u0026ldquo;Digital Microphone\u0026rdquo; appeared under the ALC257\u0026rsquo;s card in PipeWire – it was following the UCM logic.\nThis reinforced that acppdmmach needed to be fully functional and correctly interpreted by the higher layers.\nWirePlumber\u0026rsquo;s Sanity WirePlumber is the session manager that makes many of the decisions about how PipeWire connects things. Its logs are invaluable. journalctl -b --user -u wireplumber revealed the smoking gun:\nMay 24 19:39:01 wangzhiheng wireplumber[1808]: wp-device: SPA handle \u0026#39;api.alsa.acp.device\u0026#39; could not be loaded; is it installed? May 24 19:39:01 wangzhiheng wireplumber[1808]: s-monitors: Failed to create \u0026#39;api.alsa.acp.device\u0026#39; device There it was. WirePlumber was explicitly failing to load or create something called api.alsa.acp.device. SPA stands for Simple Plugin API, which PipeWire uses for its plugins. This strongly suggested that WirePlumber, despite ALSA knowing about acppdmmach (card 2), couldn\u0026rsquo;t properly interface with the ACP device to make its DMIC functionality available as a source.\nOther errors in the WirePlumber log like \u0026lt;WpAsyncEventHook:0x64962e406260\u0026gt; failed: \u0026lt;WpSiStandardLink:0x64962e7914f0\u0026gt; Object activation aborted were likely downstream consequences of this primary failure. If the ACP device isn\u0026rsquo;t properly created, linking to/from it will fail.\nThe logs also mentioned: s-monitors-libcamera: PipeWire's libcamera SPA plugin is missing or broken. This was unrelated to the microphone but worth noting for camera troubleshooting later, perhaps. For now, focus on audio.\nI also ran tree /usr/share/wireplumber to understand its configuration structure. There was no /etc/wireplumber override directory on my system, and notably, no 50-alsa-config.lua in the common paths mentioned in some online troubleshooting guides. This meant WirePlumber was likely running with its default configuration scripts found in /usr/share/wireplumber/scripts/ and main config /usr/share/wireplumber/wireplumber.conf. The absence of 50-alsa-config.lua isn\u0026rsquo;t necessarily an error; modern WirePlumber versions might integrate its logic differently or it might be an optional override file.\nFull PCI Details with Kernel Modules (lspci -nnk) This command is a goldmine, showing PCI devices, their vendor/device IDs, and the kernel driver currently managing them, plus other candidate modules.\nOf course. Here is the revised text in a plaintext paragraph style:\nThe lspci -nnk command, which provides detailed PCI information including the kernel drivers in use, offered the most revealing clues when focused on the Multimedia Controller at 65:00.5. For this specific device, the AMD Audio Co-Processor with revision 70, the system reported:\nSubsystem: Lenovo Device [17aa:38b3] Kernel driver in use: snd_acp_pci Kernel modules: snd_pci_acp3x, snd_rn_pci_acp3x, snd_pci_acp5x, snd_pci_acp6x, snd_acp_pci, snd_rpl_pci_acp6x, snd_pci_ps, snd_sof_amd_renoir, snd_sof_amd_rembrandt, snd_sof_amd_vangogh, snd_sof_amd_acp63, snd_sof_amd_acp70 The line Kernel driver in use: snd_acp_pci confirmed that the generic ACP PCI driver was loaded and correctly bound to the device. However, the Kernel modules line was truly fascinating. This list showed all the kernel modules that the system considered potential handlers for this hardware. It critically included several important SOF (Sound Open Firmware) drivers. Among them was snd_sof_amd_rembrandt, which was logical since my \u0026ldquo;Strix Point\u0026rdquo; APU is a successor to the Rembrandt architecture. Most importantly, it listed specific drivers like snd_sof_amd_acp63 and snd_sof_amd_acp70. Since my ACP was identified as rev 70, the snd_sof_amd_acp70 module immediately stood out as a very strong candidate for providing the specialized SOF layer needed to properly operate the DMIC.\nThe other audio devices:\n65:00.1 Audio device [0403]: ...Rembrandt Radeon High Definition Audio Controller [1002:1640] Kernel driver in use: snd_hda_intel (Standard for HDMI audio).\n65:00.6 Audio device [0403]: ...Family 17h/19h/1ah HD Audio Controller [1022:15e3] Kernel driver in use: snd_hda_intel (Standard for analog HDA codecs like the ALC257).\nThis confirmed that the standard drivers were loaded for the HDMI and analog audio parts. The focus remained on the ACP and how snd_acp_pci interacts (or needs to interact) with a SOF DSP driver like snd_sof_amd_acp70 or a more generic snd_sof_amd_common.\nConnecting the Dots Okay, summarizing the clues gathered so far painted a fairly clear picture. First, ALSA, the fundamental sound layer, correctly identified an acppdmmach device as card 2, which was our prime suspect for the digital microphone. Second, the ALSA Use Case Manager (UCM) configuration for the Realtek ALC257\u0026rsquo;s \u0026ldquo;Digital Microphone\u0026rdquo; profile explicitly pointed to this acppdmmach device for its capture PCM. This meant the system intended for that device to be used.\nHowever, a critical issue arose at the PipeWire/WirePlumber level: WirePlumber\u0026rsquo;s logs showed a failure to load or create an api.alsa.acp.device. This indicated a breakdown in how the higher-level sound server was trying to interface with the ACP hardware. On the hardware and driver side, we knew the ACP hardware (1022:15e2 rev 70) was present and the generic snd_acp_pci kernel driver was loaded and active. Furthermore, the necessary SOF (Sound Open Firmware) firmware was installed, and relevant SOF-related kernel modules, such as snd_sof_amd_acp70, were available on the system.\nThese points strongly suggested that while the basic components were in place, the interaction or initialization sequence between the generic ACP driver (snd_acp_pci) and the more specialized SOF layer needed for the DMIC was not happening correctly, leading to WirePlumber\u0026rsquo;s inability to properly utilize the ACP device.\nMy hypothesis: The snd_acp_pci driver by itself might not be enough, or it\u0026rsquo;s not being initialized in a way that fully exposes the PDM/DMIC capabilities to the SOF layer that WirePlumber expects for api.alsa.acp.device. Essentially, the DSP part of the ACP, which handles the DMIC array, might not be \u0026ldquo;activated\u0026rdquo; correctly for PipeWire\u0026rsquo;s consumption.\nThis is a common pattern on newer AMD (and Intel) laptops where DMICs are processed by a dedicated DSP firmware (SOF) running on an audio coprocessor. If this firmware isn\u0026rsquo;t loaded correctly or the driver isn\u0026rsquo;t configured to use it for PDM microphones, things go silent.\nThe Fix Many SOF-based drivers, especially for PDM microphones, have module options to enable or configure specific features. A common one is related to enabling PDM microphone support.\nGiven the list of kernel modules from lspci -nnk, particularly snd_sof_amd_acp70, and the existence of a more generic snd_sof_amd_common module that often serves as a wrapper or common codebase for various AMD ACP SoF drivers, I searched for module options related to these.\nA frequently suggested fix for AMD ACP DMIC issues, based on community forums and bug reports, involves using an enable_pdm kernel module option. The main question was, which specific module should this option target? Possibilities included snd_sof_amd_acp70, the more specific SOF driver for my ACP revision; snd_sof_amd_common, which often serves as a common codebase or umbrella for newer AMD platforms before a highly tailored driver is fully mature or mainlined; or even snd_acp_pci itself, though this was less likely as enable_pdm is typically a SOF-specific feature. The prevailing wisdom for recent AMD platforms often points towards using snd_sof_amd_common for enabling PDM microphone support.\nTherefore, the proposed solution was to add this kernel module option. The first step was to create a new configuration file, for instance, by running sudo nano /etc/modprobe.d/99-thinkbook-mic-fix.conf. The 99- prefix can help ensure this configuration is loaded late, although the loading order for simple options lines isn\u0026rsquo;t usually critical unless there are direct conflicts; the .conf suffix is, however, mandatory for the system to recognize the file.\nInside this new file, the critical line to add was:\noptions snd_sof_amd_common enable_pdm=1 This instruction tells the snd_sof_amd_common kernel module to explicitly enable PDM microphone support when it loads during system startup. The value 1 is equivalent to true for this boolean option.\nAfter saving this configuration file, the next crucial step was to rebuild the initramfs (initial RAM filesystem). Module options can affect how devices are probed very early in the boot process, so updating the initramfs ensures these new options are available at that stage. On Arch-based systems like my CachyOS installation, this is typically done with the command:\nsudo mkinitcpio -P This command rebuilds all preset initramfs images, incorporating the new modprobe configuration.\nFinally, a full reboot of the system was necessary. This allows the kernel to load with the new module option active, potentially changing how the audio hardware is initialized. This approach felt like a strong candidate for a fix because it directly addressed the PDM (Pulse Density Modulation) aspect of the digital microphone array, targeted a relevant SOF module (snd_sof_amd_common), and was a widely reported solution for similar audio problems on AMD hardware.\nVerification After the reboot, it was time for the moment of truth.\nI opened a voice recorder application and spoke into the laptop. And there it was – the input level meter danced! The microphone was working.\nsudo dmesg | grep -Ei 'sof|acp|dmic|snd_sof_amd_common|snd_sof_amd_acp70'\n[ 0.000000] BIOS-e820: [mem 0x0000000009f00000-0x0000000009f37fff] ACPI NVS ... (many ACPI table lines) ... [ 0.411425] ACPI: \\_SB_.PCI0.GPPA.ACP_.PWRS: New power resource // ACP Power Resource defined in ACPI ... [ 5.676187] snd_acp_pci 0000:65:00.5: enabling device (0000 -\u0026gt; 0002) // The snd_acp_pci driver enabling the device. The crucial line here is [ 5.676187] snd_acp_pci 0000:65:00.5: enabling device (0000 -\u0026gt; 0002). This shows the generic ACP PCI driver is indeed initializing the hardware.\nIdeally, what we would hope to see in the dmesg output after a successful SOF-based DMIC initialization, which the enable_pdm=1 option is intended to trigger, are more specific log lines. These might include messages from sof-audio-pci-intel (or its AMD equivalents like snd_sof_amd_common or snd_sof_amd_acp70) indicating that they have successfully probed or initialized the Digital Signal Processor (DSP). We might also look for lines confirming the detection of PDM devices or DMICs by the SOF driver. Furthermore, logs indicating that the acp-pdm-mach ALSA device is now being registered by the SOF layer would be strong evidence of a successful initialization sequence.\nThis troubleshooting journey, specific to a ThinkBook 16 G7+ ASP with an AMD \u0026ldquo;Strix Point\u0026rdquo; APU, underscores several common themes in Linux audio problem-solving. The audio stack\u0026rsquo;s layered complexity, from hardware and kernel drivers ( ALSA, SOF) through the sound server (PipeWire) and session manager (WirePlumber) to applications, means issues can arise at many points of interaction. Consequently, examining logs is paramount: dmesg (or journalctl -k) for kernel messages, and user-level service logs for WirePlumber and PipeWire, are indispensable. Ensuring up-to-date firmware, particularly linux-firmware and sof-firmware, is non-negotiable for modern systems. ALSA UCM files also play a vital role in how PipeWire interprets complex audio devices, and while they can sometimes require patches for new hardware, the UCM seemed correct in this instance. Kernel module parameters, configured via /etc/modprobe.d/, are powerful tools for enabling features or addressing hardware quirks, though finding the correct module and option often necessitates research. The increasing prevalence of dedicated DSPs and audio coprocessors, like AMD\u0026rsquo;s ACP running SOF firmware for tasks such as DMIC array processing, introduces another layer that must function correctly; the enable_pdm=1 option is a direct result of this architectural shift. Furthermore, ACPI tables significantly influence how the OS discovers and configures hardware, including audio components. Finally, the collective wisdom of the Linux community found in forums, wikis, and bug trackers is an immense resource. If the applied fix, such as options snd_sof_amd_common enable_pdm=1, hadn\u0026rsquo;t resolved the issue, the next steps would have involved trying the enable_pdm=1 option with a more specific module like snd_sof_amd_acp70, searching for entirely different module options, testing newer kernel versions (as driver support continually improves), checking for BIOS/UEFI updates from the laptop manufacturer, or, as a last resort, filing detailed bug reports with the relevant upstream projects. Given that this ThinkBook model and its APU are quite new, it\u0026rsquo;s not uncommon for the latest hardware to require such targeted adjustments until broader Linux support fully matures and these configurations become default or are integrated into UCM profiles.\n","permalink":"https://blog.tategotoazarasi.me/en/posts/troubleshooting-a-stubborn-dmic-on-a-thinkbook-16-g7-plus-asp-with-linux/","summary":"Detailed troubleshooting process for fixing a silent digital microphone on a Lenovo ThinkBook 16 G7+ ASP (AMD Ryzen AI 9) laptop running Linux, primarily resolved by adding the kernel module parameter \u003ccode\u003eoptions snd_sof_amd_common enable_pdm=1\u003c/code\u003e.","title":"Troubleshooting a Stubborn DMIC on a ThinkBook 16 G7+ ASP with Linux"},{"content":"Anki, a powerful spaced repetition software, is widely appreciated for its flexibility and customizability. Many users download or purchase elaborately designed card templates from the internet. These templates often incorporate complex HTML, CSS, and JavaScript to achieve rich interactive effects and aesthetically pleasing visual presentations. However, this complexity sometimes introduces a challenge: when we need to migrate data, adjust templates, or simply understand how card content is generated, we may find that the data within the template is not directly visible but is instead dynamically rendered via JavaScript or presented in some form of \u0026ldquo;obfuscation.\u0026rdquo;\nThis blog post aims to explore a systematic approach to \u0026ldquo;demystify\u0026rdquo; such complex Anki cards, extract their core data, and lay the groundwork for subsequent data reuse (for example, migrating to new, simpler templates or performing data analysis). We will use actual card templates encountered (such as a political review template and a driving test question bank template) as examples to progressively analyze the processing flow and key technical points. This article focuses more on the thought process and methodology rather than a direct reiteration of code, hoping that readers, after understanding, can adapt and practice according to their own needs.\nWhy Demystify Card Templates? Before diving into the technical details, it is essential to first clarify the motivations and value detrás de demystifying Anki card templates. A core driving factor is the need for data migration and template replacement. Over long-term Anki usage, users might wish to migrate their accumulated card content from one template to another – perhaps to a self-designed one, a superior community-sourced template, or transitioning from a complex commercial template to a lighter, more personalized one. Direct copy-pasting is often unfeasible because much of the visible content in advanced templates is dynamically generated, underscoring the necessity of extracting the raw, underlying data through demystification.\nAnother significant benefit lies in data cleaning and format unification. Original card data can be intermingled with a considerable amount of HTML tags and inline style information that are not essential to the content itself, or the data formatting across different fields might be inconsistent. By demystifying the cards and extracting relatively pure data, we can more conveniently perform subsequent data cleansing tasks, unify data formats, and establish a solid foundation for further data processing and utilization.\nFurthermore, the structured data extracted through this process opens up broad possibilities for data analysis and reuse. This extracted data can be employed for various statistical analyses, such as examining the distribution покупатели of different question types within a test bank or the frequency of specific knowledge points appearing across cards, thereby providing data-driven insights for adjusting learning strategies. Concurrently, this structured raw data can serve as a valuable resource for generating other forms of learning materials, such as mind maps or summary notes, enabling a multi-dimensional presentation and utilization of the acquired knowledge.\nFrom a technical skill development perspective, the demystification process itself presents a valuable opportunity for * learning template mechanisms and customization*. By meticulously reverse-engineering how data is processed and presented in complex card templates, users can gain a deeper understanding of advanced Anki templating system features, acquiring skills in areas like dynamic JavaScript interactions and sophisticated CSS layout techniques. Such experience is immensely beneficial for users aspiring to independently design and customize more powerful and highly personalized Anki templates in the future.\nFinally, and perhaps most fundamentally, mastering card demystification techniques empowers users to break free from dependency on specific templates. Once the core data is in their own hands, users are no longer tethered to a particular template that might become obsolete due to a lack of maintenance by the author, features no longer meeting their needs, or incompatibility issues with newer Anki versions. Data autonomy translates to greater flexibility and long-term control over one\u0026rsquo;s learning resources.\nIn essence, the primary goal of card demystification is to revert the \u0026ldquo;what you see is what you get\u0026rdquo; card content back to its intrinsic data structure, thereby gaining greater control and understanding over the card\u0026rsquo;s informational core.\nOverview of the Core Technical Stack To effectively achieve automated demystification of Anki cards, we need to leverage a combination of modern programming tools and libraries. Central to our scripting and development efforts are Node.js and TypeScript. Node.js provides a robust JavaScript runtime environment, making it highly suitable for executing automated scripts either server-side or locally. TypeScript, as a superset of JavaScript, introduces static type checking, which significantly enhances code robustness and maintainability. This is particularly advantageous when dealing with complex data structures and intricate logical flows, as it helps in identifying potential type errors температураly in the development cycle, thereby improving both development efficiency and overall code quality.\nFor simulating browser behavior and executing client-side JavaScript, Puppeteer plays an indispensable role. This Node library, maintained by the Google Chrome team, offers a high-level API that allows us to control Chrome or Chromium browsers programmatically via the DevTools Protocol. In the context of Anki card demystification, Puppeteer\u0026rsquo;s core value lies in its ability to create an authentic browser environment, typically operating in headless mode, meaning it can execute in the background without a graphical user interface. Many sophisticated Anki card templates extensively use JavaScript to dynamically generate content, manage user interactions, or even perform simple data decryption or transformations. If we were to analyze only the static HTML template files, we would often fail to capture the complete data as it is ultimately presented to the user after JavaScript processing. Puppeteer addresses this by loading the HTML, executing any embedded JavaScript, and simulating the browser\u0026rsquo;s full rendering pipeline, ultimately providing the final, rendered Document Object Model (DOM) structure. This capability is crucial for handling cards where content is not statically hardcoded into the HTML.\nOnce Puppeteer has completed the page rendering and returned the HTML string containing all dynamically generated content, JSDOM comes into play. JSDOM is a pure JavaScript implementation of the WHATWG DOM and HTML standards, primarily designed to facilitate the use of common web browser objects—such as window, document, and Element —within a Node.js environment. Specifically, JSDOM can parse the HTML string output by Puppeteer and transform it into a complete DOM tree structure. This DOM tree can then be manipulated and queried much like one would operate on the document object in a browser\u0026rsquo;s developer console, using standard DOM APIs like document.getElementById(), document.getElementsByClassName(), and document.querySelectorAll(). This provides immense convenience for precisely locating and extracting the required data from complex HTML structures.\nLastly, to interact with the Anki application itself—for reading source card data and writing processed new cards—we rely on AnkiConnect. AnkiConnect is a highly practical Anki add-on that exposes a local HTTP service interface, allowing external applications to programmatically control Anki. In our demystification workflow, AnkiConnect primarily undertakes the following responsibilities: First, through its findNotes action, we can batch-retrieve a list of note IDs that require processing, based on criteria such as deck name, tags, or other query parameters. Second, for each note ID, we can use the notesInfo action to obtain comprehensive details about the note, including the raw content of all its fields (e.g., \u0026ldquo;Question,\u0026rdquo; \u0026ldquo;Answer,\u0026rdquo; \u0026ldquo;Explanation\u0026rdquo;) and its associated tags and other metadata. Finally, after the data extraction and transformation are complete, we can utilize the addNote action to send the organized new data, structured according to a specified note type and field mapping, back to Anki, thereby completing the data migration or reformatting process. AnkiConnect, therefore, serves as the critical bridge facilitating data exchange between our automated script and the Anki database.\nGeneral Demystification Workflow Although different Anki card templates vary in complexity and implementation, the fundamental demystification process is largely similar and can be summarized into the following core stages:\nPreparation Phase: Understanding the Source Card This preparatory phase is of paramount importance, directly influencing the efficiency and accuracy of the subsequent automated scripting.\nThe initial step involves identifying the data source. This means precisely specifying the Anki deck containing the cards that need to be processed. Once the target deck is determined, AnkiConnect\u0026rsquo;s findNotes action can be utilized, constructing a query (e.g., deck:YourDeckName, where YourDeckName must be replaced with the actual deck name) to retrieve a list of unique IDs for all notes within that deck. This list of IDs will serve as the entry point for our automated processing.\nFollowing this, we move to analyzing the card structure, which is key to understanding how data is stored and rendered. It is advisable to select one or more representative card samples from the Anki card browser for meticulous examination. The first task here is to inspect the \u0026ldquo;Fields\u0026rdquo; content of these cards. It\u0026rsquo;s crucial to discern what type of raw data each field stores—for instance, which field holds the question text, which contains the options (paying close attention to potential delimiters between options, such as double vertical bars ||), which field records the correct answer, which provides a detailed explanation or notes, and whether auxiliary fields like question numbers or tags exist. A clear understanding of the source data at the field level forms the basis for subsequent data mapping.\nEven more critically, we need to delve into Anki\u0026rsquo;s template editor to carefully study the \u0026ldquo;Front Template,\u0026rdquo; \u0026ldquo;Back Template,\u0026rdquo; and \u0026ldquo;Styling (CSS).\u0026rdquo; Regarding the HTML structure, one must observe how data from various fields is embedded into the final HTML document via Anki\u0026rsquo;s placeholders (e.g., {{FieldName}} or {{cloze:FieldName}}). This helps in comprehending the mapping between raw data and the eventually displayed content.\nHowever, for complex card templates, analyzing JavaScript behavior often represents the core and most challenging aspect of demystification. Many templates leverage JavaScript to achieve dynamic content rendering and interactive effects. It\u0026rsquo;s necessary to broadly outline the main functionalities of the JavaScript code found within \u0026lt;script\u0026gt; tags. These scripts might be responsible for parsing raw data from placeholders (e.g., splitting an option string delimited by || into individual options and rendering them as HTML list items), dynamically highlighting options based on user selection and the correct answer, or controlling the display/hide logic for supplementary learning content like \u0026quot; Explanation\u0026quot; sections. Understanding how this JavaScript manipulates the DOM and processes data is vital for accurately simulating the rendering process later with Puppeteer.\nConcurrently, during the analysis of HTML and JavaScript, special attention must be paid to the use of **CSS class names **. CSS classes that are dynamically added or modified by JavaScript often serve as important clues for identifying card states, such as user-selected options, correct answers, or incorrect answers. For instance, a template might assign classes like correct-light or correct to a selected correct option, and wrong-light or wrong to incorrect ones. Identifying these key CSS class names will greatly assist in accurately extracting information from the rendered HTML using JSDOM later on.\nFinally, after a thorough understanding of the source card\u0026rsquo;s data composition and rendering logic is achieved, we need to determine the extraction targets. This involves clearly listing the specific data items we wish to extract from the old cards and how these items will correspond to the fields in the new card template. A well-defined set of targets will guide the development of our subsequent data extraction and transformation logic.\nCore Automation Process Having gained a deep understanding of the source cards in the preparation phase, we can proceed to the core automation process. The central idea of this process is to iterate through the list of note IDs obtained earlier and execute a standardized series of data extraction and transformation operations for the card represented by each ID.\nFor every note ID in the list, the first step in the processing pipeline is to retrieve the note\u0026rsquo;s information. We utilize AnkiConnect\u0026rsquo;s notesInfo action, passing the current note ID as a parameter. AnkiConnect will then return comprehensive details for that note, typically as an object containing all its fields and their corresponding values, along with a list of the note\u0026rsquo;s tags. These raw field values form the basis for constructing the HTML document that will be rendered.\nNext is the task of building the HTML document for rendering. This requires having a local HTML template file prepared beforehand, whose structure should be identical or very similar to the source Anki card\u0026rsquo;s template ( encompassing front, back, and CSS styles). Examples from our previous discussions include pol2.html or jiazhao.html. Once the field data for the current note is fetched, our script will systematically replace the predefined placeholders (e.g., {{Question}}, {{Options}}) in this local HTML template file with the actual data. A crucial detail here is that if placeholders contain special characters, such as colons (common in {{cloze:Question}}), these characters must be properly escaped when used in regular expressions for replacement to ensure accuracy. For instance, an {{Options}} placeholder would be replaced with the options string retrieved from the note, which is typically delimited by ||.\nOnce the HTML content, now populated with specific note data, has been constructed, the process moves to dynamic rendering using Puppeteer. The script first writes this generated HTML content to a temporary local HTML file. Then, Puppeteer is launched, and a new headless browser page instance is created. A particularly critical step, especially when dealing with templates like jiazhao.html that rely on Persistence.js or similar libraries for managing session state, is to perform necessary environment simulation. Some templates store user preferences or card states (e.g., whether to randomize options, whether to display explanations by default) in the Anki WebView\u0026rsquo;s session storage during user interaction with the front of the card. The back template then reads these settings upon loading to determine how to present its content. If our automated script attempts to directly render a template containing both front and back logic (or just the back, expecting it to show the \u0026ldquo;answer revealed\u0026rdquo; state) without first establishing the session state expected by Persistence.js, certain JavaScript logic dependent on these values might not execute as intended. A common consequence is that the \u0026ldquo;Explanation\u0026rdquo; section might remain hidden by default.\nTo address this, we employ Puppeteer\u0026rsquo;s page.evaluateOnNewDocument() method. This powerful API allows us to inject custom JavaScript code into the page before any of its own scripts are executed. We can leverage this to create a mock implementation of the Persistence object within the page\u0026rsquo;s context. This mock object needs to provide the same core APIs as the real library (such as isAvailable, getItem, setItem, and removeItem) and allow us to preset specific key-value pairs. For example, we can use code like window.Persistence.setItem('ANKI-SETTINGS-HIDE-NOTES', '0'); to compel the template\u0026rsquo;s script to believe that the user preference is to show the explanation. Similarly, to handle potential randomization of option order by the front template (which often stores the randomized order in an ANKI-OPTIONS-ORDER key for the back template to read), we can preset a fixed, non-random order in our mock, such as window.Persistence.setItem('ANKI-OPTIONS-ORDER', '1,2,3,4'); (assuming up to four options displayed in their original sequence).\n// Illustrative Puppeteer script snippet await page.evaluateOnNewDocument(() =\u0026gt; { const mockStore = {}; window.Persistence = { isAvailable: () =\u0026gt; true, getItem : (key) =\u0026gt; mockStore[key] ? JSON.parse(mockStore[key]) : null, setItem : (key, value) =\u0026gt; { mockStore[key] = JSON.stringify(value); }, // ... other necessary methods like removeItem, clear, getAllKeys, if used by the template script }; // Force display of explanations window.Persistence.setItem(\u0026#39;ANKI-SETTINGS-HIDE-NOTES\u0026#39;, \u0026#39;0\u0026#39;); // Set a default option order to ensure correct parsing and highlighting on the back // This value should align with the original option order expected by the card\u0026#39;s front template JS // or simply be set to \u0026#39;1,2,3,4...\u0026#39; Persistence.setItem(\u0026#39;ANKI-OPTIONS-ORDER\u0026#39;, \u0026#39;1,2,3,4\u0026#39;); }); After injecting the mocked Persistence environment, we use page.goto() to load the temporary HTML file containing the card\u0026rsquo;s data. Since JavaScript execution within templates is often asynchronous, **waiting for rendering to complete ** is an indispensable part of this stage. We must ensure that all relevant JavaScript logic has finished executing and the DOM has been updated to its final state before attempting to extract content. This can be achieved in several ways. One approach is to use page.waitForSelector(), which pauses execution until one or more elements matching a critical CSS selector appear in the DOM. For example, on the back of the card, we might wait for CSS classes indicating option states (such as correct, incorrect, or should-have-been-selected, e.g., .correct-light, .should-select-light, .correct) to be applied to the option \u0026lt;li\u0026gt; elements. Another method is page.waitForFunction(), which can wait for a JavaScript function executed in the page\u0026rsquo;s context to return a truthy value; for instance, we could write a function to check if the container for the \u0026ldquo;Explanation\u0026rdquo; has been populated with text. Once these waiting conditions are met, signifying that the page has fully rendered, we can invoke page.content() to retrieve the complete HTML content string of the rendered page.\nUpon obtaining the rendered HTML, the next step is to parse the HTML and extract structured data using JSDOM. We pass the HTML string returned by Puppeteer to JSDOM\u0026rsquo;s constructor, which generates a document object that can be manipulated within our Node.js environment using APIs highly compatible with those found in web browsers. Leveraging this document object, we can employ standard DOM traversal and query methods to precisely extract the desired data. For example, the question text is typically found within an element possessing a specific class name (e.g., .question), and may require further processing such as removing a question number prefix. For option texts, we would first locate the parent container holding all options (e.g., a div with id=\u0026quot;back-options\u0026quot;), then iterate through each child element representing an option (e.g., \u0026lt;li class=\u0026quot;option\u0026quot;\u0026gt;), extracting its textContent. The extraction of the correct answer(s) relies on inspecting these option elements for CSS classes that denote \u0026ldquo;correct\u0026rdquo; or \u0026ldquo;should-be-selected\u0026rdquo; states. Based on these classes and the original order of the options in the list (which can be determined by analyzing their index within the parent container or via option-specific IDs if present), we can determine the corresponding letter identifiers (A, B, C, D, etc.). If the card involves multiple correct answers, we need to concatenate the letters of all correctly marked options. Explanations or remarks are also usually housed within specific container elements (e.g., the \u0026lt;div id=\u0026quot;notes-wrapper\u0026quot;\u0026gt;\u0026lt;div class=\u0026quot;notes-container\u0026quot;\u0026gt;...\u0026lt;/div\u0026gt;\u0026lt;/div\u0026gt; structure in the jiazhao.html template); we can extract their innerHTML if preserving HTML formatting is desired, or textContent for plain text. As for tag information, this can be directly obtained from the notesInfo object fetched via AnkiConnect in the initial step. After extracting various data pieces, data cleansing is often necessary, which might involve removing leading/trailing whitespace using .trim() or stripping out unwanted HTML tags, depending on the requirements of the target field.\nOnce all required data has been successfully extracted from the rendered HTML and properly cleaned, we proceed to * constructing the new note data*. At this stage, we need to organize the extracted and processed data into a JavaScript object that conforms to the field structure of the target Anki note type and the requirements of AnkiConnect\u0026rsquo;s addNote action. This object will specify the target deck name (deckName), the target note type name (modelName), and a fields object. The keys of the fields object will be the field names in the target note type, and their values will be the data we just extracted and prepared. An example structure might look like this:\n{ deckName: \u0026#34;My New Driving Test Deck\u0026#34;, modelName : \u0026#34;Driving Test MCQ - Simplified\u0026#34;, fields : { \u0026#34;QuestionStem\u0026#34; : extractedQuestionText, \u0026#34;OptionA\u0026#34; : extractedOptions[0] || \u0026#34;\u0026#34;, \u0026#34;OptionB\u0026#34; : extractedOptions[1] || \u0026#34;\u0026#34;, // ... \u0026#34;CorrectAnswer\u0026#34; : extractedCorrectAnswerLetters, // e.g., \u0026#34;A\u0026#34;, \u0026#34;BC\u0026#34;, \u0026#34;ACD\u0026#34; \u0026#34;DetailedExplanation\u0026#34; : extractedRemarkText } , tags: originalTagsArray } The final step in the core loop is adding the new note to Anki. We invoke AnkiConnect\u0026rsquo;s addNote action, passing the meticulously constructed note data object from the previous step as an argument. AnkiConnect will then process this request and create a new, clean, and properly structured card in Anki. With this, the demystification and data migration (or restructuring) process for a single source note is complete.\nAuxiliary Features When designing and implementing automated scripts for Anki card demystification, beyond the core logic of data extraction and transformation, it\u0026rsquo;s prudent to incorporate certain auxiliary features to enhance the script\u0026rsquo;s robustness and user experience. Among these, a comprehensive error handling and logging mechanism is indispensable. Throughout the process of iterating over and processing each card, various unforeseen errors can occur due to the involvement of multiple components such as file I/O, network communication (via AnkiConnect), browser automation (via Puppeteer), and DOM parsing (via JSDOM). Examples include network connection interruptions, Puppeteer operation timeouts, or failures in JSDOM parsing due to an inability to find expected DOM elements. Therefore, within the main processing loop, the handling of each note should be encapsulated within a try...catch block. Upon catching an exception, the script should ideally not terminate immediately. Instead, it should log the ID of the note that caused the error, along with detailed error information (including error type, message, and potentially a stack trace) to a dedicated log file. The advantage of this approach is that even if some cards fail to process, the script can continue attempting to process the remaining ones. After the entire batch is finished, the user can review the log file to identify problematic cards and perform targeted troubleshooting or necessary manual intervention. Furthermore, for certain predictable, non-critical \u0026ldquo;minor issues,\u0026rdquo; such as a source note missing a non-essential field, we can opt to log a warning message and gracefully skip processing that particular note, rather than halting the entire script due to such minor imperfections.\nOn the other hand, providing clear progress indicators and an estimated time of arrival (ETA) is equally important for improving the user experience, especially when dealing with large decks containing hundreds or thousands of cards, as the entire automation process can be quite time-consuming. If the script provides no feedback during its execution, users might become anxious or uncertain about whether it is still running correctly. To mitigate this, we can output real-time processing progress to the console, for instance, by displaying messages like \u0026ldquo;Processing note X / Y\u0026hellip;\u0026rdquo;, where X is the number of notes processed so far, and Y is the total number of notes. Taking this a step further, we can also dynamically estimate the remaining processing time (ETA) based on the average time taken to process the notes completed thus far. A practical way to do this is to record the total time elapsed since the script began processing notes. After each note is processed, calculate the average processing time per note (total elapsed time / number of notes processed) and then multiply this average by the number of remaining notes. This yields a rough estimate of the time still required. Presenting this ETA information (e.g., formatted as \u0026ldquo;Estimated time remaining: HH:MM:SS\u0026rdquo;) alongside the progress update gives the user a clear expectation and makes the waiting period less opaque.\nExperience with Typical Templates In previous discussions and practical applications, we\u0026rsquo;ve encountered several representative types of Anki card templates, each presenting distinct challenges during the demystification process. Taking a **political review template ** (pol2.html) as an example, its primary characteristic was relatively straightforward data substitution, where card content was largely populated by filling Anki field placeholders within the HTML structure. However, the template\u0026rsquo;s complexity was concentrated in its JavaScript component, particularly in how it handled options. The \u0026ldquo;Options\u0026rdquo; field in the source data was typically a single string with options concatenated by a specific delimiter (e.g., A. xxx||B. yyy). The template\u0026rsquo;s internal JavaScript was responsible for parsing this string, dynamically rendering it into multiple distinct \u0026lt;div class=\u0026quot;option\u0026quot;\u0026gt; HTML elements, each corresponding to a single choice. Consequently, when automating the processing of such templates, the critical factor was ensuring that Puppeteer could correctly and completely execute this client-side JavaScript. Once the JavaScript execution finished and the DOM structure was updated, JSDOM could then be used to extract the specific text content of each option from the rendered HTML, and to determine the correct answer(s) by inspecting the CSS classes applied to the option elements. Additionally, the display logic for the \u0026ldquo;Explanation\u0026rdquo; section in this template might also be controlled by JavaScript. In such cases, using waitForSelector to wait for CSS class names indicating option states (like highlighting) to appear often indirectly ensures that the explanation content (if loaded synchronously or immediately after the option logic) has also been correctly rendered onto the page, making it available for JSDOM to capture.\nAnother category of templates, exemplified by the driving test question bank template (jiazhao.html), introduced a higher degree of complexity, primarily due to its use of libraries like Persistence.js. As mentioned earlier in the technical stack overview, Persistence.js (or similar libraries) is commonly used to store user preferences or card states within the Anki WebView\u0026rsquo;s session, such as whether the user prefers options to be randomized, or if the \u0026quot; Explanation\u0026quot; section should be displayed by default when the back of the card is revealed. The main challenge posed by this mechanism is that if our automated script attempts to directly render a template containing the complete front and back logic (or only the back part, expecting it to be in an \u0026ldquo;answer revealed\u0026rdquo; state) without first establishing the session state that Persistence.js relies upon (particularly crucial settings like ANKI-SETTINGS-HIDE-NOTES), then the JavaScript on the back of the template (e.g., a prepareNotes() function) might not inject the \u0026ldquo;Explanation\u0026rdquo; content into the designated DOM container (such as .notes-container) because it cannot read the expected setting value. This would directly prevent us from extracting the \u0026ldquo;Explanation\u0026rdquo; information using JSDOM later.\nThe core solution for such templates that depend on session storage is to leverage Puppeteer\u0026rsquo;s page.evaluateOnNewDocument() method. This powerful API allows us to inject custom JavaScript code into the target HTML page before any of its native scripts are executed. We can use this opportunity to create a mock implementation of the Persistence object within the page\u0026rsquo;s context. This mock object needs to emulate the key API interfaces provided by the real Persistence.js library, such as isAvailable(), getItem(), and setItem(). By using this mock object, we can proactively call Persistence.setItem('ANKI-SETTINGS-HIDE-NOTES', '0');, thereby \u0026ldquo;tricking\u0026rdquo; the card\u0026rsquo;s back-side script into believing that the user has set the preference to display explanations. As a result, the template\u0026rsquo;s JavaScript logic will render the \u0026ldquo;Explanation\u0026rdquo; content into the DOM as intended, enabling JSDOM to extract it successfully.\nFurthermore, another noteworthy detail concerning the driving test template is the handling of option order. Its front template\u0026rsquo;s showFrontOptions function might include logic to randomize the display order of options, storing this randomized sequence (typically as a comma-separated string of the options\u0026rsquo; original indices, e.g., 2,1,4,3) in a Persistence key named ANKI-OPTIONS-ORDER. The getOptionObjs function on the card\u0026rsquo;s back template then reads this stored order when rendering options and highlighting the correct answer, to ensure that the option text correctly corresponds to its original answer identifier (e.g., numbers 1, 2, 3, 4 mapping to A, B, C, D). In our render function, because we usually render the entire HTML template (potentially with merged front and back logic) at once using Puppeteer, and because we can preset a deterministic, non-random option order (e.g., simply setting it to '1,2,3,4', representing display in the original order) for ANKI-OPTIONS-ORDER during the evaluateOnNewDocument phase, this guarantees a stable and predictable relationship between the option content and its correctness evaluation during the back-card rendering. This facilitates the accurate extraction of formatted options and answers.\nPractical Considerations and Future Outlook When actually writing and applying Anki card demystification scripts, adhering to certain key **practical considerations ** can significantly enhance work efficiency and the reliability of the results. Foremost among these is thorough template analysis. Before rushing into coding, it is imperative to dedicate sufficient time within the Anki environment, utilizing browser developer tools, to deeply dissect the target card template\u0026rsquo;s HTML structure, the dynamic changing patterns of CSS class names, and, crucially, the execution logic of its JavaScript. A full comprehension of how data flows and is transformed within the template is fundamental to subsequently programming precise rendering logic in Puppeteer and accurate extraction rules in JSDOM.\nSecondly, an iterative approach to building and testing is a highly effective strategy for managing complexity. It is advisable to break down the entire demystification process into several independently verifiable modules or steps. For instance, one might first focus on correctly reading the local HTML template file and ensuring accurate substitution of field values (obtained from AnkiConnect) into the template\u0026rsquo;s placeholders. Once this is achieved, the Puppeteer component can be tested to confirm its ability to load the data-filled HTML, execute the embedded JavaScript correctly, and output the final rendered HTML to the Node.js console. Building on this, JSDOM parsing and data extraction functions can be developed and tested in isolation, using the HTML string output by Puppeteer as input, to validate the precise extraction of elements like the question, individual options, correct answers, and explanations. Only after these core data processing stages have been thoroughly debugged should one integrate the AnkiConnect APIs for a complete end-to-end test of reading source notes from Anki and writing new notes back. This divide-and-conquer, incremental iteration methodology facilitates rapid problem identification and reduces debugging complexity. Furthermore, when crafting JSDOM extraction logic, striving for robust CSS selectors is a factor deserving special attention. To make the script as resilient as possible to minor future modifications in the source card template, one should prioritize using ID selectors (if unique IDs are provided for key elements in the template), as IDs generally offer high stability. In the absence of IDs, an attempt should be made to use sufficiently specific and unique combinations of class names, or to construct CSS selectors incorporating tag names, attributes, and other features to minimize the risk of the extraction script failing due to the template author altering an unrelated class name or HTML hierarchy. It\u0026rsquo;s best to avoid overly generic selectors or those heavily reliant on deep DOM level nesting.\nRegarding the content extracted from cards, it\u0026rsquo;s necessary to carefully differentiate and handle the boundary between HTML and plain text. Anki fields such as questions, options, and explanations may inherently contain HTML tags, for example, \u0026lt;img\u0026gt; tags for images, \u0026lt;br\u0026gt; for line breaks, or \u0026lt;strong\u0026gt; and \u0026lt;em\u0026gt; for text emphasis. When extracting the text from these fields using JSDOM, a decision must be made whether the target field ultimately requires plain text or should retain some or all of the HTML formatting. For plain text, the element\u0026rsquo;s textContent property can be used; to preserve HTML structure, innerHTML should be employed. This choice should be guided by how your new card template is designed to render these fields. Additionally, for Anki-specific placeholders like {{cloze:FieldName}}, if the target note type is also a cloze deletion type, this Anki-recognized format should be preserved when constructing the field data for the new note. However, if the corresponding field in the target note type is just a regular text field, then you might need to extract either the elided portion from the source text (i.e., the content between c1:: and }}) or the full text after removing the cloze markers.\nGiven that the entire demystification workflow heavily relies on asynchronous operations—such as file I/O (e.g., using fs/promises), Puppeteer\u0026rsquo;s page loading and interactions, and AnkiConnect\u0026rsquo;s network requests—proficient and correct management of asynchronous operations is crucial. In modern JavaScript/TypeScript development, a_sync/await_ syntax should be preferentially used for handling Promises, as this makes the logic of asynchronous code more closely resemble the intuitive flow of synchronous code, thereby greatly enhancing code readability and maintainability. It is imperative to ensure that all asynchronous operations are properly await-ed to guarantee that steps execute in the intended sequence, thus avoiding elusive logical errors that can arise from improperly handled asynchronous callbacks.\nIn terms of resource management, an often-overlooked detail is prompt resource cleanup. Each time Puppeteer is launched, it creates a browser instance that consumes system resources. Therefore, within the script\u0026rsquo;s try...finally blocks, or at least before the script terminates, it is essential to ensure that the browser.close() method is called to shut down the Puppeteer-created browser instance. This releases the memory and processes it was using, preventing resource exhaustion issues that could arise from long-running scripts or multiple script executions.\nFinally, concerning the use of temporary files, writing the data-filled HTML content to a local temporary file and then having Puppeteer load this file via the file:// protocol is a simple and effective strategy. The benefits include avoiding the need to pass excessively long HTML strings directly to Puppeteer and facilitating debugging by allowing easy inspection of the actual page content that Puppeteer is loading. After processing is complete, one might consider adding logic to clean up these temporary files. Although in most scenarios, scripts will overwrite the same temporary file on each note\u0026rsquo;s processing, so active cleanup might not be strictly necessary to prevent disk space issues, maintaining a tidy environment is generally good practice.\nAdhering to these considerations and techniques will contribute to a smoother and more reliable execution of Anki card demystification tasks.\nIn summary, by skillfully combining Puppeteer\u0026rsquo;s dynamic rendering capabilities with JSDOM\u0026rsquo;s robust DOM parsing, we can effectively \u0026ldquo;unwrap\u0026rdquo; complex Anki cards that rely on JavaScript for content generation. The crux lies in understanding the source card\u0026rsquo;s rendering mechanisms and then either precisely simulating or appropriately bypassing these mechanisms within the Puppeteer environment to obtain the final, structured HTML. For libraries like Persistence.js that maintain state across an Anki WebView session, leveraging Puppeteer\u0026rsquo;s page.evaluateOnNewDocument method to perform necessary environment mocking is a key technique to ensure that target content, such as the \u0026quot; Explanation\u0026quot; on the card\u0026rsquo;s back, is correctly rendered and becomes extractable.\nThis demystification process not only provides users with an effective means to extract and migrate valuable learning data but also serves as an excellent opportunity to deepen one\u0026rsquo;s understanding of web front-end technologies (HTML, CSS, JavaScript, DOM interaction) and advanced Anki template customization mechanisms. While this article has offered a general framework and solutions to specific template challenges, every Anki card template can possess its unique intricacies and complexities. Therefore, when tackling any specific demystification task, patient and meticulous analysis, rigorous step-by-step debugging, and the flexibility to adapt to actual circumstances are indispensable qualities for achieving success.\nLooking ahead, once the aforementioned demystification workflows and technical methods are further refined and abstracted, they hold considerable potential to be encapsulated into more universal, user-friendly tools or libraries. Such tools could significantly lower the technical barrier for a_verage_ Anki users to manage complex card templates. Furthermore, these techniques could be integrated into larger Anki auxiliary management systems or add-ons, thereby providing Anki users with even more powerful capabilities for managing and repurposing their knowledge bases, ultimately enhancing Anki\u0026rsquo;s effectiveness as a personalized learning platform.\n","permalink":"https://blog.tategotoazarasi.me/en/posts/delving-into-anki-cards-demystifying-templates-for-data-extraction-and-practical-application/","summary":"Uncover techniques to demystify complex Anki card templates using Puppeteer and JSDOM for accurate data extraction from dynamically rendered content and facilitate migration.","title":"Delving into Anki Cards: Demystifying Templates for Data Extraction and Practical Application"},{"content":"Today I want to share a somewhat special programming project – one not born out of a desire for flashy tech or commercial application, but from a simple idea: helping my mom lighten her workload a bit.\nMy mother is a dedicated teacher, and with the rise of online education, much of her work, including grading assignments, has moved online. While this undoubtedly increases teaching flexibility, it also brings new challenges. Grading homework on certain online platforms, in particular, involves a significant amount of repetitive actions, which is both time-consuming and mentally draining. Seeing her often busy late into the night, as her son, I always thought about whether I could use the technical skills I\u0026rsquo;ve learned to do something for her.\nThe specific target this time was the grading process for \u0026ldquo;short-answer questions\u0026rdquo; on her teaching platform. These types of questions typically require the teacher to read the student\u0026rsquo;s response, then assign a score and provide written feedback. While the final judgment and personalized feedback are irreplaceable, there seemed to be room for automation in the initial scoring and basic comment generation.\nThus began an exploratory journey combining browser scripting, DOM manipulation, API calls, and a touch of artificial intelligence.\nStarting with the Browser Console Getting started is always the hardest part. The most direct thought was: \u0026ldquo;Can I run some code in the browser to simulate mouse clicks and keyboard input?\u0026rdquo; The answer is yes. The browser\u0026rsquo;s developer tools (opened by pressing F12) and specifically the \u0026ldquo;Console\u0026rdquo; tab are powerful weapons for this.\nThe first step in automation is teaching the code to \u0026ldquo;recognize\u0026rdquo; the elements on the page. Just as a person grading needs to find the question, the answer box, the score field, the comment area, and the submit button, the code needs specific \u0026ldquo;addresses\u0026rdquo; – DOM selectors – to locate these elements.\nOpening a typical assignment grading page, the initial exploration involved carefully studying its HTML structure using the developer tools. We discovered some consistent design patterns: the content and response area for each short-answer question were usually entirely wrapped within a list item \u0026lt;li\u0026gt; element. To differentiate question types or provide unique identifiers, these \u0026lt;li\u0026gt; elements often carried specific attributes, such as data-questiontype=\u0026quot;5\u0026quot; perhaps marking it as a short-answer question, along with a unique id attribute.\nInside each \u0026lt;li\u0026gt; representing a question, we could find an \u0026lt;input type=\u0026quot;number\u0026quot;\u0026gt; tag used for entering the score. This input field not only revealed the maximum possible score for the question via its max attribute but also hinted at how backend data processing might be indexed through its name attribute (often formatted like questions[0].studentScore).\nThe commenting functionality appeared more dynamic. Initially, only a \u0026ldquo;Comment\u0026rdquo; button was visible, typically implemented as a \u0026lt;span\u0026gt; tag (perhaps with a class like modify). A user click on this button would dynamically reveal the text area for entering the comment – usually a \u0026lt;textarea\u0026gt; element (possibly with classes like teacherWrite comments) – along with a \u0026ldquo;Done\u0026rdquo; or \u0026ldquo;Confirm\u0026rdquo; button (maybe a \u0026lt;div\u0026gt; tag) to finalize the comment input.\nFinally, near the bottom of the page, there was generally a global action button, such as \u0026ldquo;Submit and Grade Next,\u0026rdquo; designed to save all the grading results (scores and comments) for the current page in one go and then load the next assignment requiring attention. Locating and understanding these key elements formed the foundation for the subsequent automation script design.\nWith the interactive page elements identified, the basic automation workflow began to take shape. First, the script needed to identify all the short-answer questions on the page requiring attention. This could be achieved using the document.querySelectorAll method combined with the selectors identified earlier (like li[data-questiontype=\u0026quot;5\u0026quot;]), yielding a list of all the relevant \u0026lt;li\u0026gt; elements.\nNext, the script would need to process each question element in this list sequentially, typically involving a loop. Within each iteration of the loop, focusing on the current question, the script would perform two core tasks: filling in the score and adding a comment.\nFor scoring, the initial idea was to locate the score input field within the current question element, read its max attribute to get the maximum possible score, and then directly set the input\u0026rsquo;s value to this maximum score. This was conceived as a basic starting strategy, open to later refinement with more complex logic, but viable as a starting point.\nAdding the comment was slightly more complex due to the dynamic nature of the elements involved. The script would first need to find and simulate a click on the \u0026ldquo;Comment\u0026rdquo; button. Crucially, after the click, it couldn\u0026rsquo;t immediately search for the comment box; a waiting period was essential because the comment input area and the \u0026ldquo;Done\u0026rdquo; button were dynamically loaded or displayed, requiring time for the page to react. Once the wait was over, the script would locate the newly appeared \u0026lt;textarea\u0026gt; element for comments and set its value to a predefined, generic comment text. Finally, it would find and simulate a click on the corresponding \u0026ldquo;Done\u0026rdquo; button to confirm the entry of this comment.\nTo enhance the stability of the automation process and better mimic human interaction patterns, incorporating a short delay after completing all steps for one question was deemed necessary. This pause helps prevent issues caused by excessively rapid operations that might outpace the page\u0026rsquo;s script responsiveness or potentially trigger anti-bot mechanisms on some websites.\nAfter processing all questions according to this flow, the logical final step would be to simulate a click on the global submit button at the bottom of the page to save all the grading results. However, considering the risks associated with full automation and the importance of allowing the teacher a final review opportunity, this final submission step was initially designated as optional or to be triggered manually by the user. This preliminary plan, primarily relying on direct DOM manipulation, laid the groundwork for the subsequent coding implementation.\nThis initial concept relied heavily on direct DOM manipulation (finding an element -\u0026gt; modifying its attributes/triggering clicks). Within the browser console, this is often feasible.\n// Pseudocode Example: Initial Concept function gradeShortAnswer(questionElement) { // Find score input and set to max score const scoreInput = questionElement.querySelector(\u0026#39;input.student-score\u0026#39;); const maxScore = scoreInput?.max; if (scoreInput \u0026amp;\u0026amp; maxScore) { scoreInput.value = maxScore; console.log(`Set score for ${questionElement.id} to ${maxScore}`); // Trigger events so the page knows the value changed (Important!) scoreInput.dispatchEvent(new Event(\u0026#39;input\u0026#39;, {bubbles: true})); scoreInput.dispatchEvent(new Event(\u0026#39;change\u0026#39;, {bubbles: true})); } // Find and click the \u0026#34;Comment\u0026#34; button const commentButton = questionElement.querySelector(\u0026#39;.comment span.modify\u0026#39;); if (commentButton) { commentButton.click(); // Need to wait for the comment box to appear... setTimeout(() =\u0026gt; { const commentArea = questionElement.querySelector(\u0026#39;.comment textarea.teacherWrite\u0026#39;); const confirmButton = questionElement.querySelector(\u0026#39;.comment div.confirm\u0026#39;); if (commentArea \u0026amp;\u0026amp; confirmButton) { commentArea.value = \u0026#34;Student\u0026#39;s response is good!\u0026#34;; // Preset comment confirmButton.click(); console.log(`Comment added for ${questionElement.id}`); } }, 1000); // Assuming a 1-second wait } } // Get all short-answer questions and process // document.querySelectorAll(\u0026#39;#shiti-content li.subjective[data-questiontype=\u0026#34;5\u0026#34;]\u0026#39;) // .forEach(el =\u0026gt; gradeShortAnswer(el)); // Note: Real application requires more complex async handling and error management This approach seems appealing, but in practice, especially on complex, dynamically loaded modern web pages, it often encounters its first major roadblock.\nExploring the API Route (and Rich Text Editor Pitfalls) In the context of real-world teaching platforms (including both the discussion forum scenarios explored earlier and the current homework grading task), the comment input field is frequently not a simple \u0026lt;textarea\u0026gt;. Instead, it\u0026rsquo;s often a * *Rich Text Editor (RTE)**, such as the familiar UEditor, CKEditor, or TinyMCE.\nThese editors typically render a complex structure, often involving an \u0026lt;iframe\u0026gt; or a \u0026lt;div\u0026gt; with the contenteditable attribute, in place of the original \u0026lt;textarea\u0026gt; (or sometimes even a \u0026lt;script\u0026gt; tag), and provide a toolbar for formatting.\nHowever, this assumption of directly manipulating a simple \u0026lt;textarea\u0026gt; encounters challenges when faced with the Rich Text Editors commonly used on these platforms. The introduction of these editors complicates the seemingly straightforward interaction. Firstly, the DOM structure itself changes. We can no longer directly target a simple text input; instead, we must navigate into the more complex structure generated by the editor, such as an embedded \u0026lt;iframe\u0026gt; element, and then further locate the editable \u0026lt;body\u0026gt; within that iframe, or perhaps a specific \u0026lt;div\u0026gt; element configured with the contenteditable attribute.\nSecondly, and more critically, is the dependency on the editor\u0026rsquo;s API. Rich Text Editors usually have their own set of JavaScript APIs to manage content and state. If we bypass these APIs and directly modify the innerHTML of the \u0026lt;iframe\u0026gt; or the innerText of a contenteditable element, the editor\u0026rsquo;s internal state might not update accordingly. The direct consequence is that when a save action is triggered (like clicking the \u0026ldquo;Done\u0026rdquo; button), the editor might still consider the content empty or unchanged because it relies on its API calls to synchronize and retrieve the final content (often synchronizing it to a hidden form field). Therefore, merely altering the visual presentation doesn\u0026rsquo;t guarantee the data will be captured correctly.\nFinally, instance management also becomes a factor. On a page containing multiple short-answer questions, each question\u0026rsquo;s comment box is typically an independent instance of the Rich Text Editor. This means if we want to interact via API, we must be able to accurately identify and obtain the specific instance object for the editor corresponding to the question currently being processed. Only then can we call its specific methods (like setting content or focusing). This undoubtedly increases the complexity of the automation script. The emergence of these issues indicated that direct DOM manipulation might be insufficient for handling RTE scenarios, necessitating an exploration into using the editor\u0026rsquo;s API.\nFacing the challenges posed by Rich Text Editors, the natural next step was to attempt interaction using the editor\u0026rsquo;s own provided API, which is generally the more standardized and reliable method. Taking the common UEditor as an example, the standard operational workflow typically involves these steps: First, identify the unique identifier of the target editor container within the page\u0026rsquo;s DOM. This ID is usually associated with the element (sometimes a \u0026lt;script\u0026gt; tag, sometimes the outermost \u0026lt;div\u0026gt; rendered by the editor) that the editor was initialized upon. Once this ID is obtained, one can call UEditor\u0026rsquo;s global method UE.getEditor('editorID') to retrieve the JavaScript instance object for that specific editor. With this instance object in hand, various methods can be invoked, such as using editorInstance.setContent('your comment HTML', false) to set the editor\u0026rsquo;s content (where the second argument false usually means overwriting existing content), or calling editorInstance.focus() to bring focus to the editor. However, before invoking methods that manipulate content, it\u0026rsquo;s crucial to ensure the editor has fully initialized and is ready to accept API calls. This is typically achieved using the editorInstance.ready(callback) method, placing the actual content manipulation code within the provided callback function to avoid errors caused by calling APIs on an incompletely loaded editor.\nThis standard API workflow sounds quite robust and sufficient for interacting with Rich Text Editors. However, theory and practice sometimes diverge. In our previous automation explorations, both in discussion forum scenarios and during the current homework grading attempt, trying to interact with UEditor instances via the API led to unexpected and perplexing difficulties. The primary issue encountered was that editor instance registration seemed delayed or even failed entirely. Using script logic, we could accurately locate the editor\u0026rsquo;s container \u0026lt;div\u0026gt; element rendered on the page, for instance, one with the ID edui78. But subsequent attempts to fetch the instance for this ID using UE.getEditor('edui78') frequently returned null or undefined, indicating failure. To investigate further, we examined UEditor\u0026rsquo;s global object UE.instants, which manages all initialized instances, only to be surprised that the ID edui78 we found was not listed in this global registry at all! This implied that although the editor was visually rendered and present on the page, its corresponding JavaScript control instance wasn\u0026rsquo;t being registered in the expected manner, preventing us from accessing it through the official API.\nAdditionally, we sometimes encountered ID mismatch issues. In certain situations, the ID under which the editor instance was actually registered in UE.instants did not match the ID of the outermost container \u0026lt;div\u0026gt; rendered on the page. This meant that even if an instance was successfully registered, we might fail to retrieve it because we were using the incorrect ID derived from the visible DOM element, further complicating and adding uncertainty to automation attempts via the API. These practical pitfalls made the seemingly ideal API approach fraught with difficulty.\nAfter multiple attempts involving increased delays, trying different selectors, and inspecting UE.instants, the conclusion was clear: in this specific dynamically loaded context, relying on the UEditor API to inject comments was unreliable. The editor\u0026rsquo;s initialization process likely involved some specific mechanism or perhaps a bug that prevented us from consistently obtaining and controlling the target comment box instance.\n// Pseudocode Example: Failed API Attempt async function tryApiComment(questionElement, commentHtml) { const commentContainer = questionElement.querySelector(\u0026#39;.comment\u0026#39;); const editorDiv = commentContainer?.querySelector(\u0026#39;div.edui-editor[id^=\u0026#34;edui\u0026#34;]\u0026#39;); if (!editorDiv || !editorDiv.id) { console.error(\u0026#34;Cannot find editor div\u0026#34;); return; } const editorId = editorDiv.id; // e.g., \u0026#34;edui78\u0026#34; console.log(`Attempting to get editor instance: ${editorId}`); // **** THE PROBLEM AREA **** // Often failed because \u0026#39;edui78\u0026#39; wasn\u0026#39;t registered in UE.instants // or UE.getEditor returned undefined/null even if the div existed. let editorInstance; try { editorInstance = UE.getEditor(editorId); // \u0026lt;--- Fails or returns unusable instance if (!editorInstance || typeof editorInstance.setContent !== \u0026#39;function\u0026#39;) { throw new Error(\u0026#34;Instance invalid or not ready\u0026#34;); } } catch (e) { console.error(`Failed to get or validate UE instance ${editorId}:`, e); return; } // **** END PROBLEM AREA **** // Code below here would likely not be reached or would fail await new Promise(resolve =\u0026gt; editorInstance.ready(resolve)); editorInstance.setContent(commentHtml, false); // ... click confirm ... } With the API route seemingly blocked, the only option left was to return to the more \u0026ldquo;primitive\u0026rdquo; method.\nRevisiting DOM Manipulation After repeatedly hitting roadblocks while trying to use the editor\u0026rsquo;s API, and facing an editor instance that seemed uncontrollable through standard means, we had to reconsider our initial approach and return to the path of direct DOM manipulation. This time, however, we needed to learn from past mistakes and make adjustments based on our understanding of how Rich Text Editors typically function.\nFirst, the target of manipulation needed to be more precise. We knew that the final content displayed by an RTE usually resides within an \u0026lt;iframe\u0026gt; element. Therefore, the script\u0026rsquo;s core task shifted from trying to find and manipulate a potentially non-existent or inaccessible \u0026lt;textarea\u0026gt; to accurately locating this \u0026lt;iframe\u0026gt;. It then needed to delve deeper into its contentDocument to find the actual \u0026lt;body\u0026gt; element (often marked with a class like view and having its contentEditable attribute set to true) which holds the content and allows user editing.\nSecond, the issue of data synchronization had to be addressed. Since we couldn\u0026rsquo;t reliably find and update the hidden \u0026lt;textarea\u0026gt; that, in theory, should exist for form submission (it either didn\u0026rsquo;t appear as expected or was too deeply buried by the editor\u0026rsquo;s complex mechanisms), we had to make a critical, albeit risky, assumption. We assumed that when the user (or our script) clicks the \u0026ldquo;Done\u0026rdquo; button associated with the comment box, the page\u0026rsquo;s own JavaScript logic bound to that button reads the current content directly from the \u0026lt;iframe\u0026gt;\u0026rsquo;s inner \u0026lt;body\u0026gt; element, rather than from the elusive \u0026lt;textarea\u0026gt;. This content would then be used for subsequent processing, such as saving or submitting the comment. This was undoubtedly an assumption based on observation and reverse-engineering guesswork, but given the inability to gain control via the editor\u0026rsquo;s instance, it seemed the only viable path forward, albeit tinged with a degree of uncertainty. This assumption allowed us to bypass the API and simulate comment input by directly modifying the \u0026lt;iframe\u0026gt;\u0026rsquo;s content.\nBased on this key assumption – that the page\u0026rsquo;s \u0026ldquo;Done\u0026rdquo; button reads directly from the \u0026lt;iframe\u0026gt; content – we readjusted the core automation workflow. The general sequence of steps for the script became: first, as before, identify all the short-answer list item \u0026lt;li\u0026gt; elements on the page; next, iterate through these questions sequentially. For each question, initially locate its score input field, set the desired score (e.g., defaulting to the maximum), and ensure relevant update events are triggered. Then comes the crucial step: find and simulate a click on that question\u0026rsquo;s \u0026quot; Comment\u0026quot; button. After clicking, it\u0026rsquo;s vital to patiently wait, allowing the page sufficient time to dynamically load or display the Rich Text Editor\u0026rsquo;s \u0026lt;iframe\u0026gt; along with the adjacent \u0026ldquo;Done\u0026rdquo; button. Once these elements appear, the script concentrates on finding the target \u0026lt;iframe\u0026gt; – typically nested within the editor\u0026rsquo;s main container \u0026lt;div\u0026gt; ( perhaps with a class like edui-editor) and possibly having an ID following a pattern (like ueditor_X). Upon successfully locating the \u0026lt;iframe\u0026gt;, the script accesses its contentDocument.body property to get the internal editable area, and then directly sets the innerHTML property of this area to our predefined comment text. The final action within the loop is to find and simulate a click on the \u0026ldquo;Done\u0026rdquo; button, thereby triggering the page\u0026rsquo;s own logic for saving the comment. Naturally, after completing all these steps for one question, incorporating an appropriate delay before proceeding to the next remains essential for process stability.\n// Pseudocode Example: Revised DOM Manipulation (Iframe Only) async function commentViaIframe(questionElement, commentHtml) { const commentButton = questionElement.querySelector(\u0026#39;.comment span.modify\u0026#39;); if (!commentButton) return; commentButton.click(); await delay(1000); // Wait for iframe etc. const commentContainer = questionElement.querySelector(\u0026#39;.comment\u0026#39;); const editorDiv = commentContainer?.querySelector(\u0026#39;div.edui-editor\u0026#39;); const editorIframe = editorDiv?.querySelector(\u0026#39;iframe[id^=\u0026#34;ueditor_\u0026#34;]\u0026#39;); const confirmButton = commentContainer?.querySelector(\u0026#39;div.confirm\u0026#39;); if (editorIframe \u0026amp;\u0026amp; confirmButton) { const iframeDoc = editorIframe.contentDocument || editorIframe.contentWindow?.document; if (iframeDoc?.body) { iframeDoc.body.innerHTML = commentHtml; // Set content directly console.log(`Set iframe content for ${questionElement.id}`); confirmButton.click(); // Trigger the page\u0026#39;s own logic console.log(`Clicked confirm for ${questionElement.id}`); } else { console.error(\u0026#34;Could not access iframe body for \u0026#34; + questionElement.id); } } else { console.error(\u0026#34;Could not find iframe or confirm button for \u0026#34; + questionElement.id); } } Surprisingly, this approach of \u0026ldquo;modify only the iframe, ignore the textarea\u0026rdquo; actually worked on our target page! This implied that the \u0026ldquo;Done\u0026rdquo; button\u0026rsquo;s click event handler indeed read the content from the \u0026lt;iframe\u0026gt; to save the comment, without requiring us to manually sync the mysterious (or perhaps non-existent) \u0026lt;textarea\u0026gt;. It was a welcome breakthrough.\nIntroducing AI for Personalized Comments Having solved the basic operational simulation, the next goal was to make the comments less repetitive. Generating comments based on the student\u0026rsquo;s specific answer would make the process more intelligent.\nThis naturally led to considering Large Language Models (LLMs). Several excellent models are available, including Baidu\u0026rsquo;s Ernie series in China. They provide APIs that allow sending requests (containing information like the question, student answer, etc.) to receive model-generated content, such as the comments we needed.\nChoosing the Model Considering the potential need to process numerous questions during grading, response speed was a requirement. At the same time, the task of generating a short comment is relatively simple. We opted for the Ernie Speed model from the Ernie family, as it offered a good balance between speed and performance for this task.\nAPI Call Workflow To interact with the AI model, a typical API call process involves two main steps. The first is obtaining an authorization credential, known as an access_token. This requires making a request to the platform\u0026rsquo;s OAuth authentication endpoint, passing along the API Key (AK) and Secret Key (SK) obtained from the platform registration. Upon successful authentication, the endpoint returns an access_token string, which usually has an expiration period ( e.g., 30 days). This token acts like a temporary passport, credentialing subsequent interactions with the specific AI model services.\nOnce a valid access_token is acquired, the second step is to call the specific AI model\u0026rsquo;s dialogue interface, such as the Ernie Speed Chat API. This is typically a POST request sent to an endpoint URL that includes the obtained access_token as a parameter. The request body must be structured according to the API\u0026rsquo;s specifications. The core component is the messages field, an array representing the conversation history, which must contain at least one message with role: \u0026quot;user\u0026quot;. The content of this user message is our carefully crafted prompt, containing the question, student\u0026rsquo;s answer, and instructions for the AI comment generation. Additionally, an optional system field can be included to define the AI\u0026rsquo;s role and provide global instructions (e.g., \u0026ldquo;You are a university TA generating comments\u0026rdquo;). Depending on the need, other parameters like temperature or top_p can be added to control the diversity and randomness of the generated output. Upon receiving the request, the AI model processes the input based on the messages and system prompt and returns the generated result.\nCross-Origin Issues (CORS) Attempting to call external APIs (like https://aip.baidubce.com) directly from browser console scripts or standard webpage JavaScript using fetch or XMLHttpRequest immediately runs into CORS (Cross-Origin Resource Sharing) issues. For security reasons, browsers block such cross-domain requests by default, unless the target server explicitly permits them via specific response headers (like Access-Control-Allow-Origin). Baidu\u0026rsquo;s API endpoints, being primarily designed for server-to-server communication, usually do not send the necessary headers to allow direct requests from arbitrary web origins.\nThe browser console will explicitly state the block:\nAccess to fetch at \u0026#39;https://aip.baidubce.com/...\u0026#39; from origin \u0026#39;http://...\u0026#39; has been blocked by CORS policy... The Tampermonkey Solution Userscript managers like Tampermonkey provide a privileged channel: the GM_xmlhttpRequest function. Code running within a userscript environment can use this function to make cross-domain requests because the request originates from the browser extension itself, not the webpage\u0026rsquo;s restricted context, thus bypassing standard CORS limitations.\nHowever, using this powerful function requires attention to a few key details to ensure the script works correctly and has the necessary permissions. Firstly, explicit authorization must be declared in the script\u0026rsquo;s metadata block (the section starting with // ==UserScript== and ending with // ==/UserScript==). A line // @grant GM_xmlhttpRequest must be included, essentially informing Tampermonkey: \u0026ldquo;This script needs permission to make cross-domain requests.\u0026rdquo; Without this declaration, attempts to call the function will fail.\nSecondly, for security purposes, Tampermonkey usually requires the script to explicitly declare the external domains it intends to connect to. Therefore, within the same metadata block, a declaration like // @connect aip.baidubce.com is needed, specifying that the script will communicate with Baidu\u0026rsquo;s AI platform API server. This declaration helps users understand the script\u0026rsquo;s network activity and allows the script manager to enforce finer-grained permissions.\nLastly, it\u0026rsquo;s crucial to understand that GM_xmlhttpRequest is inherently an asynchronous operation. This means that after initiating a request, the script execution doesn\u0026rsquo;t pause to wait for the response; it continues with the subsequent code. Consequently, handling the request\u0026rsquo;s outcome requires using asynchronous programming patterns. Common approaches include providing callback functions to GM_xmlhttpRequest—such as onload for successful responses, onerror for network or other errors, and ontimeout for timeouts. A more modern and often more manageable approach for complex workflows involves wrapping the GM_xmlhttpRequest call within a JavaScript Promise object, which then allows the use of async/await syntax. This makes the asynchronous code appear more like synchronous code, leading to clearer logic for sending requests and processing their results.\n// Pseudocode Example: Using GM_xmlhttpRequest for Token function getAccessTokenGM(apiKey, secretKey) { const url = `https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials\u0026amp;client_id=${apiKey}\u0026amp;client_secret=${secretKey}`; return new Promise((resolve, reject) =\u0026gt; { GM_xmlhttpRequest({ method : \u0026#34;POST\u0026#34;, url : url, headers : {\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Accept\u0026#34;: \u0026#34;application/json\u0026#34;}, onload : function (response) { if (response.status === 200) { const data = JSON.parse(response.responseText); if (data.access_token) { resolve(data.access_token); } else { reject(new Error(\u0026#34;Token not found in response\u0026#34;)); } } else { reject(new Error(\u0026#34;HTTP error getting token: \u0026#34; + response.status)); } }, onerror : reject, ontimeout: reject }); }); } With CORS issues resolved, we could now communicate freely with the AI from within the script.\nPrompt Engineering and JSON Agreement Simply being able to call the AI wasn\u0026rsquo;t enough; the key was instructing it effectively to get the desired output. This is where Prompt Engineering comes in.\nInitial Prompt Initially, one might just concatenate the question and student answer into the prompt and let the AI write a comment freely.\nQuestion: Please explain the function of an empty shot (kūjìngtóu). Student Answer: Empty shots can establish the environment and transition time/space. Please provide a one-sentence comment: This might yield a comment like, \u0026ldquo;The answer is basically correct, but not comprehensive enough,\u0026rdquo; which is a decent start.\nAdding More Context Providing only the question and student\u0026rsquo;s answer, while capable of generating a basic comment, might lack the nuance needed for more accurate evaluation. To enhance the AI\u0026rsquo;s assessment capabilities, we can enrich the prompt with additional contextual information. For instance, including the correct answer explicitly allows the AI to know the standard benchmark. Similarly, providing the official answer analysis or grading rubric, if available on the page, helps the AI better grasp the question\u0026rsquo;s intended focus and evaluation criteria. Furthermore, informing the AI about the maximum score for the question gives it a sense of the grading scale, potentially leading to more reasonable score suggestions if we later task it with assisting in grading. By integrating these extra pieces of information, we construct a more comprehensive prompt, guiding the AI towards making more precise and well-founded judgments and comments.\nYou are a university teaching assistant. Please evaluate the student\u0026#39;s answer based on the following information and provide a concise comment: Question (Max Score: 8 points): What are the specific functions and artistic values of an empty shot (kūjìngtóu)? Student Answer: (1) Establish the story environment (2) Serve as a means of spatiotemporal transition Correct Answer Reference: (1) Establish the story environment (2) Serve as a means of spatiotemporal transition (3) Render atmosphere, enhance emotion (4) Create artistic conception (yìjìng) Answer Analysis Reference: An empty shot is one containing only scenery, no characters... It serves multiple expressive functions and artistic values, specifically in four aspects. Comment: This allows the AI to more clearly see which points the student covered and which were missed.\nDefining Output Format (JSON) Free-text comments still require the script to parse them. What if we also want the AI to suggest a score? Including the score directly within the comment makes parsing more difficult and error-prone. A better approach is to have the AI return structured data, such as JSON.\nThis requires modifying the System Prompt (global role/instructions for the AI) and the User Prompt (specific request) to explicitly ask for a JSON object in a specific format:\n// Example System Prompt const DEFAULT_SYSTEM_PROMPT = ` You are a university teaching assistant grading homework. Based on the provided context, evaluate the student\u0026#39;s answer. Respond ONLY with a JSON object containing two keys: 1. \u0026#34;score\u0026#34;: A numerical score between 0 and the Maximum Score (inclusive). 2. \u0026#34;comment\u0026#34;: A brief, positive, and constructive comment (max 25 characters). Example Response Format: { \u0026#34;score\u0026#34;: 10, \u0026#34;comment\u0026#34;: \u0026#34;Accurate answer, key points clear.\u0026#34; } Do NOT include any other text or markdown formatting. `; // Example User Prompt (used with the System Prompt) const userPrompt = ` Question (Max Score: ${maxScore} points): ${questionText} Student Answer: ${studentAnswerText} Correct Answer Reference: ${correctAnswerText || \u0026#34;None provided\u0026#34;} Answer Analysis Reference: ${analysisText || \u0026#34;None provided\u0026#34;} Based on the information above, please return ONLY the JSON object with the score and comment: `; Additionally, when calling the API, if the specific endpoint supports it (like some newer Ernie API versions), one might try adding the \u0026quot;response_format\u0026quot;: \u0026quot;json_object\u0026quot; parameter to further enforce the JSON output structure.\nParsing and Application Upon receiving the result string containing the AI\u0026rsquo;s response, the script needs to perform several processing steps before the information can be applied to the webpage. First, since AIs sometimes wrap JSON strings in markdown code block markers (like ```json ... ```), an optional cleanup step is necessary to remove these extraneous characters, yielding a clean JSON string.\nThe next critical step is parsing. Using JavaScript\u0026rsquo;s built-in JSON.parse() method, this cleaned string is converted into a standard JavaScript object. If the AI adhered to the instructions, this object should contain the expected keys, such as score and comment.\nHowever, trusting external service responses implicitly is unwise, making validation an essential part of the process. The script must check if the parsed object actually contains both the score and comment properties. For the score, further validation is needed to ensure its value is a valid number and falls within the acceptable range (e.g., greater than or equal to 0 and less than or equal to the question\u0026rsquo;s maximum score). For the comment, a simple check for a non-empty string might suffice.\nOnly after passing these validation checks can the results be confidently applied to the user interface. The validated score is inserted into the corresponding question\u0026rsquo;s score input field, and the retrieved comment is written into the comment textarea using the previously determined DOM manipulation method (which might involve clicking \u0026quot; Comment,\u0026quot; finding the textarea, setting its value, and then clicking \u0026ldquo;Done\u0026rdquo;). If issues arise during parsing or validation (e.g., the response isn\u0026rsquo;t valid JSON, or the score is out of range), the script should implement appropriate error handling, such as logging a warning and populating the comment field with a default message indicating failure or invalidity.\n// Pseudocode: Handling AI JSON Response async function handleAiResponse(aiResultString, scoreInput, commentArea, confirmButton, maxScore) { let score = 0; let comment = \u0026#34;(Failed to process comment)\u0026#34;; try { // Clean potential markdown backticks const cleanedString = aiResultString.replace(/^```json\\s*|```$/g, \u0026#39;\u0026#39;).trim(); const result = JSON.parse(cleanedString); if (result \u0026amp;\u0026amp; typeof result.score === \u0026#39;number\u0026#39; \u0026amp;\u0026amp; typeof result.comment === \u0026#39;string\u0026#39;) { // Validate score const potentialScore = parseFloat(result.score); const maxScoreNum = parseFloat(maxScore); if (!isNaN(potentialScore) \u0026amp;\u0026amp; !isNaN(maxScoreNum) \u0026amp;\u0026amp; potentialScore \u0026gt;= 0 \u0026amp;\u0026amp; potentialScore \u0026lt;= maxScoreNum) { score = potentialScore; } else { console.warn(`Invalid score from AI: ${result.score}, Max: ${maxScore}. Defaulting to 0.`); comment = `(Invalid Score) ${result.comment}`; // Prepend warning } comment = result.comment; // Use AI comment regardless of score validity (unless invalid JSON) } else { console.warn(\u0026#34;AI response is not valid JSON or missing keys:\u0026#34;, result); comment = \u0026#34;(Invalid comment format)\u0026#34;; } } catch (e) { console.error(\u0026#34;Error parsing AI JSON response:\u0026#34;, e, aiResultString); comment = \u0026#34;(Error parsing comment)\u0026#34;; } // Apply to UI if (scoreInput) scoreInput.value = score; if (commentArea) commentArea.value = comment; if (confirmButton) confirmButton.click(); console.log(`Applied Score: ${score}, Comment: ${comment}`); } Integration, Results, and Reflections Bringing together precise DOM manipulation, cross-domain API calls via Tampermonkey, essential data extraction, carefully designed AI interaction (including prompt engineering and JSON parsing), and a user-friendly trigger button resulted in a functional Tampermonkey script for assisted grading. Key to its success were crucial design principles: * asynchronous flow control* using async/await to manage network requests and delays sequentially; robust error handling with try...catch to prevent single failures from stopping the entire process; an enhanced **user experience ** through SweetAlert for confirmations and feedback; modular code organization into functions for readability and maintenance; on-demand execution via a button click for user control; and a constant awareness of security implications, limiting the script\u0026rsquo;s use to personal, trusted environments due to the client-side handling of API keys.\nWhile this script demonstrably cannot replace a teacher\u0026rsquo;s nuanced judgment or personalized feedback, it effectively met its primary objective: significantly reducing repetitive workload. It provides preliminary AI-suggested scores ( defaulting to the max or using parsed JSON values), requiring only teacher review and adjustment. It automatically generates basic comments based on AI analysis, serving as a starting point for faster feedback. Most importantly, it enables batch processing, handling all short-answer questions on the page sequentially with a single click, saving considerable time.\nThis project offered profound insights into the complexity of real-world web applications, where dynamic loading and third-party components often complicate standard approaches, sometimes making direct DOM manipulation a necessary, if potentially fragile, alternative to unreliable APIs. It also clearly defined the boundaries of automation; technology excels at efficiency and repetitive tasks but cannot replicate the deep understanding and empathy required for high-quality educational feedback – the goal must be assistance, not replacement. Furthermore, the experience underscored the critical role of prompt engineering in effectively communicating intent to AI and the value of structuring requests for predictable, usable output like JSON. Finally, it served as a potent reminder about security consciousness when handling sensitive credentials in client-side scripts.\nIn conclusion, though a modest personal project, the journey of tackling these challenges and creating a genuinely helpful tool for family was deeply rewarding. Hopefully, sharing this exploration offers some practical insights. If you undertake a similar project, remember to analyze your specific target platform meticulously, always prioritize security (especially with API keys), and embrace the debugging process using your browser\u0026rsquo;s developer tools. Often, the true joy of coding lies in overcoming these practical hurdles and making tangible, positive changes in everyday life. Thank you for reading!\n","permalink":"https://blog.tategotoazarasi.me/en/posts/automating-online-grading-with-tampermonkey-and-ai/","summary":"Discover how a Tampermonkey userscript was developed using Baidu Ernie AI to automate scoring and commenting for online short-answer homework, significantly reducing repetitive grading tasks for educators.","title":"Automating Online Grading with Tampermonkey and AI"},{"content":"Today, let\u0026rsquo;s chat about a commonplace yet timeless topic – matrix multiplication. \u0026ldquo;Matrix multiplication? Learned that in university linear algebra, isn\u0026rsquo;t it just three for loops?\u0026rdquo; you might say. Indeed, the most basic implementation is exactly that, simple and direct. But in the world of high-performance computing, where every cycle counts, there\u0026rsquo;s a whole universe hidden behind those three nested loops. Different implementation methods can lead to performance differences that are worlds apart, sometimes by factors of hundreds or even thousands!\nSounds a bit exciting, doesn\u0026rsquo;t it? Like comparing the speed of an F1 race car to a mobility scooter. Why such a massive gap? Modern CPU and GPU architectures, compiler optimizations, parallel computing techniques, specialized math libraries\u0026hellip; these are all critical factors influencing performance.\nTo get a firsthand feel for these differences, I recently conducted a matrix multiplication (square matrices, C = A * B) \u0026ldquo;performance showdown\u0026rdquo; on my new gear – a Lenovo ThinkBook 16 G7+ laptop equipped with an AMD Ryzen AI 9 365 processor (featuring integrated Radeon 880M graphics). We invited several \u0026ldquo;contenders\u0026rdquo; to the ring, covering a wide range of approaches: from the most naive implementation to methods leveraging CPU multi-cores, SIMD instruction sets, calling professional math libraries, and even harnessing GPU acceleration (using OpenCL, Vulkan Compute, and ROCm/HIP).\nThis blog post will walk you through the entire benchmarking process: from introducing the \u0026ldquo;race track\u0026rdquo; environment, dissecting the technical characteristics of each \u0026ldquo;contender,\u0026rdquo; to analyzing the final results and summarizing the takeaways. We\u0026rsquo;re not aiming for a stern academic paper, but rather a relaxed, natural discussion about the technical intricacies and the allure of performance optimization. Hopefully, this will provide some inspiration and satisfy your curiosity about high-performance computing.\nReady? Buckle up, let\u0026rsquo;s get started!\nHardware and Software Environment As the saying goes, \u0026ldquo;To do a good job, one must first sharpen one\u0026rsquo;s tools.\u0026rdquo; Before diving into the performance tests, let\u0026rsquo;s lay out the \u0026ldquo;tools of the trade,\u0026rdquo; meaning the hardware and software environment used for this benchmark. Understanding this background information will help us better interpret the subsequent performance data.\nMy core hardware configuration includes an AMD Ryzen AI 9 365 processor, belonging to family 26, model 36. This is a fairly new CPU, boasting 10 physical cores and supporting 20 threads, with a base frequency of 2.0 GHz. It features crucial AVX, AVX2, FMA, and, importantly, AVX-512 instruction set support (including various flavors like AVX512F, DQ, CD, BW, VL). While it also integrates an NPU (Neural Processing Unit), our tests primarily focus on its general-purpose CPU and GPU compute capabilities. For memory, the system is equipped with 27.2 GiB (approximately 32GB as reported by the system) of DDR5 RAM; memory size and speed are critical for the performance of large-scale matrix operations. The integrated graphics card is the AMD Radeon Graphics (Radeon 880M). According to information from rocminfo and vulkaninfo, its GPU model identifier is gfx1150 (sometimes shown as 11.5.0), featuring 12 Compute Units (CUs), each containing 2 SIMD units. It can reach a maximum clock frequency of 2900MHz and supports both FP16 and FP64 ( double-precision) computations. This integrated GPU supports Vulkan, OpenCL, and AMD\u0026rsquo;s ROCm/HIP platform, offering multiple avenues for GPU acceleration in our tests. It\u0026rsquo;s worth noting specifically that during the benchmark execution, I set the HSA_OVERRIDE_GFX_VERSION=11.5.1 environment variable. This might slightly influence the target code generation or runtime behavior for HIP or hipBLAS, a practice often employed because official rocblas support for gfx1150 wasn\u0026rsquo;t fully implemented at the time.\nOn the software side, I\u0026rsquo;m running Arch Linux, a rolling-release distribution, which keeps my software packages relatively up-to-date. The specific kernel version is 6.14.2-2-cachyos (64-bit); CachyOS is an Arch derivative often incorporating performance-enhancing patches. The desktop environment is KDE Plasma 6.3.4, operating on the Wayland display server protocol. For compilation, I primarily use GCC (g++), whose version varies with Arch Linux updates but certainly supports C++17/20 standards along with OpenMP and AVX/AVX-512 instructions. HIP code compilation relies on hipcc from the ROCm toolchain, which is based on Clang. Project building is managed by CMake (version 3.20 or higher).\nThe core libraries and drivers are key components for this benchmark. The ROCm platform needs to support the gfx1150 or gfx1151 GPU model; rocminfo output in the test logs indicates Runtime Version 1.1 and Extension Version 1.6. The OpenCL environment is slightly complex, with two platforms present: the AMD APP SDK (providing OpenCL 2.1, driver version 3635.0) and Mesa rusticl (providing OpenCL 3.0). However, based on the test log stating OpenCL Info: Selected AMD Platform. Using first GPU device. Device Name: gfx1151, we specifically selected the GPU device under the official AMD driver platform for testing, identified as gfx1151. For Vulkan, the instance version is 1.4.309, using the RADV driver (from Mesa 25.0.4), which identifies the device as AMD Radeon Graphics (RADV GFX1150). We utilized the glslc tool to compile GLSL compute shaders into SPIR-V format. The system also has a BLAS (Basic Linear Algebra Subprograms) implementation installed, likely OpenBLAS, a common high-performance choice on Linux distributions, successfully located by CMake\u0026rsquo;s find_package(BLAS). Additionally, the open-source OpenCL BLAS library, CLBlast, is installed and discoverable by CMake. Furthermore, we tested the popular C++ template library Eigen3 (version 3.3+, provided as header files) and the computer vision library OpenCV (version 4.x, with its core module correctly found by CMake).\nFinally, the entire benchmarking framework is Google Benchmark (v1.9.2). This is an industry-standard C++ benchmarking library offering convenient test fixture management, precise timing, automatic iteration count adjustment, and standardized result output, ensuring the rigor and reliability of our tests.\nTo squeeze out as much performance as possible, we employed some rather aggressive compilation options. For C++ code, we used the GCC (g++) compiler with the -Ofast optimization level, combined with the -march=native flag, allowing the compiler to generate the most optimized machine code based on the specific features of my native CPU (including its AVX-512 capabilities). We also explicitly added -mavx2 -mfma -mavx512f -mavx512dq flags to ensure these SIMD instructions could be utilized. For HIP code, we similarly used the -Ofast optimization option with hipcc (based on Clang). Moreover, CMAKE_HIP_ARCHITECTURES was set to gfx1150 via CMake (based on rocminfo findings) to guide the compiler in generating code for the target GPU architecture. OpenCL Kernel optimization differs; it\u0026rsquo;s specified not during host code compilation but at runtime via options passed to the clBuildProgram function. A commonly used optimization flag is -cl-fast-relaxed-math, which permits the OpenCL compiler to perform mathematical optimizations that might slightly affect floating-point precision but can significantly improve execution speed. Lastly, for Vulkan compute shaders, we also included the -O option when compiling them into SPIR-V format using the glslc tool, enabling compile-time optimization.\nWith this background set, let\u0026rsquo;s introduce the contenders and see what tricks they have up their sleeves.\nThe Contenders: Matrix Multiplication Implementations Detailed Next, we\u0026rsquo;ll introduce each matrix multiplication implementation method that participated in this performance showdown.\nNaive Implementation This contender is the one we\u0026rsquo;re most familiar with and the starting point for all optimizations. It strictly follows the definition of matrix multiplication, C[i][j] = Σ(A[i][k] * B[k][j]), using three nested loops:\n// Pseudo-code example for i = 0 to N-1: for j = 0 to N-1: sum = 0; for k = 0 to N-1: sum += A[i][k] * B[k][j]; // or A[i*N + k] * B[k*N + j] for row-major 1D array C[i][j] = sum; // or C[i*N + j] = sum The advantage of this naive implementation lies in its extreme simplicity and logical clarity, making it easy to understand. However, its disadvantage is extremely poor performance. This stems mainly from several factors. First, it\u0026rsquo;s Cache Unfriendly. During computation, access to the B matrix occurs column-wise (in the innermost k-loop, j is constant, k increments, accessing B[k*N + j]), but data is stored row-wise (Row-Major) in memory. This mismatch between access pattern and storage layout leads to frequent CPU cache line misses, requiring constant reloading from main memory and drastically reducing memory access efficiency. Accesses to matrix A (row-wise) and writes to matrix C (element-wise) are comparatively better for caching, but the B matrix access pattern becomes the performance killer. Second, this implementation is entirely serial, failing to utilize the valuable multi-core parallel processing capabilities of modern CPUs. Finally, it also makes no use of the CPU\u0026rsquo;s SIMD (Single Instruction, Multiple Data) units for vectorized computation; each operation handles only a single element\u0026rsquo;s multiplication and addition, resulting in low efficiency.\nThis one primarily serves as a performance baseline to see how much improvement other methods can offer.\nOpenMP (CPU Multi-core Parallelism) OpenMP is a parallel programming model based on shared memory, primarily using compiler directives (Pragmas) to guide the compiler in automatically generating parallel code. For loop-intensive tasks like matrix multiplication, it can easily distribute the outer loop (typically the i loop) across different CPU cores for execution.\nImplementation-wise, it merely involves adding a #pragma omp parallel for directive before the outer loop of the Naive version:\n#pragma omp parallel for default(none) shared(A, B, C, N) schedule(static) for (size_t i = 0; i \u0026lt; N; ++i) { // Inner j and k loops remain unchanged for (size_t j = 0; j \u0026lt; N; ++j) { ValueType sum = 0.0; for (size_t k = 0; k \u0026lt; N; ++k) { sum += A[i * N + k] * B[k * N + j]; } C[i * N + j] = sum; } } Let\u0026rsquo;s break down the key parts of this OpenMP directive. parallel for is the core instruction, telling the compiler to parallelize the subsequent for loop. default(none) is a recommended good practice, forcing the programmer to explicitly declare the scope of each variable within the loop—either shared (shared) or thread-private (private)—to prevent potential errors. shared(A, B, C, N) declares that the matrices A, B, C, and the size N are shared among all concurrently executing threads; A and B are read-only during computation, while C is written to, but since OpenMP typically distributes work row-wise, different threads usually write to different rows of C, generally avoiding write conflicts. Finally, schedule(static) defines the work distribution strategy. It statically pre-divides the loop\u0026rsquo;s entire iteration space (here, the N iterations of i) into roughly equal chunks and assigns these chunks to the available threads. For well-load-balanced loops like matrix multiplication, static scheduling typically incurs low runtime overhead.\nThe primary advantage of using OpenMP is its implementation simplicity; often, just adding a single compiler directive ( Pragma) before a critical loop conveniently utilizes the CPU\u0026rsquo;s multi-core resources. Compared to the fully serial Naive implementation, performance usually sees a significant boost, ideally approaching a speedup factor close to the number of CPU cores, although the actual improvement is constrained by factors like memory bandwidth and cache efficiency. However, it also has drawbacks. First, it doesn\u0026rsquo;t resolve the cache unfriendliness issue present in the Naive version, particularly the column-wise access pattern for matrix B, which limits further performance gains. Second, its performance ceiling is inherently limited by the number of physical CPU cores and the system\u0026rsquo;s memory bandwidth. Furthermore, for very small matrix sizes (N), the overhead introduced by parallel computing (such as thread creation, management, and synchronization) might even outweigh the time saved by parallel execution, leading to performance degradation instead of improvement.\nCPU SIMD (AVX2/AVX-512 + FMA) SIMD (Single Instruction, Multiple Data) is a crucial feature of modern CPUs. It allows a single instruction to perform the same operation on multiple data elements simultaneously. For instance, AVX2 can process 4 double values at once ( using 256-bit registers), while AVX-512 can handle 8 double values (using 512-bit registers). FMA (Fused Multiply-Add) instructions further enhance efficiency, and potentially precision, by combining a multiplication and an addition into a single instruction.\nTo leverage SIMD, we typically need to use compiler-specific intrinsic functions. This makes the code considerably more complex than the Naive or OpenMP versions.\nAVX2 + FMA (256-bit) To utilize AVX2 and FMA instructions, we included the immintrin.h header file, which provides access to the necessary intrinsic functions. A key optimization strategy here involves changing the loop nesting order to i-k-j. The advantage of this order is that it allows for efficient vectorization within the innermost j loop. Specifically, for fixed i and k, we can first take the scalar value A[i][k] and broadcast it into all 4 double-precision elements of a 256-bit vector a_vec using the _mm256_set1_pd() intrinsic. Next, we load 4 consecutive double values from the k-th row of matrix B (starting at address \u0026amp;B[k*N + j]) into a vector b_vec. Since matrix B is stored row-major, this consecutive load is generally cache-friendly. We opted for _mm256_loadu_pd(), which allows loading from unaligned memory addresses, offering more flexibility. Concurrently, we load the corresponding 4 partial sums from the i-th row of matrix C (address \u0026amp;C[i*N + j]) into c_vec, also using _mm256_loadu_pd(). The core computational step involves executing the FMA (Fused Multiply-Add) operation, c_vec = a_vec * b_vec + c_vec, using the _mm256_fmadd_pd() intrinsic. This single instruction performs 4 pairs of multiplications and additions simultaneously. Finally, the updated result vector c_vec is written back to the corresponding location in matrix C using _mm256_storeu_pd(). Naturally, the implementation of the innermost j loop needs to iterate with a step size of 4 (the AVX2_DOUBLE_COUNT) and also requires special handling for any remaining elements at the end of the row (less than 4), which typically fall back to standard scalar computation.\n// Pseudo-code example (AVX2 + FMA) constexpr size_t AVX2_DOUBLE_COUNT = 4; for (size_t i = 0; i \u0026lt; N; ++i) { for (size_t k = 0; k \u0026lt; N; ++k) { __m256d a_vec = _mm256_set1_pd(A[i*N + k]); // Broadcast A[i][k] for (size_t j = 0; j \u0026lt; N_aligned; j += AVX2_DOUBLE_COUNT) { // Aligned part __m256d b_vec = _mm256_loadu_pd(\u0026amp;B[k*N + j]); // Load 4 doubles from B row k __m256d c_vec = _mm256_loadu_pd(\u0026amp;C[i*N + j]); // Load 4 doubles from C row i c_vec = _mm256_fmadd_pd(a_vec, b_vec, c_vec); // Fused Multiply-Add _mm256_storeu_pd(\u0026amp;C[i*N + j], c_vec); // Store back to C } // Handle remaining elements j = N_aligned to N-1 using scalar operations } } AVX-512 + FMA (512-bit) The implementation principle for AVX-512 + FMA is identical to the AVX2 version. The main difference lies in using 512-bit wide registers and their corresponding intrinsic functions, such as the __m512d type, _mm512_set1_pd, _mm512_loadu_pd, _mm512_fmadd_pd, and _mm512_storeu_pd. Because the registers are wider, the vector computation step size increases to 8 (AVX512_DOUBLE_COUNT), meaning a single instruction can now process 8 double-precision values. Successfully compiling and running AVX-512 code requires the CPU itself to support the instruction set (our Ryzen AI 9 365 processor meets this condition) and necessitates enabling these instructions via appropriate compiler options (like -mavx512f) during compilation.\nThis SIMD-based optimization approach offers significant advantages. Primarily, it can drastically improve the computational performance of a single CPU core. Additionally, employing the i-k-j loop order enhances the memory access pattern for matrix B, making it more cache-friendly. The core benefit comes from fully utilizing the powerful vector processing units within the CPU. However, this method also comes with notable disadvantages. Writing and maintaining code using SIMD intrinsics is considerably complex, and the resulting code suffers from poor portability as it directly depends on the specific instruction sets supported by the target CPU. Developers must also manually handle potential memory alignment issues (although loadu/storeu provide unaligned access, aligned loads/stores are generally faster) and manage the boundary conditions at the end of loops. Furthermore, historically, executing AVX-512 instructions could sometimes trigger the CPU to reduce its operating frequency to manage power consumption and heat generation; while this issue has been largely mitigated in modern CPUs, it remains a potential consideration.\nSIMD + OpenMP (AVX2/AVX-512 + FMA + OpenMP) Since OpenMP can parallelize the outer loop and SIMD can accelerate the inner computations, combining them seems like a powerful synergy. Indeed, it is.\nThe implementation simply involves adding the OpenMP parallel directive before the outer i loop of the SIMD (either AVX2 or AVX-512) version using the i-k-j loop order:\n#pragma omp parallel for default(none) shared(A, B, C, N, N_aligned) schedule(static) for (size_t i = 0; i \u0026lt; N; ++i) { // Inner k and j (SIMD) loops remain unchanged for (size_t k = 0; k \u0026lt; N; ++k) { // ... SIMD intrinsics code as before ... } } The primary advantage of combining SIMD instructions (be it AVX2 or AVX-512) with OpenMP multithreading is its ability to leverage both the CPU\u0026rsquo;s multi-core parallel processing power and its instruction-level parallelism (vectorization) simultaneously. This two-pronged approach often allows reaching, or at least closely approaching, the theoretical peak performance of the CPU for the given task. However, this method also has clear disadvantages. Firstly, it further compounds the code complexity, incorporating intricacies from both SIMD intrinsics programming and OpenMP parallel management. Secondly, as computation speed is pushed to its limits, the application\u0026rsquo;s performance bottleneck is very likely to shift from the computation itself to being limited by memory bandwidth – meaning the CPU cores can process data faster than the memory subsystem can supply it. Lastly, achieving optimal performance usually requires careful tuning of OpenMP-related parameters, such as selecting the most effective thread scheduling strategy (e.g., static, dynamic, guided via the schedule clause) and potentially employing advanced thread management techniques like thread affinity or load balancing adjustments.\nBLAS (Basic Linear Algebra Subprograms) BLAS isn\u0026rsquo;t a specific library but rather a standardized API specification defining interfaces for basic vector and matrix operations. Many organizations and companies provide implementations of BLAS. These libraries typically contain highly optimized C, Fortran, or even assembly code tailored for specific hardware (CPU architecture, cache sizes, SIMD instructions). They often internally implement sophisticated techniques like blocking (or tiling) to maximize cache utilization and automatically employ both SIMD instructions and multithreading.\nWe only need to call the standard C interface cblas_dgemm (\u0026rsquo;d\u0026rsquo; for double precision, \u0026lsquo;gemm\u0026rsquo; for general matrix-matrix multiplication):\n// Pseudo-code example cblas_dgemm( CblasRowMajor, // Tell BLAS our data is stored row by row CblasNoTrans, CblasNoTrans, // Neither A nor B needs transposing N, N, N, // M, N, K (for N x N matrices) 1.0, // alpha (for C = alpha*A*B + beta*C) A.data(), N, // Pointer to A data and its leading dimension (cols for RowMajor) B.data(), N, // Pointer to B data and its leading dimension 0.0, // beta (set to 0 to overwrite C, i.e., C = A*B) C.data(), N // Pointer to C data and its leading dimension ); Using a BLAS library for matrix multiplication offers several advantages. The most prominent is extreme ease of use; developers typically only need to call a single highly optimized library function (like cblas_dgemm) to perform the complex computation, significantly simplifying the programming effort. Secondly, because these libraries incorporate extensive hardware-specific optimizations, their performance is usually excellent, often approaching the theoretical peak computational throughput of the hardware. Furthermore, as BLAS is a standard interface, it provides good portability – code can generally run unmodified on any target platform that has a compliant BLAS library implementation. Calling a library function also results in very concise application code. Of course, using BLAS also has disadvantages. First, the application needs to be linked against the corresponding BLAS library file during the build process. Second, and most critically, the final performance achieved heavily depends on the quality of the specific BLAS implementation being used. Different BLAS libraries (like OpenBLAS, Intel MKL, ATLAS, etc.) can exhibit significant performance variations even on the same hardware.\nEigen \u0026amp; OpenCV Besides low-level interfaces like BLAS, many high-level C++ libraries also provide matrix operations. We tested two popular examples: Eigen and OpenCV.\nEigen Let\u0026rsquo;s take a look at the Eigen library. Its key characteristic is being a C++ template library renowned for its elegant API and powerful \u0026ldquo;Expression Templates\u0026rdquo; technology. This technique allows Eigen to analyze and optimize complex chains of linear algebra expressions at compile time, avoiding the creation of unnecessary intermediate temporary objects and often automatically generating SIMD instructions for the underlying computations. In terms of usage, Eigen code is also very concise. We can first use Eigen::Map to \u0026ldquo;map\u0026rdquo; our raw data stored in std::vector onto Eigen\u0026rsquo;s internal matrix object – this mapping itself incurs zero memory copy overhead. Then, we can directly use the overloaded * operator to perform the matrix multiplication, like so:\n// Pseudo-code example (Map existing data) Eigen::Map\u0026lt;const EigenMatrixType\u0026gt; A_map(A.data(), N, N); Eigen::Map\u0026lt;const EigenMatrixType\u0026gt; B_map(B.data(), N, N); EigenMatrixType C_eigen(N, N); // Eigen\u0026#39;s result matrix matrix_multiply_eigen(A_map, B_map, C_eigen); // C_eigen.noalias() = A_map * B_map; It\u0026rsquo;s worth noting the use of the noalias() method in the code. This explicitly informs Eigen that the output matrix C does not overlap in memory with the input matrices A or B (no aliasing), enabling Eigen to employ more efficient and aggressive internal implementations for optimization.\nOverall, Eigen\u0026rsquo;s advantages include its very modern API, ease of use, and high code readability. Its ability to perform compile-time optimizations via C++ template metaprogramming is also a significant strength. However, it also has disadvantages. In terms of performance, it might not match specialized, deeply hand-optimized BLAS libraries (the final performance largely depends on the compiler\u0026rsquo;s optimization capabilities and the complexity of the specific expression). Additionally, due to its heavy reliance on templates, compile times can be relatively longer.\nOpenCV Next up is OpenCV. Its primary characteristic is being a comprehensive library mainly focused on computer vision tasks. However, its core module (core) also provides very powerful matrix operations centered around the cv::Mat class. cv::Mat can manage its own memory or conveniently \u0026ldquo;wrap\u0026rdquo; existing external data, avoiding unnecessary copies. An important advantage is that when performing computationally intensive operations like matrix multiplication, OpenCV typically attempts to leverage available underlying optimization mechanisms to accelerate the process. This might include Intel IPP (Integrated Performance Primitives), OpenMP multithreading, or potentially even calling a system-installed BLAS library. When using it, we can wrap the data from our std::vector into cv::Mat objects without copying, specifying the rows, columns, data type (CV_64F for double), and the data pointer. Then, we call the cv::gemm function provided by OpenCV to perform the matrix multiplication. This function\u0026rsquo;s interface is very similar to the gemm function in BLAS:\n// Pseudo-code example cv::Mat A_cv(N, N, CV_64F, A.data()); // Wrap existing data cv::Mat B_cv(N, N, CV_64F, B.data()); cv::Mat C_cv(N, N, CV_64F); // OpenCV result matrix matrix_multiply_opencv(A_cv, B_cv, C_cv); // cv::gemm(A_cv, B_cv, 1.0, cv::Mat(), 0.0, C_cv); OpenCV\u0026rsquo;s advantages lie in its extremely rich feature set, extending far beyond just matrix multiplication to cover a vast range of image processing and computer vision functionalities. If your project is already using OpenCV, employing it for matrix operations allows for seamless integration with other library features. Furthermore, it may leverage various backend optimization libraries to enhance performance. However, its disadvantages are also notable, primarily the fact that it introduces a relatively large and complex library dependency. If your task solely involves pure linear algebra computations, incorporating the entire OpenCV library might not be the most lightweight choice.\nOpenCL Now we turn to OpenCL (Open Computing Language), an open standard framework designed for cross-platform, heterogeneous parallel computing, allowing programs to utilize various compute devices including CPUs, GPUs, DSPs, and even FPGAs.\nThe typical workflow for computing with OpenCL is rather involved, encompassing multiple steps. First, one needs to query available OpenCL platforms (like the AMD APP SDK) and select a compute device from one of them (such as the gfx1151 GPU used in our tests). Next, a Context must be created; this acts as a container for managing the selected device(s) and associated resources like memory objects and command queues. Following that, a Command Queue is created for the chosen device, which serves as the conduit for submitting tasks (like memory transfers and kernel executions) to the device. The core data (matrices A, B, C) needs to reside in Memory Buffers on the device, created as cl_mem objects; this necessitates copying the input data A and B from host (CPU) memory to their respective device buffers. The computational task itself is defined in an OpenCL Kernel, typically written in a separate .cl file (like our matrix_mult.cl); this source code must be loaded, compiled (at which point optimization options like -cl-fast-relaxed-math can be passed), and built into an OpenCL Program object (cl_program). From this program object, the specific kernel function object (cl_kernel) to be executed is obtained. Before executing the kernel, its arguments must be set using clSetKernelArg, passing the device buffer objects (the cl_mem handles for A, B, C) and the matrix size N, among other potential parameters. Kernel execution is initiated by enqueuing the task onto the command queue using clEnqueueNDRangeKernel. This requires specifying the total number of global work-items (usually N* N, with each work-item calculating one element of C) and optionally, the local work-item size (the Workgroup size, e.g., 16x16, which impacts resource usage and performance). After the kernel finishes execution on the device, the results stored in the C buffer on the device must be copied back to host memory using clEnqueueReadBuffer. Finally, and crucially, all created OpenCL objects (kernel, program, buffers, queue, context) must be explicitly released to prevent resource leaks.\nRegarding the OpenCL Kernel code (matrix_mult.cl), it\u0026rsquo;s written in the OpenCL C language, a dialect based on the C99 standard with extensions for parallel computing. In our matrix multiplication kernel, each work-item (think of it as a lightweight thread) uses the built-in functions get_global_id(0) and get_global_id(1) to determine its unique coordinates (column col, row row) within the global N x N computation grid. Then, each work-item independently executes the inner loop over k to compute the dot product and store the result for C[row][col]. Since we\u0026rsquo;re using double as our data type, the kernel code needs the #pragma OPENCL EXTENSION cl_khr_fp64 : enable directive to explicitly enable support for double-precision floating-point numbers.\nThe main advantages of OpenCL are its theoretical cross-platform and cross-vendor compatibility and its ability to fully leverage the massive parallel compute power of GPUs and other accelerators. However, its disadvantages are also significant: the programming model is relatively complex, requiring developers to manually manage platforms, devices, contexts, memory, synchronization, and more, which often leads to verbose code. Furthermore, data must be explicitly transferred between the host and the device, introducing latency and bandwidth overhead that can negatively impact performance (become counterproductive), especially for computationally small tasks. Additionally, the actual performance of an OpenCL application can be sensitive to the quality of the specific vendor\u0026rsquo;s driver implementation.\nCLBlast CLBlast can be thought of as the BLAS implementation for the OpenCL ecosystem. Its design goal is to provide an API compatible with the traditional BLAS interface, but its internal computational logic is implemented using the OpenCL standard, enabling it to run on any GPU (or other accelerator) that supports OpenCL.\nIn terms of usage, invoking CLBlast is significantly simpler than manually writing and managing OpenCL kernels. First, you still need an initialized OpenCL environment, including a context and a command queue; we can directly reuse the global context g_clContext prepared for the pure OpenCL implementation. Next, OpenCL memory buffers need to be created for the input and output matrices, and the host data must be copied to the input buffers, just as in standard OpenCL. Once these prerequisites are met, the core step involves calling the CLBlast function clblast::Gemm\u0026lt;ValueType\u0026gt;(...) ( using the C++ template interface here, where ValueType automatically determines the precision). When calling this function, you need to pass arguments describing the matrix layout (row-major or column-major), whether the input matrices should be transposed, the matrix dimensions (M, N, K), the scalar values alpha and beta, pointers to the device-side OpenCL buffer objects, the leading dimension of each matrix (which is typically the number of columns for row-major storage), and the OpenCL command queue to use for execution. The CLBlast library then takes care of internally invoking its pre-compiled and optimized OpenCL kernels to perform the actual computation. After the computation is complete, the developer still needs to copy the results from the device-side C buffer back to host memory, similar to standard OpenCL practice.\nThe primary advantages of CLBlast are that it offers a standard BLAS interface, greatly simplifying the programming effort required for GPU-accelerated matrix operations using OpenCL. Furthermore, because the kernel functions within the CLBlast library are typically meticulously optimized by its developers (likely employing advanced techniques like sophisticated tiling, shared memory optimization, etc.), its performance is often superior to relatively simple OpenCL kernels written by application developers. However, it also has disadvantages. First, it relies on the target system having a correctly installed and configured OpenCL runtime environment as well as the CLBlast library itself. Second, like all GPU acceleration schemes based on a discrete memory model, it still involves the overhead of data transfer between the host and the device, which can become a performance bottleneck for small problems or in bandwidth-limited scenarios.\nVulkan Compute Next is Vulkan Compute. Vulkan itself was primarily designed as a next-generation, high-performance graphics rendering API, but it also incorporates powerful general-purpose computing (GPGPU) capabilities implemented via Compute Shaders.\nThe workflow for performing computations using Vulkan is arguably even more verbose and lower-level than OpenCL. Broadly, it involves the following sequence of steps: First is the initialization of a Vulkan Instance, followed by selecting a suitable Physical Device (usually the GPU), and then creating a Logical Device based on it, along with obtaining a Compute Queue for submitting computational tasks. The computation logic itself needs to be written in a compute shader (like our matrix_mult.comp), typically using the GLSL language. This shader must then be compiled into Vulkan\u0026rsquo;s standard intermediate representation, SPIR-V format (using a tool like glslc -O), and this SPIR-V code is loaded to create a Shader Module (VkShaderModule). For data storage, you must explicitly allocate Memory ( VkDeviceMemory) on the device and create Vulkan Buffers (VkBuffer) to hold the input matrices A, B, and the output matrix C. This process involves complex decisions regarding memory type selection, allocation, and binding buffers to the allocated memory. Copying data from the host (CPU) to these device buffers usually requires an intermediate, host-visible Staging Buffer. To allow the shader to access these buffer resources, Descriptors must be set up. This includes defining a Descriptor Set Layout (VkDescriptorSetLayout) to declare the resources the shader needs (e.g., three storage buffers), creating a Descriptor Pool (VkDescriptorPool) from which to allocate descriptor sets, allocating a specific Descriptor Set (VkDescriptorSet), and finally \u0026ldquo;connecting\u0026rdquo; or updating this descriptor set with the information about our created buffers. With the shader module and descriptors ready, the next step is to create the Compute Pipeline. This requires first creating a Pipeline Layout (VkPipelineLayout), which associates the descriptor set layouts used by the shader, and then creating the actual compute pipeline object (VkPipeline) based on this layout and the shader module. The actual commands are submitted via a Command Buffer. One must be allocated from a Command Pool (VkCommandPool). Then, you begin recording commands into it: first, you bind the compute pipeline and the descriptor set containing the resource information, and then you invoke vkCmdDispatch to launch the computation. vkCmdDispatch requires specifying the number of workgroups to launch, which usually needs to be calculated based on the matrix size N and the number of threads per workgroup defined in the shader (the local_size). Once command recording is complete, the command buffer is submitted to the previously obtained compute queue for execution. Since submission is asynchronous, Vulkan synchronization primitives like Fences or Semaphores must be used to wait for the GPU computation to finish. After completion, the results in the device\u0026rsquo;s C buffer need to be copied back to host memory, again likely using a staging buffer. The final step involves meticulously destroying all created Vulkan objects ( pipeline, layout, descriptors, pool, buffers, memory, device, instance, etc.) in the reverse order of creation to release resources properly.\nOur compute shader (matrix_mult.comp) is written in GLSL (OpenGL Shading Language). The layout (local_size_x = 16, local_size_y = 16) directive at the top defines that each workgroup consists of 16x16=256 work-items (threads). The layout(set = 0, binding = ...) specifications define how the shader accesses the buffers A, B, and C via binding points (0, 1, 2) within descriptor set 0. Inside the main function, the built-in variable gl_GlobalInvocationID.xy provides the global coordinates of the current work-item within the overall compute grid ( where id.x corresponds to the column and id.y to the row). The core computation logic, involving the loop over k to calculate the dot product for C[id.y * N + id.x], is very similar to the OpenCL kernel.\nThe advantages of using Vulkan Compute lie in it being a modern graphics API designed to reduce driver overhead on the CPU. If an application already requires graphics rendering, using Vulkan Compute allows for better integration with the rendering pipeline, potentially sharing resources and context. Vulkan also offers very fine-grained control over the hardware, enabling deep performance optimization. However, its disadvantages are quite prominent: the API is extremely verbose, and the initialization and setup processes are highly complex, leading to massive code overhead and comparatively lower development productivity. Vulkan\u0026rsquo;s primary design focus remains graphics rendering; although its compute capabilities are powerful, the ecosystem for general-purpose computing, including high-level library support and overall ease of use, might be considered somewhat less mature compared to OpenCL or NVIDIA\u0026rsquo;s CUDA / AMD\u0026rsquo;s HIP. And, just like OpenCL, the overhead of data transfer between host and device persists and needs careful management.\nHIP (Heterogeneous-Compute Interface for Portability) Now let\u0026rsquo;s discuss HIP (Heterogeneous-Compute Interface for Portability). HIP is an integral part of AMD\u0026rsquo;s ROCm (Radeon Open Compute) platform, designed to provide a C++ GPU programming model very similar to NVIDIA\u0026rsquo;s CUDA. One of its primary goals is to simplify the process of porting existing CUDA code to run on AMD GPUs.\nThe host-side (Host Code) workflow for GPU computing using HIP is considerably more concise compared to OpenCL and Vulkan, closely resembling the CUDA style. First, you need to allocate device memory for the input matrices A, B, and the output matrix C on the target GPU device using the hipMalloc() function. Then, data is transferred from host memory to device memory using hipMemcpy() (specifying hipMemcpyHostToDevice as the direction) for matrices A and B. The core computational task is initiated by launching the kernel function (matrix_multiply_hip_kernel) using a syntax very similar to CUDA\u0026rsquo;s \u0026lt;\u0026lt;\u0026lt;GridDim, BlockDim\u0026gt;\u0026gt;\u0026gt; notation, which specifies the kernel\u0026rsquo;s execution configuration. GridDim defines the number of thread blocks (analogous to OpenCL workgroups) to launch, while BlockDim defines the number of threads within each block (e.g., we might set it to 16x16). The grid dimensions usually need to be calculated based on the total matrix size N and the chosen block dimensions to ensure the entire computation is covered. Since kernel launches are asynchronous, the host code must call hipDeviceSynchronize() to wait for all computations on the GPU to complete. After computation finishes, the results from the C matrix in device memory are transferred back to host memory using hipMemcpy() (this time specifying hipMemcpyDeviceToHost). Finally, it\u0026rsquo;s crucial to release all device memory allocated earlier using the hipFree() function. Throughout this process, it\u0026rsquo;s recommended to use our defined HIP_CHECK() macro (which internally calls hipGetErrorString) to check the return value of every HIP API call for timely error detection and handling.\nThe HIP device-side code (in the matrix_mult_hip.hip file) is written using standard C++ syntax along with some HIP-specific extensions. Functions marked with the __global__ keyword are kernel functions that can be launched from the host using the \u0026lt;\u0026lt;\u0026lt;...\u0026gt;\u0026gt;\u0026gt; syntax. Inside the kernel function, built-in variables like blockIdx (index of the current block within the grid), threadIdx (index of the current thread within its block), and blockDim (dimensions of the block) are accessible. By combining these variables, we can calculate the global ID of the current thread ( corresponding to the row and col in the result matrix), similar to how global IDs are obtained in OpenCL/Vulkan ( e.g., via get_global_id or gl_GlobalInvocationID). The core computational logic of our matrix multiplication kernel (the inner loop over k) is essentially the same as the OpenCL and Vulkan kernels we saw earlier.\nOverall, HIP\u0026rsquo;s main advantages are its provision of a C++ interface, which is generally easier to use and learn compared to OpenCL\u0026rsquo;s C API or the extremely verbose Vulkan API. Its high degree of syntactic similarity to CUDA significantly facilitates porting existing CUDA codebases to the AMD platform. Being part of the ROCm platform, HIP is tightly integrated with AMD\u0026rsquo;s GPU drivers and toolchain (like the hipcc compiler), usually resulting in good performance and compatibility. However, HIP also has disadvantages. It primarily targets AMD GPUs (although the HIP Clang project provides some capability to run on certain NVIDIA GPUs, this isn\u0026rsquo;t its main focus). Using HIP requires installing the relatively large ROCm SDK. And, like all GPU computing solutions based on a discrete memory model, the overhead of data transfer between the host and device remains a performance factor to consider.\nhipBLAS Finally, we arrive at hipBLAS. You can think of it as the BLAS library within the HIP ecosystem, analogous to cuBLAS in the CUDA world or CLBlast in the OpenCL sphere. hipBLAS is the officially provided library from the ROCm platform, offering Basic Linear Algebra Subprograms accelerated using HIP technology for AMD GPUs.\nUsing hipBLAS follows a pattern similar to other GPU BLAS libraries and is simpler than writing raw HIP kernels. First, a functional HIP runtime environment is a prerequisite. Before using hipBLAS functions, you need to create a hipBLAS handle, an object managing the library\u0026rsquo;s internal state, via hipblasHandle_t handle; hipblasCreate(\u0026amp;handle); for initialization. Memory management proceeds as with HIP kernels: use hipMalloc to allocate memory for A, B, and C on the GPU device, and use hipMemcpy to transfer host data to the device buffers for A and B. The core computation involves calling the hipblasDgemm() function (\u0026rsquo;d\u0026rsquo; for double precision). Its parameter list closely resembles cblas_dgemm, with key differences being the need to pass the previously created hipBLAS handle and the fact that the pointers for A, B, and C must be device memory pointers. You also need to specify the operation for each matrix, e.g., whether it needs transposition (HIPBLAS_OP_N for no transpose). One crucial detail to pay attention to is that hipBLAS, like many traditional BLAS libraries, defaults to expecting data stored in column-major order. However, C++ developers typically work with row-major storage. If our inputs A and B are row-major, and we want to compute the row-major result C = A * B, calling hipblasDgemm directly requires careful handling of the data layout. A common trick is to leverage the mathematical identity CT = BT * AT. This involves telling hipblasDgemm to compute BT * AT (passing HIPBLAS_OP_T for both A and B), swapping the device pointers passed for A and B, swapping their leading dimensions (lda, ldb), and also swapping the matrix dimensions M and N. The resulting buffer computed this way is C transposed (CT stored in column-major order), which happens to have the exact same memory layout as C stored in row-major order. Alternatively, a more direct approach, if supported by your hipBLAS version, would be to check for an API function or setting that directly supports row-major inputs. However, for our matrix_multiply_hipblas implementation, we assume it internally handles the layout correctly (perhaps via the transpose trick or a newer interface) to provide behavior consistent with cblas_dgemm. Since the call executes asynchronously, it\u0026rsquo;s necessary to call hipDeviceSynchronize() to ensure the hipBLAS operation is completed and synchronized. Afterwards, use hipMemcpy to copy the result from the device C buffer back to the host. Finally, don\u0026rsquo;t forget to destroy the hipBLAS handle using hipblasDestroy(handle) to release its resources. As always, using the HIPBLAS_CHECK() macro to verify the status of each hipBLAS API call is recommended for robust error handling.\nThe primary advantages of hipBLAS are that it provides a standard BLAS interface, making high-performance linear algebra on AMD GPUs relatively easy to use. The library contains HIP kernels that are highly optimized by AMD specifically for their GPU architectures, thus usually delivering very high performance and effectively leveraging the hardware\u0026rsquo;s potential. Naturally, there are disadvantages too. Using hipBLAS depends on having the ROCm/HIP development environment and the hipBLAS library correctly installed. Like all GPU acceleration methods, the cost of data transfer between host and device remains. Furthermore, developers must pay close attention to handling the row-major versus column-major data layout issue to ensure correct function calls and parameter settings.\nAlright, all the contenders have been introduced. From simple serial loops to complex GPU programming, we\u0026rsquo;ve covered a spectrum of mainstream performance optimization ideas and technology stacks. Next up, let\u0026rsquo;s see how they actually performed in our benchmark tests!\nBenchmarking Methodology To ensure a fair comparison between these different implementations, we utilized the Google Benchmark framework. We also designed a specific test fixture, named MatrixMultFixture, to manage the setup and teardown tasks associated with each individual test run.\nDuring the test setup (SetUp) phase for each test case, the program first determines the current square matrix size N based on parameters passed by the Google Benchmark framework. It then allocates host (CPU) memory, typically using std::vector\u0026lt;ValueType\u0026gt;, for the input matrices A and B, as well as for an output matrix C intended to store results from CPU, SIMD, or some GPU implementations. Subsequently, matrices A and B are filled with random numbers. If the test involves the Eigen or OpenCV libraries, their respective specific result matrices (like C_eigen, C_cv) are also allocated at this stage. It\u0026rsquo;s important to note that for technologies requiring a persistent global context, such as OpenCL, Vulkan, and HIP, their initialization (e.g., via functions like initOpenCL, initVulkan) and final cleanup ( e.g., cleanupOpenCL) are performed once at the beginning and end of the entire benchmark program\u0026rsquo;s execution (within the main function), not within the per-test SetUp and TearDown. This avoids the significant overhead of repeatedly initializing and destroying these heavyweight contexts for every single test iteration.\nNext comes the test execution phase, driven by Google Benchmark\u0026rsquo;s macros. Each distinct matrix multiplication implementation corresponds to a separate Benchmark test function, for instance, BENCHMARK_F(MatrixMultFixture, BM_Naive) signifies the test for the Naive implementation. Inside each such test function, the core logic resides within a for (auto _ : state) loop controlled by Google Benchmark. Within this loop, we invoke the specific matrix multiplication function currently being tested, such as matrix_multiply_naive(A, B, C, N). The Google Benchmark framework intelligently and automatically adjusts the number of times this loop runs to ensure stable and reliable timing measurements are obtained. For libraries that necessitate data mapping or wrapping (like Eigen and OpenCV), the mapping (Eigen::Map) or wrapper object creation (cv::Mat) typically occurs inside this loop, but since these are usually zero-copy or low-overhead operations, their impact on the performance measurement is minimal. For the GPU-accelerated implementations (including OpenCL, Vulkan, HIP, CLBlast, hipBLAS), calling their respective execution functions usually encapsulates a sequence of operations: potentially creating (or reusing) device-side memory buffers, transferring input data from host to device (Host-to-Device), launching the computation kernel on the GPU, waiting for kernel execution to complete (synchronization), and finally transferring the computed results back from device to host (Device-to-Host).\nThe test cleanup (TearDown) phase is relatively straightforward, mainly involving the release of the host memory resources allocated during the SetUp phase, for example, by calling methods like A.clear(), B.clear(), C.clear(), and so forth.\nRegarding the test scope, we selected a range of N values for benchmarking, specifically 64, 128, 256, 512, and 1024. Choosing these powers of two is a common practice in benchmarking, as it helps in observing performance trends as the problem scale increases, particularly when plotted on logarithmic axes.\nIn terms of performance metrics, Google Benchmark primarily measures and reports real_time, which corresponds to the wall-clock time elapsed. Based on this measured time (typically in nanoseconds, ns) and the current matrix size N, we calculated a more informative core performance metric: GFLOPS (Giga Floating-point Operations Per Second). The formula used was GFLOPS = (2.0 * N^3) / (time_ns / 1e9). This calculation assumes that a standard square matrix multiplication requires 2 * N^3 floating-point operations (roughly N^3 multiplications and N^3 additions). All benchmark results were ultimately saved to a JSON formatted file named benchmark_results.json for convenient post-processing.\nFinally, for results visualization and easier comparison, we used Python along with the powerful data manipulation library pandas and the plotting library matplotlib. A script reads the generated JSON file, parses the data, calculates GFLOPS for each run, and then generates the performance comparison plot. In the plot, the X-axis represents the matrix size N (using a base-2 logarithmic scale to better show power-of-two relationships), and the Y-axis represents the performance in GFLOPS (also using a logarithmic scale to accommodate the vast differences in performance). This graphical representation allows us to see the performance gaps between different implementations and their respective scaling trends with problem size at a glance.\nNow, let\u0026rsquo;s see the final report card!\nPerformance Data Analysis Please take a look at the performance comparison chart plotted from the benchmark results:\nTo interpret this information-rich chart, let\u0026rsquo;s first examine the axes. The X-axis represents the matrix size N, spanning from 64 to 1024, and employs a base-2 logarithmic scale. The Y-axis denotes performance in GFLOPS (billions of floating-point operations per second) and also uses a logarithmic scale. The choice of logarithmic scales is crucial here; it helps to clearly display implementations with vastly different performance levels on the same graph and makes it easier to observe the relative performance trends as N changes. The legend on the right side lists all the implementation methods tested, along with their corresponding markers and colors, allowing easy identification of each line.\nLooking at the overall trends, several prominent patterns emerge. First, most implementations exhibit improved performance as the matrix size N increases, reflected by the generally upward slope of the curves. This is expected because for larger N, the total computational workload (which scales as O(N^3)) becomes much larger relative to fixed or slower-growing overheads (like function call costs, GPU data transfer latencies, thread startup times, etc.). This allows the benefits of parallelism and optimization to become more pronounced. Additionally, larger computational tasks are better at amortizing memory access latencies. Second, there\u0026rsquo;s an extremely wide range of performance across different implementations, differing by orders of magnitude. This is strikingly evident when comparing the lowest performer, the Naive implementation, to the top performer, hipBLAS (at N=1024). The performance gap exceeds 170,000 times! (Specifically, Naive at ~0.0006 GFLOPS vs. hipBLAS at ~102 GFLOPS). This dramatically underscores the importance and potential impact of optimization. Third, we observe that some curves tend to flatten out or even slightly decrease at larger values of N. This typically indicates that the performance of that implementation is hitting a bottleneck under the current conditions. Such bottlenecks could be varied, including saturated memory bandwidth (data can\u0026rsquo;t be supplied fast enough), insufficient CPU or GPU cache capacity for the working set causing lower cache hit rates, reaching the limit of GPU core utilization, or perhaps certain unoptimized overheads growing linearly or faster with N, starting to negate the computational speedup.\nTo analyze the performance data more deeply, we can group the implementations and compare them within and across groups.\nFirst, let\u0026rsquo;s look at the CPU Basic Group, comparing the simplest Naive implementation against the version using only OpenMP for parallelism. The Naive implementation (yellow \u0026lsquo;+\u0026rsquo; marker) is undeniably the slowest. Its curve hugs the bottom of the chart on the log scale, showing very little growth with N, reaching only about 0.6 GFLOPS at N=1024 ( re-reading based on plot, correcting potential misinterpretation of raw data; GFLOPS derived from JSON). In contrast, the OpenMP version (orange square marker), leveraging the CPU\u0026rsquo;s 20 threads, shows a marked improvement, achieving around 4 GFLOPS at N=1024, roughly 6-7 times faster than Naive. Nevertheless, compared to more advanced optimization techniques, this is still quite slow. Its relatively flat performance curve suggests that simple multi-core parallelism might quickly become limited by factors like memory bandwidth.\nNext is the CPU SIMD Group, where we examine the impact of using AVX2 and AVX-512 instructions, both alone and combined with OpenMP. The single-threaded AVX2+FMA implementation (dark blue circle) already demonstrates the power of vectorization, delivering respectable performance (~1.7 GFLOPS at N=1024), even slightly outperforming the pure OpenMP version for N \u0026lt; 512. Moving to AVX512+FMA (green triangle) yields further speedup, as the 512-bit vectors can process twice the data per instruction compared to AVX2, reaching about 2.4 GFLOPS at N=1024. The real performance leap occurs when combining SIMD with multi-threading. AVX2+FMA_OMP (red diamond) achieves roughly 9.5 GFLOPS at N=1024, more than 5 times faster than single-threaded AVX2 and over twice as fast as pure OpenMP. The champion within this group, and indeed the top performer among all CPU implementations tested, is AVX512+FMA_OMP (purple inverted triangle). By combining the widest available SIMD vectors with multi-core parallelism, it hits an impressive 15 GFLOPS at N=1024, about a 60% improvement over the AVX2+OMP version. Its line sits at the pinnacle of the CPU-only results.\nNow, let\u0026rsquo;s consider the CPU Professional Library Group, comparing BLAS, Eigen, and OpenCV. BLAS (purple \u0026lsquo;V\u0026rsquo; marker) delivered excellent performance, reaching approximately 53 GFLOPS at N=1024 (correction based on re-reading the plot), nearly matching or slightly exceeding our best manually tuned CPU code (AVX512+FMA_OMP). This strongly indicates that the BLAS library installed on the system (likely OpenBLAS) is extremely well-optimized internally, effectively utilizing both SIMD instructions and multi-threading. Equally impressive was OpenCVLib (light blue circle), whose performance closely tracked BLAS, even slightly surpassing it at N=1024 with about 54 GFLOPS. This suggests that OpenCV\u0026rsquo;s gemm implementation benefits from powerful backend optimizations, possibly by calling an optimized BLAS library or another performance kernel library like IPP internally. However, EigenLib (pink star) showed surprisingly poor performance in this specific test, lagging behind even the basic OpenMP version and achieving only about 0.7 GFLOPS at N=1024. This contrasts sharply with Eigen\u0026rsquo;s generally high-performance reputation. Possible reasons for this anomaly could include suboptimal usage of Eigen in the test code (though unlikely if using standard operations), the compiler failing to adequately optimize Eigen\u0026rsquo;s expression templates for this specific case, or perhaps compatibility issues between the particular Eigen version and the test environment. Therefore, this result for Eigen should be viewed with caution and not generalized; it\u0026rsquo;s likely specific to the conditions of this benchmark.\nFinally, we examine the GPU Acceleration Group, comprising implementations using OpenCL, Vulkan, HIP, and the corresponding BLAS libraries CLBlast and hipBLAS. A general trend across all GPU methods is that their performance tends to be lower than well-optimized CPU methods (like BLAS or AVX+OMP) at smaller matrix sizes (e.g., N=64), sometimes even slower than Naive+OpenMP. This is primarily due to the overhead associated with GPU computing, namely the time spent transferring data between the CPU and GPU (Host-to-Device and Device-to-Host) and the latency involved in launching the GPU kernel. For small tasks, these fixed overheads constitute a large portion of the total execution time. However, as N increases, the massive parallel processing capability of the GPU dominates, and their performance curves rise rapidly, quickly surpassing all CPU-based implementations.\nAmong the manually written kernels (where we coded the computation logic in OpenCL C, GLSL, or HIP C++), OpenCL (cyan diamond) performed quite well, reaching about 58 GFLOPS at N=1024, with a steep curve indicating good scalability. Vulkan (green up-triangle) also delivered good performance, although slightly lower than OpenCL and the HIP kernel, at around 29 GFLOPS for N=1024. Given Vulkan\u0026rsquo;s API complexity, this result seems reasonable, possibly leaving room for further driver or shader optimization. The HIP kernel (gray \u0026lsquo;X\u0026rsquo; marker) exhibited anomalously low performance at N=64 ( potentially due to measurement error or an initialization glitch), but for N=128 and larger, its performance quickly caught up and closely mirrored that of OpenCL, reaching about 57 GFLOPS at N=1024. This suggests that for this relatively simple kernel, the underlying execution efficiency of HIP and OpenCL on this particular AMD GPU is quite similar.\nPerformance took another significant jump when using the GPU BLAS libraries. CLBlast (brown diamond), being the OpenCL BLAS library, far outperformed our handwritten OpenCL kernel, achieving roughly 95 GFLOPS at N=1024. This highlights the value of specialized library optimizations; CLBlast likely employs more sophisticated techniques internally, such as advanced memory access patterns, data tiling, and efficient use of GPU shared memory (LDS). The undisputed overall winner of this entire benchmark was hipBLAS (red down-triangle). As the native BLAS library for AMD\u0026rsquo;s ROCm platform, it delivered the most outstanding performance, breaking the 100 GFLOPS barrier at N=1024 and reaching approximately 102 GFLOPS. This typically signifies that hipBLAS is best able to leverage the specific hardware features and instructions of the AMD GPU.\nLet\u0026rsquo;s briefly summarize the highlights and points of caution from this benchmark. The clear performance leaders at N=1024 were the GPU BLAS libraries, hipBLAS and CLBlast. Within the CPU realm, the system BLAS library, OpenCV, and the manually crafted AVX512+FMA+OMP implementation were the top contenders. The sheer magnitude of performance improvement observed was astounding: from the basic Naive method to the fastest hipBLAS implementation, the speedup at N=1024 exceeded a factor of 170,000! The advantage of using GPUs became evident for matrix sizes of N=256 and larger in our tests, with the gap widening as N increased. This also underscored the importance of using professional libraries like BLAS, CLBlast, hipBLAS, and even OpenCV, which often outperform manual optimization efforts (especially simpler custom GPU kernels) by encapsulating extensive hardware-specific tuning. On the cautionary side, the anomalously poor performance of Eigen in this specific test warrants further investigation and should not be taken as a general statement about Eigen\u0026rsquo;s capabilities. Similarly, the outlier result for the HIP kernel at N=64 suggests that this particular data point might be invalid and should be treated carefully.\nIn essence, this performance showdown vividly illustrates the vast differences that various technological approaches can make. From elementary CPU loops to intricate GPU programming, every optimization technique has its rationale and optimal use case.\nDeeper Dive: Discussion \u0026amp; Caveats While this performance benchmark provides us with a wealth of direct data, it also prompts further reflection and requires acknowledging certain limitations and important considerations when interpreting the results.\nFirst and foremost, the results are highly hardware-dependent. All tests were conducted on a specific platform featuring an AMD Ryzen AI 9 processor paired with a Radeon 880M integrated GPU. Running the same benchmarks on different hardware, such as an Intel CPU or an NVIDIA GPU, could yield dramatically different outcomes and performance rankings. For example, Intel CPUs often show exceptional performance when coupled with Intel\u0026rsquo;s own MKL (Math Kernel Library), while NVIDIA GPUs would necessitate the use of the CUDA programming model and the cuBLAS library to achieve their best results.\nSecond, the choice of compiler and library versions can significantly influence the outcome. The specific version of GCC or Clang used, the selected optimization flags (e.g., -Ofast versus -O3 might trade precision or standard conformance for speed), and the particular version and build configuration of mathematical libraries like BLAS, OpenCV, or Eigen can all impact the final performance numbers. For instance, substituting OpenBLAS with MKL on an Intel CPU could lead to completely different BLAS performance results.\nFurthermore, the data type and matrix characteristics are crucial factors. This benchmark exclusively used double ( 64-bit double-precision floating-point numbers) for square matrices. If we were to switch to float (32-bit single-precision), performance would generally be higher due to halved data volume reducing memory bandwidth pressure, SIMD instructions processing twice as many elements per operation, and some hardware intrinsically favoring single-precision computations. Additionally, our tests focused on dense square matrices. For matrices with special structures like sparsity, symmetry, or bandedness, employing specialized storage formats, algorithms, and dedicated libraries is essential for efficient computation.\nMoreover, GFLOPS isn\u0026rsquo;t the whole story. While GFLOPS is a vital metric for gauging raw computational throughput, it doesn\u0026rsquo;t capture the full picture of real-world application performance. Especially in the context of GPU computing, the time spent transferring data between the host (CPU) and the device (GPU) – operations like hipMemcpy or clEnqueueWrite/ReadBuffer – constitutes an integral part of the total task duration. Our benchmark, likely focusing on the time spent within the Google Benchmark loop, might primarily measure the core computation time and potentially underrepresent or exclude the full data transfer overhead. In practical applications, the end-to-end execution time is what truly matters. For small matrix problems, this data transfer overhead can even dominate the overall time.\nWe must also consider the trade-offs between implementation complexity and ease of use. The highest-performing solutions, such as hipBLAS or CLBlast, while relatively simple to use (calling library functions), rely on the user having the specific SDKs (like ROCm) and environments correctly installed and configured. On the other hand, manually writing SIMD intrinsics or GPU kernel code (for OpenCL, Vulkan, or HIP) might offer finer control over performance but demands deep expertise in low-level hardware details and parallel programming, often involving significant development, debugging, and optimization effort. The Naive and OpenMP approaches are the simplest to implement but yield the poorest performance. Therefore, selecting the right implementation method for a real-world project requires careful balancing between performance requirements, development costs, code portability, and long-term maintainability.\nIt\u0026rsquo;s also worth acknowledging that regarding cache optimization, the CPU SIMD and GPU kernels (OpenCL/Vulkan/HIP) that we manually implemented were relatively basic and did not incorporate sophisticated data blocking (or tiling) strategies. Blocking is an advanced optimization technique that involves partitioning large matrices into smaller sub-matrices (blocks) and performing computations block-wise. Its main goal is to maximize the utilization of CPU or GPU caches by improving data locality and cache hit rates. This technique is one of the core reasons why high-performance BLAS libraries achieve near-peak hardware performance. If we were to implement complex blocking in our manual code, their performance might improve further, but at the cost of a dramatic increase in code complexity.\nFinally, the anomalous results observed for the Eigen library and the HIP kernel at N=64 serve as a reminder that benchmark results should always be interpreted critically. When encountering data that starkly contradicts expectations, one should resist jumping to immediate conclusions and instead try to investigate potential causes – could it be a bug in the code, an issue with compilation flags, measurement inaccuracies, interference from other system processes, or perhaps a compatibility problem specific to the test environment? Only through careful scrutiny and validation can we gain confidence in the benchmark findings.\nThe Finish Line: Conclusion \u0026amp; Outlook Having journeyed through this comprehensive matrix multiplication performance showdown—spanning CPUs and GPUs, serial and parallel approaches, manual optimizations, and professional libraries—we can draw several clear conclusions.\nFirst and foremost, optimization is absolutely crucial. The chasm in performance between the most basic Naive implementation and highly optimized solutions is immense, vividly demonstrating that for compute-intensive tasks, selecting the right algorithms and implementation techniques is paramount for achieving acceptable, let alone excellent, performance. Second, leveraging hardware features yields significant rewards. Utilizing modern CPU capabilities like multi-core processing (e.g., via OpenMP) and SIMD instruction sets (either through manual intrinsics or library-provided automatic vectorization) provides substantial speedups; combining these two often pushes CPU performance towards its practical limits. Third, the potential for GPU acceleration is enormous. For computational tasks of sufficient scale (in our tests, starting around N=256), the massively parallel architecture of GPUs enables performance levels far exceeding what CPUs can offer. Fourth, it highlights the value of making good use of professional libraries. Specialized math libraries such as BLAS (and its various implementations like OpenBLAS, MKL, AOCL-BLAS), CLBlast, hipBLAS (or cuBLAS for NVIDIA), encapsulate a vast amount of low-level optimization expertise. Employing them is frequently the most effective path to achieving both high performance and good development productivity. Even higher-level libraries like OpenCV may rely on these optimized backends internally. However, we must also recognize that there is no \u0026ldquo;silver bullet\u0026rdquo; in performance optimization; no single method reigns supreme in all scenarios. Small-scale problems might favor CPU implementations due to avoided data transfer overheads, while large-scale problems clearly benefit from GPU acceleration. The optimal choice will invariably depend on the specific hardware platform, the required precision ( single vs. double), and the available development resources and constraints. Finally, all these findings point towards the importance of continuous learning and hands-on practice. High-performance computing is a rapidly evolving field with constant advancements in hardware architectures, programming models, and compiler technologies. Maintaining curiosity, persistently learning new techniques, and personally testing and validating assumptions are the keys to truly mastering the art and science of performance optimization.\nHopefully, this exploration into the performance landscape of matrix multiplication has provided everyone with a more tangible understanding of the diverse computing technologies available. From the humble three nested loops to blistering speeds exceeding one hundred GFLOPS, the journey reflects the culmination of ingenuity in computer architecture, parallel computing, and software engineering. Perhaps the next time you\u0026rsquo;re faced with a task involving large-scale matrix operations, you\u0026rsquo;ll recall the contenders we discussed today and feel more equipped to choose the most suitable acceleration strategy for your application!\nAppendix: Benchmark Results Data Table omitted as requested.\nImplementation Matrix Size (N) Real Time (ns) Performance (GFLOPS) Naive 64 640,561 0.818 Naive 128 5,250,421 0.799 Naive 256 42,393,811 0.791 Naive 512 569,762,981 0.471 Naive 1024 3,447,583,101 0.623 OpenMP 64 149,270 3.512 OpenMP 128 1,036,590 4.046 OpenMP 256 6,844,282 4.903 OpenMP 512 62,077,042 4.324 OpenMP 1024 578,410,614 3.713 AVX2+FMA 64 311,178 1.685 AVX2+FMA 128 2,505,685 1.674 AVX2+FMA 256 19,324,494 1.736 AVX2+FMA 512 152,734,950 1.758 AVX2+FMA 1024 1,237,421,611 1.735 AVX512+FMA 64 221,951 2.362 AVX512+FMA 128 1,702,158 2.464 AVX512+FMA 256 14,094,445 2.381 AVX512+FMA 512 107,877,880 2.488 AVX512+FMA 1024 921,593,993 2.330 AVX2+FMA_OMP 64 90,276 5.808 AVX2+FMA_OMP 128 664,552 6.311 AVX2+FMA_OMP 256 3,656,076 9.178 AVX2+FMA_OMP 512 27,922,787 9.613 AVX2+FMA_OMP 1024 216,519,971 9.918 AVX512+FMA_OMP 64 86,896 6.033 AVX512+FMA_OMP 128 427,994 9.799 AVX512+FMA_OMP 256 2,648,926 12.667 AVX512+FMA_OMP 512 18,439,355 14.558 AVX512+FMA_OMP 1024 140,055,382 15.333 Eigen 64 904,785 0.579 Eigen 128 12,846,593 0.326 Eigen 256 32,201,997 1.042 Eigen 512 284,153,414 0.945 Eigen 1024 2,316,560,842 0.927 OpenCV 64 33,326 15.732 OpenCV 128 73,443 57.110 OpenCV 256 538,501 62.311 OpenCV 512 4,811,569 55.790 OpenCV 1024 36,290,270 59.175 BLAS 64 10,609 49.420 BLAS 128 73,929 56.734 BLAS 256 535,021 62.716 BLAS 512 5,210,261 51.521 BLAS 1024 36,608,529 58.661 Vulkan 64 258,650 2.027 Vulkan 128 850,222 4.933 Vulkan 256 2,015,570 16.648 Vulkan 512 15,517,304 17.300 Vulkan 1024 69,655,183 30.830 OpenCL 64 69,397 7.555 OpenCL 128 147,861 28.367 OpenCL 256 593,376 56.548 OpenCL 512 5,842,253 45.947 OpenCL 1024 38,429,528 55.881 CLBlast 64 61,002 8.595 CLBlast 128 127,007 33.024 CLBlast 256 426,358 78.700 CLBlast 512 3,740,453 71.765 CLBlast 1024 20,777,060 103.358 HIP 64 856,032,739 0.000612 HIP 128 171,225 24.496 HIP 256 613,603 54.684 HIP 512 5,788,911 46.371 HIP 1024 38,210,712 56.201 hipBLAS 64 2,080,484 0.252 hipBLAS 128 2,146,978 1.954 hipBLAS 256 2,691,232 12.468 hipBLAS 512 5,960,233 45.038 hipBLAS 1024 21,356,498 100.554 ","permalink":"https://blog.tategotoazarasi.me/en/posts/matrix-multiplication-performance-benchmark-from-triple-loops-to-100-plus-gflops-on-amd-ryzen-ai-radeon/","summary":"An in-depth benchmark comparing the performance of 11 matrix multiplication implementations (Naive, CPU multi-core/SIMD/BLAS, GPU via OpenCL/HIP/Vulkan) on AMD Ryzen AI + Radeon, revealing vast performance gaps and optimization insights.","title":"Matrix Multiplication Performance Benchmark: from Triple Loops to 100+ GFLOPS on AMD Ryzen AI + Radeon"},{"content":"Let\u0026rsquo;s dive back into our evolving C++/Rust/WASM project. In our previous explorations, we successfully:\nEstablished robust methods for managing entity relationships (1:1, 1:N, N:N) within the C++ EnTT ECS framework. Built a bridge using Wasmtime for bidirectional communication and memory sharing between a C++ host and a Rust WASM module. Combined these concepts, creating a stable C FFI layer to allow a Rust WASM plugin to manage EnTT entity relationships residing in the C++ host. This layered architecture, leveraging EnTT\u0026rsquo;s data-oriented nature and a carefully crafted C FFI, proved effective in overcoming the inherent limitations of the WASM boundary. However, as projects grow, the need for more sophisticated interaction patterns emerges. Our previous solution relied on the WASM module calling host functions to perform actions. What if we need the host to notify the WASM plugin when certain events occur within the EnTT world? What if the WASM plugin needs to intercept or modify the behaviour of host operations?\nOur initial foray into this involved creating custom \u0026ldquo;trigger\u0026rdquo; and \u0026ldquo;patching\u0026rdquo; mechanisms. While these solutions functioned, their ad-hoc nature, often depending on string-based function lookups and requiring manual management of callbacks, revealed significant drawbacks, rapidly leading to systems that were complex, brittle, and difficult to maintain. We specifically encountered a number of challenges. A primary concern was type safety; the reliance on function names represented as strings provided absolutely no compile-time guarantee that a given WASM function\u0026rsquo;s signature would actually match what the host expected for a particular trigger or patch point. Another difficulty arose in connection management: manually keeping track of which WASM functions were registered to handle which specific events became increasingly cumbersome, and tasks like disconnecting or updating these registrations necessitated meticulous, error-prone bookkeeping. Furthermore, our custom system offered no inherent capability to control the execution order or apply prioritization when multiple WASM callbacks were registered for the very same event. The handling of results presented yet another significant problem: determining how results from potentially multiple WASM \u0026ldquo;patch\u0026rdquo; functions should be combined, or even whether one WASM plugin should possess the ability to entirely prevent an action initiated by the host, was left without any standard or well-defined approach within our custom framework. Lastly, a considerable amount of boilerplate code was required; implementing the necessary registration, lookup, and invocation logic for every single trigger or patch point involved substantial and repetitive coding effort on both the C++ host and the Rust WASM sides of the system.\nIt became clear that we needed a more robust, standardized, and feature-rich eventing system. Enter Boost.Signals2.\nThis post chronicles the refactoring journey, replacing our custom trigger and patching mechanisms with the powerful and flexible Boost.Signals2 library. We\u0026rsquo;ll explore how this transition simplifies the architecture, enhances type safety ( as much as possible across FFI), provides sophisticated features like automatic connection management, prioritization, and result combination (\u0026ldquo;combiners\u0026rdquo;), and ultimately leads to a more maintainable and extensible host-plugin interaction model.\nWe\u0026rsquo;ll dissect the significant changes on both the C++ host side (introducing a SignalManager, adapting WasmHost and EnttManager, and leveraging C++ macros for signal emission) and the Rust WASM side (implementing signal slots and a new connection mechanism). Prepare for a deep dive into leveraging a mature signaling library to orchestrate complex events across the WASM boundary.\nThe Case for Signals: Why Boost.Signals2? Before dismantling our existing trigger/patch system, let\u0026rsquo;s understand why Boost.Signals2 is a compelling alternative. At its core, Boost.Signals2 implements the signals and slots programming pattern, a powerful mechanism for decoupled communication.\nAt its core, Boost.Signals2 implements the signals and slots programming pattern, a potent mechanism facilitating decoupled communication within an application. You can conceptualize signals as event broadcasters. Whenever a particular event takes place in the system, such as an entity being on the verge of creation or a name component having just been added, a corresponding signal object is formally \u0026ldquo;emitted\u0026rdquo; or \u0026ldquo;fired,\u0026rdquo; announcing the event occurrence.\nComplementing signals are the slots, which function as the designated event receivers. These slots are typically functions or callable function objects, like C++ lambdas, that are explicitly registered or \u0026ldquo;connected\u0026rdquo; to one or more specific signals. The crucial behavior is that when a signal is emitted, the framework automatically invokes all the slots currently connected to that specific signal.\nThe link established between a particular signal and a slot is represented by a connection object. A critically important feature offered by Boost.Signals2, setting it apart from many manual systems, is its provision of automatic connection management. This means that if either the signal itself or a connected slot object ceases to exist (for instance, by going out of scope) or if the connection is explicitly severed, the library automatically breaks the link. This robust management prevents the common and problematic issue of dangling callbacks, where the system might attempt to invoke a function that no longer exists, which is a significant advantage when compared to manually managed callback lists.\nWhere Boost.Signals2 particularly demonstrates its power, especially for our integration scenario, is through its concept of combiners. A combiner is essentially a rule or a policy that dictates how the return values generated by multiple slots, all connected to the same signal, should be aggregated or processed into a single outcome. For example, when dealing with \u0026ldquo;before\u0026rdquo; events, like before_create_entity, we might desire a behavior where any single connected slot has the power to veto or prevent the original operation from proceeding. This can be effectively achieved by implementing a custom combiner that intelligently stops the invocation sequence and returns immediately as soon as any slot returns true, thereby signaling that the operation should be skipped. Conversely, for \u0026ldquo;after\u0026rdquo; events where connected slots might intend to modify a result, such as in the after_get_name scenario, we could employ a standard combiner like boost::signals2::optional_last_value. This specific combiner conveniently returns the value that was produced by the very last slot executed in the sequence, a behavior that becomes particularly useful when slots are assigned different priorities. It\u0026rsquo;s also worth noting that the default combiner behavior simply returns void if the slots have no return value, or it returns a boost::optional\u0026lt;ResultType\u0026gt; containing the result from the last slot that returned a non-void value.\nFurthermore, Boost.Signals2 allows slots to be connected with associated group priorities. This feature provides developers with fine-grained control over the precise order in which slots connected to the same signal are executed relative to one another, enabling more complex interaction sequences.\nFinally, the library offers various configurable levels of thread safety. While our current host application operates in a single thread, this capability is a crucial consideration for potentially multi-threaded host environments, ensuring that signal emissions and slot connections can be handled safely under concurrent conditions.\nBy adopting Boost.Signals2, we replace our bespoke, error-prone system with a well-tested, feature-rich library designed specifically for this kind of event handling, significantly improving robustness and maintainability.\nHost-Side Revolution: The SignalManager and Macro Magic The most significant changes occur on the C++ host side. We need a central place to define our signals and manage connections to WASM slots, and we need a non-intrusive way to emit these signals when our existing C API functions are called.\nIntroducing SignalManager This new class becomes the heart of our host-side eventing system.\nSignal Definitions: Inside signal_manager.h, we define specific signal types using boost::signals2::signal. The template arguments define the signature of the slots that can connect to it (return type and parameter types). Critically, we also specify a combiner type.\n// signal_manager.h (Illustrative Snippets) #include \u0026lt;boost/signals2.hpp\u0026gt; #include \u0026lt;cstdint\u0026gt; #include \u0026lt;optional\u0026gt; // For optional_last_value combiner namespace WasmHostSignals { // Custom Combiner: Stops invocation if any slot returns true. // Useful for \u0026#34;before\u0026#34; signals to allow skipping the original action. struct StopOnTrueCombiner { typedef bool result_type; // The combiner returns bool template\u0026lt;typename InputIterator\u0026gt; result_type operator()(InputIterator first, InputIterator last) const { while (first != last) { // Dereference the iterator to get the slot\u0026#39;s return value // Assuming slots connected to signals using this combiner return bool if (*first) { // If the slot returned true... return true; // ...stop and return true (indicating skip) } ++first; } return false; // No slot returned true, return false (don\u0026#39;t skip) } }; // --- Signal Type Definitions --- // Example: Entity Creation // bool(): Return true to skip creation. using SignalBeforeCreateEntity = boost::signals2::signal\u0026lt;bool(), StopOnTrueCombiner\u0026gt;; // uint32_t(uint32_t original_id): Can modify the returned ID. using SignalAfterCreateEntity = boost::signals2::signal\u0026lt;uint32_t(uint32_t), boost::signals2::optional_last_value\u0026lt;uint32_t\u0026gt;\u0026gt;; // Example: Entity Destruction // bool(uint32_t entity_id): Return true to skip destruction. using SignalBeforeDestroyEntity = boost::signals2::signal\u0026lt;bool(uint32_t), StopOnTrueCombiner\u0026gt;; // void(uint32_t entity_id): Just a notification. using SignalAfterDestroyEntity = boost::signals2::signal\u0026lt;void(uint32_t)\u0026gt;; // Example: Get Name (Complex due to buffer) // bool(uint32_t id, char* buffer, size_t buffer_len): Can skip original get. // Note: WASM slot won\u0026#39;t easily access the host buffer content here. // Signature might be simplified in practice. using SignalBeforeGetName = boost::signals2::signal\u0026lt;bool(uint32_t, char*, size_t), StopOnTrueCombiner\u0026gt;; // size_t(uint32_t id, char* buffer, size_t buffer_len, size_t original_req_len): Can modify required_len. using SignalAfterGetName = boost::signals2::signal\u0026lt;size_t(uint32_t, char*, size_t, size_t), boost::signals2::optional_last_value\u0026lt;size_t\u0026gt;\u0026gt;; // ... Define signal types for all relevant host operations ... class WasmHost; // Forward declaration class SignalManager { public: // Signals are public members for macros to access easily // Could be private with accessor methods too. SignalBeforeCreateEntity signal_before_create_entity; SignalAfterCreateEntity signal_after_create_entity; SignalBeforeDestroyEntity signal_before_destroy_entity; SignalAfterDestroyEntity signal_after_destroy_entity; // ... Other signal members ... SignalBeforeGetName signal_before_get_name; SignalAfterGetName signal_after_get_name; // ... and many more ... explicit SignalManager(WasmHost* host); ~SignalManager(); // Deleted copy/move constructors/assignment operators SignalManager(const SignalManager\u0026amp;) = delete; SignalManager\u0026amp; operator=(const SignalManager\u0026amp;) = delete; // ... // Connects a WASM function (by name) to a specific signal (by name) bool connectWasmSlot(const std::string\u0026amp; signal_name, const std::string\u0026amp; wasm_func_name, int priority); private: WasmHost* wasm_host_ptr_; // Needed to call back into WASM // Type definition for the factory function using WasmSlotConnectorFactory = std::function\u0026lt;boost::signals2::connection( WasmHost* host, // Pointer to WasmHost boost::signals2::signal_base\u0026amp; signal, // Reference to the specific signal object const std::string\u0026amp; wasm_func_name, // Name of the WASM function int priority // Priority for connection )\u0026gt;; // Map from signal name (string) to a factory that creates the connection lambda std::map\u0026lt;std::string, WasmSlotConnectorFactory\u0026gt; slot_connector_factories_; // Initializes the slot_connector_factories_ map void initializeConnectorFactories(); // Structure to potentially track connection info (optional) struct WasmSlotInfo { std::string wasm_function_name; int priority = 0; boost::signals2::connection connection; // Stores the connection object }; // Store connections grouped by signal name (optional, for management) std::map\u0026lt;std::string, std::vector\u0026lt;std::shared_ptr\u0026lt;WasmSlotInfo\u0026gt;\u0026gt;\u0026gt; wasm_connections_; }; } // namespace WasmHostSignals Several critical design decisions shape the effectiveness of the SignalManager. The choice of combiners is fundamental to defining the interaction logic for different event types. For instance, we specifically define our custom StopOnTrueCombiner for signals intended to run before an operation (like before_create_entity), enabling any connected slot to prevent the original action simply by returning true. For signals emitted after an operation, especially those where slots might wish to modify a return value (such as after_create_entity potentially altering the returned ID), we utilize the standard boost::signals2::optional_last_value\u0026lt;T\u0026gt; combiner. This combiner has the behavior of returning the value produced by the very last slot that executed in the sequence, a feature that integrates naturally with the priority system. In cases where the signal serves purely as a notification (like after_destroy_entity), the default combiner, which simply returns void, is perfectly adequate.\nThe definition of signal signatures, such as bool(), uint32_t(uint32_t), void(uint32_t), and so forth, plays a crucial role in establishing the contract for any slots wishing to connect. These signatures dictate the exact parameter types and the return type that a compliant slot function must adhere to, which is essential for maintaining type safety across the system. It\u0026rsquo;s noteworthy that even complex scenarios, like the before_get_name signal, initially include buffer details (char*, size_t) in their signature to match the underlying C API. However, we recognize the practical difficulties of WASM slots directly manipulating host memory buffers via these parameters and anticipate that the actual WASM slot implementation might simplify its approach, perhaps ignoring these buffer arguments and opting to call back into the host via another FFI function if the buffer content is needed.\nConnecting WASM functions is facilitated by the connectWasmSlot public method. This function serves as the designated entry point that the WASM module will ultimately invoke, using the intermediary host_connect_signal FFI function, to register its handlers as slots. connectWasmSlot requires the string name of the target signal on the host and the string name of the function exported by the WASM module that should be connected to it.\nInternally, the setup relies heavily on the initializeConnectorFactories private method, which is executed within the SignalManager\u0026rsquo;s constructor. This method\u0026rsquo;s responsibility is to populate the slot_connector_factories_ map. This map uses the string name of a signal (e.g., the literal string \u0026quot;before_create_entity\u0026quot;) as its key. The corresponding value for each key is a C++ lambda function, which we term a \u0026ldquo;lambda factory.\u0026rdquo;\nEach lambda factory stored within the slot_connector_factories_ map is precisely engineered to perform a single, specific task: it knows how to connect a WASM function, identified by its name string, to one particular, hardcoded Boost.Signals2 signal member within the SignalManager instance (e.g., the factory associated with the key \u0026quot;before_create_entity\u0026quot; knows it must operate on the signal_before_create_entity member). To achieve this, the factory lambda typically captures the this pointer of the SignalManager or sometimes directly captures the specific signal member it targets. It\u0026rsquo;s designed to accept several arguments: a pointer to the WasmHost instance (necessary for invoking WASM functions), a reference to the specific target signal object (passed as a signal_base\u0026amp; for polymorphism within the factory signature, requiring a static_cast back to the concrete signal type inside), the string name of the WASM function to connect, and the desired connection priority. The core action within the factory lambda is the call signal.connect(priority, [host, wasm_func] (...) { ... }). The crucial element here is the second lambda passed to signal.connect – this inner lambda is the actual slot wrapper. This wrapper lambda is precisely what the Boost.Signals2 framework will execute whenever the specific Boost signal it\u0026rsquo;s connected to is emitted. The logic embedded within this slot wrapper lambda is responsible for bridging the gap to Wasmtime. It receives arguments directly from the Boost signal emission, matching the Boost signal\u0026rsquo;s defined signature (for example, the original_id parameter for signal_after_create_entity). Its first task is to marshal these incoming C++ arguments into the format Wasmtime expects, typically a std::vector\u0026lt;wasmtime::Val\u0026gt;. Next, it invokes the target WASM function by name using the WasmHost pointer and its callFunction method (e.g., host-\u0026gt;callFunction\u0026lt;ReturnType\u0026gt;(wasm_func, args)), carefully specifying the expected ReturnType based on the WASM function\u0026rsquo;s FFI signature (like int32_t for a WASM function returning a boolean, or uint32_t for one returning an entity ID). This call inherently involves handling potential Wasmtime traps, usually by checking the Result returned by callFunction. If the WASM call is successful, the wrapper then unmarshals the resulting wasmtime::Val back into the C++ data type that is expected by the combiner associated with the Boost signal (for instance, converting an int32_t result back to a bool for signals using the StopOnTrueCombiner, or to a uint32_t for those using optional_last_value\u0026lt;uint32_t\u0026gt;). Finally, this unmarshalled C++ value is returned by the slot wrapper lambda, feeding it back into the Boost signal\u0026rsquo;s processing mechanism ( specifically, its combiner).\nTo correctly route the connection request, the connectWasmSlot method must determine the actual boost::signals2::signal member object corresponding to the provided signal_name string. The current implementation employs a straightforward, albeit potentially lengthy, if/else if cascade to perform this mapping. It compares the input string against known signal names and, upon finding a match, passes a reference to the appropriate signal member ( like signal_before_create_entity) into the corresponding factory lambda retrieved from the slot_connector_factories_ map.\nFinally, robust connection management is implicitly handled by Boost.Signals2. While the code includes an optional mechanism to store the boost::signals2::connection object returned by connect within a wasm_connections_ map ( keyed by signal name), which could facilitate more granular future management like targeted disconnections, the primary benefit comes from the SignalManager\u0026rsquo;s destructor. Within the destructor, all stored connections are explicitly disconnected. More importantly, even without this explicit storage, Boost guarantees that connections are automatically broken if either the signal or the slot\u0026rsquo;s context (which isn\u0026rsquo;t directly applicable here since our slots are host-side lambdas calling WASM) is destroyed, significantly mitigating the risk of dangling pointers or callbacks.\nWasmHost now creates and owns both the SignalManager and the EnttManager, passing the SignalManager pointer to the EnttManager constructor. EnttManager itself is simplified – it no longer manages triggers directly but uses its SignalManager pointer to emit signals where appropriate (primarily in the onEntityDestroyedSignalHook).\nEmitting Signals via Macros (host_macros.h) We need to trigger these signals whenever the corresponding host C API functions are called from WASM. We could manually insert signal emission code into every host function lambda in host.cpp, but that\u0026rsquo;s repetitive and error-prone. Instead, we use C++ macros defined in host_macros.h.\n// host_macros.h (Illustrative Snippet) #pragma once #include \u0026#34;entt_api.h\u0026#34; #include \u0026#34;signal_manager.h\u0026#34; #include \u0026#34;wasm_host.h\u0026#34; #include \u0026lt;wasmtime.hh\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026lt;optional\u0026gt; #include \u0026lt;stdexcept\u0026gt; // For runtime_error // Helper within namespace to avoid polluting global scope namespace WasmHostUtils { // (Keep read_wasm_string_helper, check_result, handle_wasm_trap helpers here) } // namespace WasmHostUtils // Macro to define a host function taking 0 arguments and returning a value #define DEFINE_HOST_FUNC_0_ARGS_RET(LINKER, HOST_PTR, MGR_HANDLE, NAME, C_API_FUNC, WASM_TYPE, RET_TYPE, WASM_RET_TYPE, DEFAULT_RET) \\ (LINKER).func_new( \\ \u0026#34;env\u0026#34;, (NAME), (WASM_TYPE), \\ [(HOST_PTR), (MGR_HANDLE)]( \\ wasmtime::Caller caller, \\ wasmtime::Span\u0026lt;const wasmtime::Val\u0026gt; args, \\ wasmtime::Span\u0026lt;wasmtime::Val\u0026gt; results \\ ) -\u0026gt; wasmtime::Result\u0026lt;std::monostate, wasmtime::Trap\u0026gt; { \\ using namespace WasmHostSignals; \\ using namespace WasmHostUtils; \\ SignalManager\u0026amp; sig_mgr = (HOST_PTR)-\u0026gt;getSignalManager(); \\ RET_TYPE final_result = (DEFAULT_RET); /* Initialize with default */ \\ try { \\ /* --- Before Signal --- */ \\ /* Assuming signal names match: before_NAME */ \\ bool skip = sig_mgr.signal_##before_##C_API_FUNC(); \\ if (skip) { \\ std::cout \u0026lt;\u0026lt; \u0026#34;[Host Signal] Skipping \u0026#34; \u0026lt;\u0026lt; (NAME) \u0026lt;\u0026lt; \u0026#34; due to before_ signal.\u0026#34; \u0026lt;\u0026lt; std::endl; \\ } else { \\ /* --- Original C API Call --- */ \\ RET_TYPE original_result = C_API_FUNC((MGR_HANDLE)); \\ \\ /* --- After Signal --- */ \\ /* Assuming signal names match: after_NAME */ \\ /* Pass original result, combiner decides final result */ \\ final_result = sig_mgr.signal_##after_##C_API_FUNC(original_result); \\ } \\ /* --- Marshall result for WASM --- */ \\ results[0] = wasmtime::Val(static_cast\u0026lt;WASM_RET_TYPE\u0026gt;(final_result)); \\ return std::monostate(); \\ } catch (const wasmtime::Trap\u0026amp; trap) { \\ std::cerr \u0026lt;\u0026lt; \u0026#34;[Host Function Error] \u0026#34; \u0026lt;\u0026lt; (NAME) \u0026lt;\u0026lt; \u0026#34; trapped: \u0026#34; \u0026lt;\u0026lt; trap.message() \u0026lt;\u0026lt; std::endl; \\ return wasmtime::Trap(trap.message()); /* Create new trap */ \\ } catch (const std::exception\u0026amp; e) { \\ std::cerr \u0026lt;\u0026lt; \u0026#34;[Host Function Error] \u0026#34; \u0026lt;\u0026lt; (NAME) \u0026lt;\u0026lt; \u0026#34; exception: \u0026#34; \u0026lt;\u0026lt; e.what() \u0026lt;\u0026lt; std::endl; \\ return wasmtime::Trap(std::string(\u0026#34;Host function \u0026#34;) + (NAME) + \u0026#34; failed: \u0026#34; + e.what()); \\ } catch (...) { \\ std::cerr \u0026lt;\u0026lt; \u0026#34;[Host Function Error] \u0026#34; \u0026lt;\u0026lt; (NAME) \u0026lt;\u0026lt; \u0026#34; unknown exception.\u0026#34; \u0026lt;\u0026lt; std::endl; \\ return wasmtime::Trap(std::string(\u0026#34;Host function \u0026#34;) + (NAME) + \u0026#34; failed with unknown exception.\u0026#34;); \\ } \\ } \\ ).unwrap() /* Use unwrap() for example, check Result in prod */ // Other macros for different signatures (e.g., U32_VOID, U32_STR_VOID, U32_GET_STR...) // Example: Macro for uint32_t argument, void return #define DEFINE_HOST_FUNC_U32_VOID(LINKER, HOST_PTR, MGR_HANDLE, NAME, C_API_FUNC, WASM_TYPE) \\ (LINKER).func_new( \\ \u0026#34;env\u0026#34;, (NAME), (WASM_TYPE), \\ /* Lambda implementation similar to above */ \\ [(HOST_PTR), (MGR_HANDLE)](/* ... */) -\u0026gt; wasmtime::Result\u0026lt;std::monostate, wasmtime::Trap\u0026gt; { \\ /* ... extract uint32_t arg ... */ \\ uint32_t arg0_u32 = /* ... */; \\ try { \\ bool skip = sig_mgr.signal_##before_##C_API_FUNC(arg0_u32); \\ if (!skip) { \\ C_API_FUNC((MGR_HANDLE), arg0_u32); \\ sig_mgr.signal_##after_##C_API_FUNC(arg0_u32); \\ } else { /* Log skip */ } \\ return std::monostate(); /* Void return */ \\ } catch(/* ... trap/exception handling ... */) { /* ... */ } \\ } \\ ).unwrap() // Example: Macro for uint32_t argument, getting a string #define DEFINE_HOST_FUNC_U32_GET_STR(LINKER, HOST_PTR, MGR_HANDLE, NAME, C_API_FUNC, WASM_TYPE) \\ (LINKER).func_new( \\ \u0026#34;env\u0026#34;, (NAME), (WASM_TYPE), \\ /* Lambda implementation */ \\ [(HOST_PTR), (MGR_HANDLE)](/* ... */) -\u0026gt; wasmtime::Result\u0026lt;std::monostate, wasmtime::Trap\u0026gt; { \\ /* ... extract uint32_t id, char* buffer_ptr_offset, size_t buffer_len ... */ \\ uint32_t entity_id = /* ... */; \\ int32_t buffer_offset = /* ... */; \\ size_t buffer_len = /* ... */; \\ char* wasm_buffer = nullptr; \\ try { \\ /* Get memory and calculate wasm_buffer pointer safely */ \\ auto mem_span_opt = WasmHostUtils::get_wasm_memory_span_helper(caller); \\ if (!mem_span_opt) return wasmtime::Trap(\u0026#34;Failed to get WASM memory\u0026#34;); \\ auto\u0026amp; mem_span = mem_span_opt.value(); \\ if (buffer_offset \u0026gt;= 0 \u0026amp;\u0026amp; buffer_len \u0026gt; 0 /* ... more bounds checks ... */){ \\ wasm_buffer = reinterpret_cast\u0026lt;char*\u0026gt;(mem_span.data() + buffer_offset);\\ } else if (buffer_offset != 0 || buffer_len \u0026gt; 0) { /* Invalid buffer args */ } \\ \\ size_t final_req_len = 0; /* Default */ \\ bool skip = sig_mgr.signal_##before_##C_API_FUNC(entity_id, wasm_buffer, buffer_len); \\ if (!skip) { \\ size_t original_req_len = C_API_FUNC((MGR_HANDLE), entity_id, wasm_buffer, buffer_len); \\ /* Pass original_req_len to after signal */ \\ final_req_len = sig_mgr.signal_##after_##C_API_FUNC(entity_id, wasm_buffer, buffer_len, original_req_len); \\ } else { /* Log skip, return 0 */ final_req_len = 0; } \\ results[0] = wasmtime::Val(static_cast\u0026lt;int32_t\u0026gt;(final_req_len)); /* Return size_t as i32 */ \\ return std::monostate(); \\ } catch(/* ... trap/exception handling ... */) { /* ... */ } \\ } \\ ).unwrap() // ... More macros for other patterns ... The C++ macros defined in host_macros.h encapsulate several key elements essential for integrating the host\u0026rsquo;s C API functions with the Boost.Signals2 event system when exposing them to the WASM module via Wasmtime. Their primary function is boilerplate reduction; they conveniently wrap the necessary linker.func_new call required by Wasmtime and construct the complex lambda function that serves as the actual host function implementation callable by WASM.\nThese macros are highly parameterized to handle different function signatures. They typically accept arguments such as the Wasmtime linker object, a pointer to the WasmHost instance, the opaque handle for the EnttManager, the specific name the WASM module will use to import the function (referred to as NAME), a pointer to the underlying C API function being wrapped (C_API_FUNC), the corresponding Wasmtime function type definition, the expected C++ return type of the C API function, the corresponding WASM ABI type for the return value (e.g., int32_t for a C int or uint32_t), and a default return value to use if the operation is skipped by a signal.\nWithin the lambda generated by the macro, specific captures are essential. The lambda captures the HOST_PTR, which is crucial for gaining access to the SignalManager instance needed to emit signals, and it also captures the MGR_HANDLE, the opaque pointer required to invoke the original C API function.\nThe lambda implementation handles the intricate details of argument and result marshalling across the WASM boundary. It\u0026rsquo;s responsible for extracting incoming arguments from the Span\u0026lt;const wasmtime::Val\u0026gt; provided by Wasmtime, converting them to the types expected by the C API function. For functions dealing with buffers or strings, it performs necessary bounds checking, often using helper functions, to ensure memory safety when interacting with WASM\u0026rsquo;s linear memory. After the operation and potential signal handling, it marshals the final computed result back into the Span\u0026lt;wasmtime::Val\u0026gt; for the WASM caller.\nA core responsibility of the macro-generated lambda is signal emission. It first retrieves the SignalManager instance via the captured HOST_PTR. Then, before invoking the wrapped C API function, it emits the corresponding \u0026ldquo;before\u0026rdquo; signal. This emission uses C++ preprocessor token pasting (##) to dynamically construct the correct signal member name based on the C API function\u0026rsquo;s name (for example, combining signal_##before_## with entt_manager_create_entity results in signal_before_entt_manager_create_entity). The lambda carefully checks the return value provided by the \u0026quot; before\u0026quot; signal\u0026rsquo;s combiner (e.g., the boolean result from StopOnTrueCombiner). If this return value indicates that the operation should be skipped (typically true), the lambda logs a message and immediately returns the predefined default value to WASM, bypassing the call to the original C API function and the emission of the \u0026ldquo;after\u0026rdquo; signal. If the \u0026ldquo;before\u0026rdquo; signal does not indicate a skip, the lambda proceeds to call the original C API function (C_API_FUNC) using the captured manager handle and extracted arguments. Following the C API call, it emits the corresponding \u0026ldquo;after\u0026rdquo; signal, passing any relevant original arguments along with the result obtained from the C API call. Finally, it captures the return value generated by the \u0026ldquo;after\u0026rdquo; signal\u0026rsquo;s combiner (which might have been modified by WASM slots, for example, using optional_last_value) and uses this value as the final_result that is ultimately marshalled and returned to the WASM caller.\nLastly, robust error handling is built into the generated lambda. It includes comprehensive try-catch blocks designed to catch standard C++ exceptions (std::exception) as well as Wasmtime-specific traps (wasmtime::Trap) that might occur during the execution of the C API function, the signal emissions, or the slot invocations within WASM. These caught exceptions or traps are then safely converted into new wasmtime::Trap objects, ensuring that host-side errors are propagated back to the WASM runtime gracefully without crashing the host process. Special care is taken to correctly handle the move-only semantics of wasmtime::Trap when re-throwing or constructing new traps.\nIn host.cpp, we now replace the direct lambda definitions with calls to these macros for each host function we want to expose with signal support.\n// host.cpp (main, illustrative usage) // ... includes, setup ... // Get pointers and references WasmHost host(wasm_path); EnttManager* manager_raw_ptr = \u0026amp;host.getEnttManager(); EnttManagerHandle* manager_handle = reinterpret_cast\u0026lt;EnttManagerHandle*\u0026gt;(manager_raw_ptr); Linker\u0026amp; linker = host.getLinker(); Store\u0026amp; store = host.getStore(); WasmHost* host_ptr = \u0026amp;host; // For macro capture // SignalManager\u0026amp; signal_manager = host.getSignalManager(); // Not directly needed here host.setupWasi(); // Define function types... // Use the macros to define host functions DEFINE_HOST_FUNC_0_ARGS_RET(linker, host_ptr, manager_handle, \u0026#34;host_create_entity\u0026#34;, entt_manager_create_entity, void_to_i32_type, uint32_t, int32_t, FFI_NULL_ENTITY); DEFINE_HOST_FUNC_U32_VOID(linker, host_ptr, manager_handle, \u0026#34;host_destroy_entity\u0026#34;, entt_manager_destroy_entity, i32_to_void_type); DEFINE_HOST_FUNC_U32_GET_STR(linker, host_ptr, manager_handle, \u0026#34;host_get_name\u0026#34;, entt_manager_get_name, i32ptrlen_to_size_type); // ... Define ALL other host functions using the appropriate macros ... // Define the signal connection function (doesn\u0026#39;t need a macro as it doesn\u0026#39;t wrap a C API call) linker.func_new( \u0026#34;env\u0026#34;, \u0026#34;host_connect_signal\u0026#34;, /* type */ ..., // Capture signal_manager by reference [\u0026amp;signal_manager = host.getSignalManager()](...) { // Note capture detail // ... implementation using signal_manager.connectWasmSlot ... } ).unwrap(); host.initialize(); // Instantiate // ... Call connect_all_signals in WASM ... // ... Call test_relationships_oo in WASM ... // ... Manual test section calling linker.get() to ensure signal firing ... Connecting WASM Slots: host_connect_signal How does the WASM module tell the host, \u0026ldquo;Please connect my wasm_before_create_entity function to your before_create_entity signal\u0026rdquo;? We provide one more host function specifically for this: host_connect_signal.\nThis specific host function, host_connect_signal, is defined directly within host.cpp using linker.func_new and a lambda, rather than relying on one of the host function macros, primarily because it doesn\u0026rsquo;t wrap an existing C API function but instead provides new functionality specific to the signal system. The lambda implementation performs several distinct steps when invoked by the WASM module.\nFirst, it receives its necessary input arguments directly from the WASM caller via the Span\u0026lt;const Val\u0026gt; args. These arguments consist of pointers and lengths representing the signal name (signal_name_ptr, signal_name_len) and the WASM function name (wasm_func_name_ptr, wasm_func_name_len), along with an integer representing the desired connection priority.\nNext, to safely retrieve the actual string values from the potentially insecure pointers provided by WASM, the lambda utilizes the WasmHostUtils::read_wasm_string_helper utility function. This helper reads the specified number of bytes from the WASM linear memory at the given offsets, performing necessary bounds checks and returning the strings.\nCrucially, the lambda is defined in a way that it captures a reference to the host\u0026rsquo;s central SignalManager instance. This captured reference provides the context needed to interact with the signal system.\nWith the signal and function names successfully read and the SignalManager accessible, the core logic of the lambda is executed: it invokes the connectWasmSlot method on the captured signal_manager, passing the retrieved signal_name, wasm_func_name, and priority as arguments. This call delegates the actual task of creating and registering the signal-slot connection to the SignalManager.\nFinally, after the connection attempt, the lambda returns the outcome back to the WASM module. It takes the boolean success status returned by connectWasmSlot and marshals it into the expected FFI format, typically an int32_t (1 for success, 0 for failure), which is placed into the Span\u0026lt;Val\u0026gt; results for the WASM caller to interpret.\nThis provides the crucial link, allowing the WASM module to dynamically register its handlers during its initialization phase.\nWASM-Side Adaptation: Becoming a Signal Client The Rust WASM module now needs to adapt to this new signal-based system.\nThe first step in adapting the Rust WASM module involves dismantling the previous custom eventing infrastructure. This cleanup requires removing the remnants of the now-obsolete trigger and patching systems. Specifically, the src/patch_handler.rs file, along with the PatchHandler trait defined within it, must be entirely deleted from the project. Correspondingly, within the FFI layer defined in src/ffi.rs, the extern \u0026quot;C\u0026quot; import declarations that previously brought in the host functions related to registering patches and triggers, namely host_register_patch and host_register_trigger, need to be removed. Finally, the exported WASM functions that served as the initialization entry points for these old systems, init_patches and init_triggers, must also be removed from the exports list, as the host will no longer call them.\nWith the old plumbing removed, a new mechanism must be established to allow the WASM module to initiate the connection of its handlers to the host\u0026rsquo;s signals. This new process involves several coordinated steps within the Rust code. First, the necessary FFI import declaration for the new host function responsible for handling connections, host_connect_signal, must be added to the extern \u0026quot;C\u0026quot; block located in src/ffi.rs, mirroring the function signature defined on the host side. Second, to encapsulate the unsafe FFI interaction, a safe Rust wrapper function, ffi::connect_signal, needs to be created. This wrapper function should accept standard Rust string slices (\u0026amp;str) for the signal name and the WASM function name, along with an integer priority. Its implementation will handle the necessary conversions of these Rust strings into null-terminated CStrings suitable for the FFI call and will contain the unsafe block required to invoke the imported host_connect_signal function, returning a boolean indicating the success or failure of the connection attempt. Third, the responsibility for orchestrating all necessary connections from the WASM side is centralized within a new function, core::connect_all_signals, implemented in src/core.rs. This function\u0026rsquo;s sole purpose is to repeatedly call the safe ffi::connect_signal wrapper, systematically pairing the known string names of the signals exposed by the host (such as \u0026quot;before_create_entity\u0026quot;) with the string names of the corresponding exported WASM functions designed to handle those signals (like \u0026quot;wasm_before_create_entity\u0026quot;), along with their desired priorities. Fourth and finally, to expose this connection logic to the host, a C-compatible function named connect_all_signals needs to be exported from src/ffi.rs using #[no_mangle] pub unsafe extern \u0026quot;C\u0026quot;. The implementation of this exported function simply delegates the actual work by calling core::connect_all_signals(). The C++ host application will then be responsible for invoking this single exported connect_all_signals function exactly once, typically right after the WASM module has been successfully instantiated, thereby triggering the registration of all defined WASM signal handlers with the host\u0026rsquo;s SignalManager.\n// src/ffi.rs (Snippets) // ... other imports ... #[link(wasm_import_module = \u0026#34;env\u0026#34;)] unsafe extern \u0026#34;C\u0026#34; { // --- Signal Connection Import (NEW) --- fn host_connect_signal( signal_name_ptr: *const c_char, signal_name_len: usize, wasm_func_name_ptr: *const c_char, wasm_func_name_len: usize, priority: c_int, ) -\u0026gt; c_int; // Returns bool (0 or 1) for success // ... other host function imports remain ... } // --- Signal Connection Wrapper (NEW) --- pub fn connect_signal( signal_name: \u0026amp;str, wasm_func_name: \u0026amp;str, priority: i32, ) -\u0026gt; bool { // ... (Implementation as shown previously, using CString::new and unsafe call) ... let success_code = unsafe { host_connect_signal(...) }; success_code != 0 } // --- Exported Function for Host to Trigger Connections --- #[no_mangle] pub unsafe extern \u0026#34;C\u0026#34; fn connect_all_signals() { println!(\u0026#34;[WASM Export] connect_all_signals called. Connecting handlers via core...\u0026#34;); crate::core::connect_all_signals(); // Delegate to core logic } // --- Exported Signal Handler Implementations (Slots) --- // ... (Functions like wasm_before_create_entity as defined previously) ... // --- Test Runner Export --- #[no_mangle] pub unsafe extern \u0026#34;C\u0026#34; fn test_relationships_oo() { // ... runs core::run_entt_relationship_tests_oo ... } // src/core.rs (Snippet) use crate::ffi::{connect_signal, DEFAULT_SIGNAL_PRIORITY /* ... */}; /// Connects all WASM signal handlers (slots) to the corresponding host signals. /// Called by the host via the exported `connect_all_signals` function in ffi.rs. pub fn connect_all_signals() { println!(\u0026#34;[WASM Core] Connecting WASM functions to host signals...\u0026#34;); let mut success = true; // Connect slots for host_create_entity success \u0026amp;= connect_signal( \u0026#34;before_create_entity\u0026#34;, // Host signal name (string) \u0026#34;wasm_before_create_entity\u0026#34;, // Exported WASM function name (string) DEFAULT_SIGNAL_PRIORITY, ); success \u0026amp;= connect_signal( \u0026#34;after_create_entity\u0026#34;, // Host signal name \u0026#34;wasm_after_create_entity\u0026#34;, // Exported WASM function name DEFAULT_SIGNAL_PRIORITY, ); // ... connect ALL other slots similarly ... success \u0026amp;= connect_signal( \u0026#34;after_get_profile_for_player\u0026#34;, \u0026#34;wasm_after_get_profile_for_player_high_prio\u0026#34;, // Name matches exported function DEFAULT_SIGNAL_PRIORITY + 100, // Higher priority number ); if success { /* Log success */ } else { /* Log failure */ } } // ... run_entt_relationship_tests_oo() remains largely the same ... Implementing WASM Signal Slots The Rust functions that were previously designated for the custom patching mechanism, such as prefix_create_entity, are now either repurposed or replaced by new functions specifically designed to serve as the signal slots within the Boost.Signals2 framework. For these functions to correctly receive signals emitted by the host, they must adhere to two fundamental requirements.\nFirstly, they must be properly exported from the WASM module so that the host\u0026rsquo;s SignalManager (via Wasmtime) can locate and invoke them when connecting or firing signals. This necessitates marking each slot function with #[no_mangle] to prevent Rust\u0026rsquo;s name mangling and declaring it as pub unsafe extern \u0026quot;C\u0026quot; to ensure C-compatible linkage and calling conventions. Critically, the exact name assigned to each exported slot function in the Rust code must perfectly match the string literal used when connecting it within the core::connect_all_signals function; any discrepancy will result in a connection failure.\nSecondly, and equally crucial, the function signature of each WASM slot – encompassing both its parameters and its return type – must precisely align with the expectations hardcoded into the corresponding host-side slot wrapper lambda. These wrapper lambdas are defined within the SignalManager::initializeConnectorFactories method in the C++ host. Any mismatch in the number of parameters, their types, or the return type will lead to undefined behavior or runtime traps when the host attempts to call the WASM slot. For instance, the slot wasm_before_create_entity() is expected by the host wrapper to take no arguments and return a c_int, where 0 signifies continuation and 1 indicates the operation should be skipped. Similarly, wasm_after_create_entity(original_id: u32) must accept a u32 representing the original entity ID and return a u32, allowing it the opportunity to modify the ID passed back through the signal chain. A slot like wasm_after_destroy_entity(entity_id: u32) is expected to accept the ID but return void, as it functions purely as a notification. More complex cases like wasm_before_get_name(entity_id: u32, buffer_len: u32) demonstrate a simplification in the FFI signature; the host wrapper expects it to receive the entity ID and the intended buffer length but not the host-side buffer pointer itself, returning a c_int (0 or 1) to potentially veto the get_name operation. This design choice avoids the complexity and potential unsafety of the WASM slot directly accessing the host buffer; should the slot require the actual string content during this \u0026ldquo;before\u0026rdquo; phase, it would need to initiate a separate call back into the host (e.g., using host_get_name itself). Correspondingly, the wasm_after_get_name(entity_id: u32, buffer_len: u32, original_req_len: u32) slot receives the ID, buffer length, and the original required length calculated by the C API, and is expected to return a u32 representing the potentially adjusted required length. This pattern of precisely matching the parameter list and return type defined implicitly by the host\u0026rsquo;s slot wrapper lambda must be rigorously applied to all other WASM functions intended to serve as signal slots for the various host events.\n// src/ffi.rs (Slot Implementation Snippets) // --- host_create_entity --- #[no_mangle] pub unsafe extern \u0026#34;C\u0026#34; fn wasm_before_create_entity() -\u0026gt; c_int { println!(\u0026#34;[WASM Slot] \u0026lt;\u0026lt;\u0026lt; before_create_entity called\u0026#34;); 0 // Allow creation } #[no_mangle] pub unsafe extern \u0026#34;C\u0026#34; fn wasm_after_create_entity(original_id: u32) -\u0026gt; u32 { println!(\u0026#34;[WASM Slot] \u0026lt;\u0026lt;\u0026lt; after_create_entity called (Original ID: {})\u0026#34;, original_id); original_id // Return original ID } // --- host_get_profile_for_player --- #[no_mangle] pub unsafe extern \u0026#34;C\u0026#34; fn wasm_before_get_profile_for_player(player_id: u32) -\u0026gt; c_int { println!(\u0026#34;[WASM Slot] \u0026lt;\u0026lt;\u0026lt; before_get_profile_for_player called (P: {})\u0026#34;, player_id); 0 // Allow get } // Default priority postfix slot #[no_mangle] pub unsafe extern \u0026#34;C\u0026#34; fn wasm_after_get_profile_for_player(player_id: u32, original_profile_id: u32) -\u0026gt; u32 { println!(\u0026#34;[WASM Slot] \u0026lt;\u0026lt;\u0026lt; after_get_profile_for_player called (P: {}, OrigProf: {})\u0026#34;, player_id, original_profile_id); // This one just observes original_profile_id } // High priority postfix slot (runs AFTER the default one) #[no_mangle] pub unsafe extern \u0026#34;C\u0026#34; fn wasm_after_get_profile_for_player_high_prio(player_id: u32, current_profile_id: u32) -\u0026gt; u32 { println!( \u0026#34;[WASM Slot][HIGH PRIO] \u0026lt;\u0026lt;\u0026lt; after_get_profile_for_player_high_prio called (P: {}, CurrentProf: {})\u0026#34;, player_id, current_profile_id // current_profile_id is the result from the previous slot/original call ); // Example: Override profile ID for player 2 if player_id == 2 { println!(\u0026#34; \u0026gt; [HIGH PRIO] Changing profile for player 2 to 888!\u0026#34;); return 888; // Override the value } current_profile_id // Otherwise, return the value passed in } // ... Implement ALL other exported slot functions ... Now, when the host emits signal_before_create_entity, the wasm_before_create_entity function in the WASM module will be executed. When the host emits signal_after_get_profile_for_player, both wasm_after_get_profile_for_player and wasm_after_get_profile_for_player_high_prio will run (in priority order), and the optional_last_value combiner will ensure the final result seen by the host macro is the value returned by the high-priority slot.\nThe Full Picture: Execution Flow with Signals To understand the interplay between the WASM module, the host, and the signal system, let\u0026rsquo;s trace the sequence of events when the WASM module initiates an entity creation by calling host_create_entity. We assume the WASM slots wasm_before_create_entity and wasm_after_create_entity have already been successfully connected to the corresponding host signals via connect_all_signals.\nThe process begins within the WASM module. A call to the higher-level entity::create_entity() function occurs, which in turn invokes the lower-level FFI wrapper ffi::create_entity(). Inside this FFI wrapper, the unsafe block executes the actual call across the boundary: host_create_entity().\nControl now transfers to the C++ host. The specific lambda wrapper function, previously generated by the DEFINE_HOST_FUNC_0_ARGS_RET macro and registered with Wasmtime\u0026rsquo;s linker for the name host_create_entity, receives this incoming call. The first action within this host lambda is to obtain a reference to the SignalManager. Following this, the lambda emits the signal_before_create_entity signal, passing no arguments as per the signal\u0026rsquo;s definition.\nThe Boost.Signals2 framework intercepts this signal emission and proceeds to invoke any slots connected to signal_before_create_entity. In our scenario, this triggers the execution of the host-side slot wrapper lambda that was created specifically for the WASM function \u0026quot;wasm_before_create_entity\u0026quot;. This slot wrapper lambda prepares its arguments (none in this case) and executes a Wasmtime call back into the module: host-\u0026gt;callFunction\u0026lt;int32_t\u0026gt;(\u0026quot;wasm_before_create_entity\u0026quot;, ...).\nExecution jumps back to the WASM module, specifically to the wasm_before_create_entity() function. This function runs its logic, typically printing a log message indicating it was called, and then returns its result, which is 0 ( representing false in C ABI boolean convention), signaling that the operation should proceed.\nBack in the host, the slot wrapper lambda receives the int32_t result (0) from the Wasmtime call and unmarshals it into a C++ bool (false). This boolean result is then passed back to the Boost.Signals2 framework. The StopOnTrueCombiner associated with signal_before_create_entity receives this false value. Since it\u0026rsquo;s not true, the combiner allows processing to continue (if other slots were connected, they would run now). Ultimately, the combiner returns false to the original host function lambda that emitted the signal.\nThe host lambda checks the skip flag returned by the combiner. Since it\u0026rsquo;s false, the lambda determines that the operation should not be skipped and proceeds with the core logic. It now calls the underlying C API function: entt_manager_create_entity(manager_handle). This C API function, in turn, calls the EnttManager::createEntity() method on the C++ manager object. Inside the manager, registry_.create() is invoked, a new EnTT entity is created, its ID is converted to uint32_t, a creation log message is printed, and this uint32_t ID is returned.\nThe ID (original_result) travels back up the call stack from EnttManager to the C API function, and then to the host lambda. Now, the host lambda emits the second signal: signal_after_create_entity(original_result), passing the newly created entity\u0026rsquo;s ID.\nAgain, Boost.Signals2 takes over, invoking the slots connected to signal_after_create_entity. This leads to the execution of the slot wrapper lambda associated with \u0026quot;wasm_after_create_entity\u0026quot;, which is called with the original_id. This wrapper lambda prepares its arguments (packing the original_id into a wasmtime::Val) and calls back into the module: host-\u0026gt;callFunction\u0026lt;int32_t\u0026gt;(\u0026quot;wasm_after_create_entity\u0026quot;, ...). Note the expected return is int32_t because the WASM function returns u32, which fits in i32.\nExecution returns to WASM\u0026rsquo;s wasm_after_create_entity(original_id) function. It executes its logic (e.g., logging) and, in this example, simply returns the original_id it received.\nThe host slot wrapper receives this ID as an int32_t result from Wasmtime and unmarshals it back into a uint32_t. This value is passed to the Boost.Signals2 framework. The optional_last_value\u0026lt;uint32_t\u0026gt; combiner associated with signal_after_create_entity receives this result. As it\u0026rsquo;s the only (or the last) slot executed, the combiner wraps this value and returns boost::optional\u0026lt;uint32_t\u0026gt;(result) to the host lambda.\nThe host lambda receives the combiner\u0026rsquo;s result (boost::optional\u0026lt;uint32_t\u0026gt;). It extracts the contained value (or would use a default if the optional were empty, though not expected here). This extracted value becomes the final_result. The lambda then marshals this final_result (the entity ID) into the results span as a wasmtime::Val of kind I32 for the original WASM caller.\nFinally, the host lambda completes its execution by returning success (std::monostate()) to the Wasmtime runtime. Wasmtime then returns control back to the point where the initial host_create_entity() call was made within WASM\u0026rsquo;s ffi::create_entity function. This function receives the ID and returns it up to entity::create_entity, which then uses Entity::new(id) to create the final Rust wrapper object for the newly created entity. This completes the entire cross-boundary call sequence, including signal interceptions.\nThis detailed flow illustrates the powerful orchestration provided by Boost.Signals2, handling slot invocation, argument passing (from signal to slot wrapper), return value combination, and allowing interception points before and after the core C API logic, all while integrating with Wasmtime calls across the FFI boundary.\nBenefits and Considerations Revisited This significant refactoring effort yields substantial benefits for the overall architecture and maintainability of the C++/Rust/WASM integration. Foremost among these is the establishment of a unified mechanism; the Boost.Signals2 system now replaces both the previous custom trigger implementation and the separate patching framework, providing a single, consistent model for handling events between the host and the plugin. This contributes significantly to the system\u0026rsquo;s robustness. Boost.Signals2 inherently manages signal-slot connections automatically, effectively preventing the common issue of dangling callbacks that could arise in manual systems. Furthermore, its built-in combiner concept offers standard and predictable methods for aggregating or processing results when multiple listeners (WASM slots) respond to the same host signal. The refactoring also promotes better decoupling within the host application. The C API implementation layer (entt_api.cpp), for instance, becomes considerably simpler as it no longer needs any intrinsic awareness of the trigger or patching logic. The EnttManager class is similarly streamlined, offloading event management responsibilities. Instead, the newly introduced C++ macros and the dedicated SignalManager now cleanly encapsulate the logic related to signal emission and connection management. The system gains considerable flexibility through the features offered by Boost.Signals2; assignable priorities allow for precise control over the execution order of different WASM slots connected to the same signal, while the availability of various combiners enables the implementation of diverse interaction patterns, such as allowing WASM to veto host actions, modify return values, or simply receive notifications. Ultimately, this leads to improved maintainability. The clearer separation of concerns between core logic, the C API, the signal management infrastructure, and the WASM FFI/slot implementation, combined with the reliance on a well-established standard library like Boost.Signals2, makes the entire codebase easier for developers to understand, debug, and modify safely over time.\nHowever, adopting this approach also introduces several considerations that must be acknowledged. The most obvious is the introduction of a new external dependency on the Boost library, specifically requiring Boost.Signals2 which, depending on the build system and configuration, might implicitly pull in other Boost components. There is also an inherent increase in conceptual complexity; developers working with the system now need to understand the core concepts of Boost.Signals2, including signals, slots, combiners, connection management, and the specific factory pattern used within our SignalManager, which represents an initial learning curve compared to the simpler, albeit less robust, custom solutions. Additionally, the C++ macro magic employed in host_macros.h, while effective at reducing repetitive boilerplate code for signal emission, can also introduce a layer of opacity, potentially making it harder to debug the exact flow of control within the host function wrappers without understanding the macro expansions. A critical point of potential fragility remains in the FFI signature matching: the contract between the C++ host\u0026rsquo;s slot wrapper lambda ( defined within the signal connector factory) and the signature of the exported Rust WASM slot function it intends to call must be manually synchronized with extreme care. Any mismatch in parameter types, number of parameters, or return types will not be caught at compile time but will manifest as difficult-to-diagnose runtime traps or undefined behavior. Lastly, the reliance on string-based names persists during the crucial connection phase. Both the host-side connectWasmSlot method and the WASM-side connect_signal wrapper function operate using string literals to identify signals and WASM functions. Simple typographical errors in these string names will result in silent connection failures, which can be challenging to track down without careful logging or debugging procedures on both sides of the FFI boundary.\nConclusion: A More Elegant Bridge By replacing our custom eventing system with Boost.Signals2, we\u0026rsquo;ve significantly elevated the sophistication and robustness of the interaction between our C++ EnTT host and the Rust WASM plugin. We now have a unified, flexible, and more maintainable mechanism for the host and plugin to react to each other\u0026rsquo;s actions, intercept operations, and modify results in a controlled manner.\nThe SignalManager centralizes signal definition and connection logic, while the C++ macros provide a clean way to instrument our existing host C API functions with signal emissions. On the WASM side, exporting dedicated slot functions and using a single host call (host_connect_signal) to register them simplifies the plugin\u0026rsquo;s responsibility. Features like combiners (StopOnTrueCombiner, optional_last_value) and priorities unlock powerful patterns like vetoing actions or overriding results, all managed by the Boost.Signals2 framework.\nWhile it introduces a Boost dependency and requires understanding its concepts, the payoff in terms of reduced custom code complexity, automatic connection management, and standardized event handling is substantial. This architecture provides a solid foundation for building even more intricate and dynamic interactions across the WASM boundary, proving that even complex event-driven communication is achievable with the right tools and design patterns.\nOur journey continues, but this refactoring marks a significant step towards a more mature and production-ready C++/Rust/WASM integration.\n","permalink":"https://blog.tategotoazarasi.me/en/posts/beyond-basic-bridging-robust-eventing-between-cpp-entt-and-rust-wasm-with-boost-signals2/","summary":"Refactor a C++ EnTT host and Rust WASM plugin, replacing custom event triggers with Boost.Signals2 via Wasmtime for robust, decoupled FFI communication and advanced host-plugin interaction.","title":"Beyond Basic Bridging: Robust Eventing Between C++ EnTT and Rust WASM with Boost.Signals2"},{"content":"In our previous discussions, we explored the power of EnTT, a high-performance C++ ECS library (especially its approach to relationship management), and separately, how to use Wasmtime for interactions between a C++ host and Rust-compiled WebAssembly (WASM) modules (a recap on WASM Interaction Basics). Today, we\u0026rsquo;re merging these two potent technologies to tackle a more challenging yet highly rewarding topic: How can we manage entity relationships using EnTT within a C++ host and expose this management capability safely and efficiently to a Rust WASM plugin?\nThis isn\u0026rsquo;t just a simple tech mashup. It strikes at the heart of several core challenges in modern software architecture: modularity, sandboxed security, high performance, and enabling effective communication between different tech stacks – particularly between traditional object-oriented languages like C++ and environments like WASM that don\u0026rsquo;t inherently understand objects.\nHitting the WASM Boundary with C++ Imagine a mature C++ application – perhaps a game engine, simulator, or desktop tool. We want to enhance its extensibility, security, or allow third-party contributions using a WASM plugin system. It sounds great in theory, but we quickly encounter a practical hurdle: the inherent boundary between the WASM module and the C++ host.\nWASM operates within a strict sandbox. This imposes several crucial limitations when interacting with a C++ host. Firstly, WASM cannot directly access the host\u0026rsquo;s general memory address space; its view is confined to the explicitly exported linear memory buffer provided by the host. Secondly, direct calls to arbitrary C++ functions from WASM are forbidden; only functions explicitly exposed by the host through the WASM import mechanism can be invoked by the module. Thirdly, and often the biggest hurdle when coming from C++, WASM lacks any inherent understanding or capability to directly manipulate the host\u0026rsquo;s object-oriented concepts. It cannot work with C++ classes or objects in their native form, recognize inheritance hierarchies, or utilize virtual functions. As a result, attempting to instantiate a C++ object, directly call its member functions, or inherit from a C++ class from within the WASM environment is fundamentally impossible.\nThis poses a significant problem for developers accustomed to C++ OOP design. If we want a WASM plugin to interact with objects in the C++ application (like characters or items in a game world), simply passing C++ object pointers won\u0026rsquo;t work, and invoking member functions is impossible. Traditional plugin architectures, often relying on polymorphic interfaces via virtual functions, break down at the WASM boundary.\nEnTT\u0026rsquo;s Data-Driven Philosophy Just when this boundary seems insurmountable, EnTT\u0026rsquo;s design philosophy offers a way through. Recall the core tenets of EnTT we discussed, which center on a data-oriented approach. An entity, in EnTT\u0026rsquo;s paradigm, is not an object in the traditional C++ sense but rather a lightweight, opaque identifier (ID). This ID cleverly encodes an index and a version number, providing a robust and safe way to reference a conceptual \u0026ldquo;thing\u0026rdquo; in the application without the complexities of object identity or memory addresses. Data describing the state and properties of these entities is stored in components. These are typically designed as pure data containers, often resembling Plain Old Data Structures (PODS), and are directly associated with entity IDs within the system. The logic that operates on this data is encapsulated in systems. Systems query for entities that possess specific combinations of components and then process them accordingly. Within EnTT, systems are commonly implemented as straightforward free functions, lambdas, or functors that interact with the central entt::registry to access and modify the component data associated with entities.\nThis data-driven approach is fundamentally different from OOP and aligns remarkably well with WASM\u0026rsquo;s interaction model for several key reasons. First, the portability of EnTT\u0026rsquo;s entity IDs is paramount. Although entt::entity incorporates internal complexity for safety (like versioning), it can be reliably converted into a simple integer type, such as uint32_t, suitable for transmission across the FFI boundary. This integer ID then serves as a stable, unambiguous handle for referencing a specific conceptual \u0026ldquo;thing\u0026rdquo; within the host\u0026rsquo;s EnTT world, eliminating the need for the WASM plugin to comprehend complex C++ object memory layouts – it only needs the ID. Second, components naturally function as data contracts between the host and plugin. Since components in EnTT are primarily data structures, their defined memory layout can be agreed upon by both the C++ host and the Rust WASM plugin. By utilizing the shared linear memory space exported by the host, both sides gain the ability to read and write this component data directly according to the established structure, facilitating state synchronization. Finally, while direct invocation of C++ functions or EnTT systems from WASM is prohibited, logic execution can be achieved indirectly. The host builds an interface by providing a carefully selected set of C functions exposed via an FFI. These host-side FFI functions encapsulate the necessary logic, interacting internally with the entt::registry to perform actions like creating entities, adding or removing components, querying data, and, crucially for our case, managing relationships. The WASM plugin then simply imports these specific FFI functions and calls them to trigger the desired operations within the host\u0026rsquo;s EnTT system.\nThis combination forms the cornerstone of our solution: We leverage EnTT\u0026rsquo;s portable entity IDs for cross-boundary referencing, utilize components as the shared data contract through linear memory, and construct an FFI API layer to serve as the essential bridge for invoking host-side logic from the WASM plugin.\nToday\u0026rsquo;s Goal and Architecture Overview This blog post will detail how we implement the EnTT relationship management patterns (1:1, 1:N, N:N) discussed previously, integrating them into the C++/Rust/WASM architecture.\nOn the C++ host side, the implementation involves several key components working together. An EnttManager class serves as the central hub, encapsulating the entt::registry instance and implementing the specific logic for managing entity relationships, thereby providing a clean, internal C++-facing API. To bridge the gap to WASM, a distinct C API layer, defined in entt_api.h and implemented in entt_api.cpp, wraps the necessary EnttManager methods within extern \u0026quot;C\u0026quot; functions. This layer ensures a stable FFI by using only C-compatible types and establishing clear conventions, such as converting C++ bool to C int, defining a specific integer constant (FFI_NULL_ENTITY) to represent the null entity state, and employing a two-call buffer pattern for safely exchanging variable-length data like strings and vectors across the boundary. Finally, the WasmHost class, along with the application\u0026rsquo;s main function, orchestrates the Wasmtime environment, setting up the Engine, Store, and optional WASI support. It utilizes the Wasmtime C++ API, specifically linker.func_new with C++ lambdas, to register the C API functions as host functions importable by the WASM module. A crucial step here is associating the single EnttManager instance with the Wasmtime Store\u0026rsquo;s user data slot, enabling the host function lambdas to access the correct manager instance when called from WASM. The main function concludes by initiating the interaction, typically by calling an exported function within the WASM module to execute the defined tests or plugin logic.\nComplementing the host setup, the Rust WASM plugin side is structured for safety and clarity. An FFI layer, residing in ffi.rs, directly mirrors the host\u0026rsquo;s C API. It uses extern \u0026quot;C\u0026quot; blocks along with the #[link(wasm_import_module = \u0026quot;env\u0026quot;)] attribute to declare the host functions it expects to import. This module isolates all necessary unsafe blocks required for calling the external C functions, providing safe Rust wrappers around them. These wrappers handle the FFI-specific details, such as converting the C int back to Rust bool, mapping the FFI_NULL_ENTITY constant to Rust\u0026rsquo;s Option\u0026lt;u32\u0026gt;, and correctly implementing the two-call buffer pattern to interact with host functions that return strings or vectors. Above this FFI layer sits the core logic layer, typically within lib.rs::core. This is where the main functionality of the plugin is implemented using entirely safe Rust code. It operates solely through the safe wrapper functions exposed by the ffi.rs module, allowing it to interact with the host\u0026rsquo;s EnTT world and manage entity relationships without directly dealing with unsafe FFI calls or raw memory manipulation. For this demonstration, the core logic consists of tests exercising the various relationship management functions provided by the host.\nThe architecture looks like this:\nLet\u0026rsquo;s dive into the implementation details and design considerations for each part.\nCrafting the C++ Host: The EnTT World and its WASM Interface The C++ host holds the ground truth – the EnTT state – and defines the rules of engagement for the WASM plugin.\nEnttManager: Encapsulating the EnTT World Exposing entt::registry directly across an FFI boundary is impractical and breaks encapsulation. The EnttManager class acts as a dedicated layer, managing the registry and offering a higher-level API focused on our specific needs ( entities, components, and relationships).\n// entt_manager.h (Key Parts) #include \u0026lt;entt/entt.hpp\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;string\u0026gt; // ... other necessary includes ... class EnttManager { private: entt::registry registry_; // Relationship Component definitions (PlayerRelation, ParentComponent, etc.) // struct PlayerRelation { entt::entity profileEntity = entt::null; }; ... // Internal helper methods for complex logic // void unlinkPlayerProfileInternal(entt::entity entity); ... // *** The Crucial Hook for EnTT\u0026#39;s destroy signal *** // Signature must match entt::registry::on_destroy void cleanupRelationshipsHook(entt::registry\u0026amp; registry, entt::entity entity); // The actual cleanup logic called by the hook void cleanupRelationships(entt::entity entity); // Static helpers for consistent ID conversion across the FFI static uint32_t to_ffi(entt::entity e); static entt::entity from_ffi(uint32_t id); public: EnttManager(); // Constructor connects the cleanup hook ~EnttManager(); // Prevent copying/moving to avoid issues with registry state and signal connections EnttManager(const EnttManager\u0026amp;) = delete; EnttManager\u0026amp; operator=(const EnttManager\u0026amp;) = delete; // ... (move operations also deleted) ... // Public API using uint32_t for entity IDs uint32_t createEntity(); void destroyEntity(uint32_t entity_id); bool isEntityValid(uint32_t entity_id); // Note: returns bool internally // ... Component Management API (addName, getName, etc.) ... // ... Relationship Management API (linkPlayerProfile, setParent, etc.) ... }; Several key design decisions make the EnttManager effective. Strong encapsulation is maintained by keeping the entt::registry instance private; all external interactions must occur through the manager\u0026rsquo;s public methods, offering a well-defined and controlled interface to the underlying ECS state. To bridge the FFI gap for entity identification, the manager handles ID conversion internally. While it uses the entt::entity type for its core operations, its public API consistently exposes entities as simple uint32_t integers. Static helper methods, to_ffi and from_ffi, manage this translation, ensuring correct mapping between the internal entt::null state and the designated C API constant FFI_NULL_ENTITY. The implementation relies on the component-based relationship patterns previously established, utilizing structures like PlayerRelation, ParentComponent, and CoursesAttended directly within the registry to represent the connections between entities. Perhaps the most crucial feature is the automated relationship cleanup mechanism. This is achieved by leveraging EnTT\u0026rsquo;s signal system within the EnttManager constructor, where a dedicated hook method (cleanupRelationshipsHook) is connected to the registry\u0026rsquo;s on_destroy\u0026lt;entt::entity\u0026gt;() signal. This hook, which matches the signal\u0026rsquo;s required signature (entt::registry\u0026amp;, entt::entity), simply forwards the destroyed entity to the private cleanupRelationships(entt::entity) method. The essential behavior here stems from EnTT\u0026rsquo;s destruction process: when registry.destroy() is called, the on_destroy signal is emitted first, triggering our cleanup logic before the entity and its associated components are actually removed from the registry. This critical timing allows the cleanupRelationships method to inspect the registry state while the soon-to-be-destroyed entity still technically exists. Its responsibility is then to proactively find any remaining references to this destroyed entity held by other entities (like a ParentComponent on a child or an entry in a CoursesAttended vector) and remove or nullify those references, thereby automatically preserving relational integrity and preventing dangling pointers across the system.\nThe C API Layer: A Stable FFI Bridge (entt_api.h/.cpp) C++ features like classes, templates, and operator overloading cannot cross the FFI boundary. We need a stable interface based on the C ABI.\n// entt_api.h (Key Parts) #include \u0026lt;stdint.h\u0026gt; #include \u0026lt;stddef.h\u0026gt; #include \u0026lt;limits.h\u0026gt; // For UINT32_MAX // Opaque pointer to hide C++ implementation typedef struct EnttManagerOpaque EnttManagerHandle; ##ifdef __cplusplus extern \u0026#34;C\u0026#34; { ##endif // Define the null entity sentinel consistently for FFI const uint32_t FFI_NULL_ENTITY = UINT32_MAX; // Example Function Signatures int entt_manager_is_entity_valid(EnttManagerHandle* manager, uint32_t entity_id); // Returns int (0/1) for bool int entt_manager_link_player_profile(EnttManagerHandle* manager, uint32_t player_id, uint32_t profile_id); // Returns int // Two-stage call pattern for getting variable-length data size_t entt_manager_get_name(EnttManagerHandle* manager, uint32_t entity_id, char* buffer, size_t buffer_len); size_t entt_manager_find_children(EnttManagerHandle* manager, uint32_t parent_id, uint32_t* buffer, size_t buffer_len); // ... other C API declarations ... ##ifdef __cplusplus } // extern \u0026#34;C\u0026#34; ##endif // entt_api.cpp (Key Parts) #include \u0026#34;entt_api.h\u0026#34; #include \u0026#34;entt_manager.h\u0026#34; #include \u0026lt;vector\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026lt;cstring\u0026gt; // For memcpy #include \u0026lt;algorithm\u0026gt; // For std::min // Safely cast the opaque handle back to the C++ object inline EnttManager* as_manager(EnttManagerHandle* handle) { return reinterpret_cast\u0026lt;EnttManager*\u0026gt;(handle); } extern \u0026#34;C\u0026#34; { // Example Implementations int entt_manager_is_entity_valid(EnttManagerHandle* manager, uint32_t entity_id) { return as_manager(manager)-\u0026gt;isEntityValid(entity_id) ? 1 : 0; // Convert bool to int } int entt_manager_link_player_profile(EnttManagerHandle* manager, uint32_t player_id, uint32_t profile_id) { return as_manager(manager)-\u0026gt;linkPlayerProfile(player_id, profile_id) ? 1 : 0; // Convert bool to int } size_t entt_manager_get_name(EnttManagerHandle* manager, uint32_t entity_id, char* buffer, size_t buffer_len) { std::optional\u0026lt;std::string\u0026gt; name_opt = as_manager(manager)-\u0026gt;getName(entity_id); if (!name_opt) return 0; const std::string\u0026amp; name = *name_opt; size_t required_len = name.length() + 1; // For null terminator if (buffer != nullptr \u0026amp;\u0026amp; buffer_len \u0026gt; 0) { size_t copy_len = std::min(name.length(), buffer_len - 1); memcpy(buffer, name.c_str(), copy_len); buffer[copy_len] = \u0026#39;\\0\u0026#39;; // Ensure null termination } return required_len; // Always return the needed length } size_t entt_manager_find_children(EnttManagerHandle* manager, uint32_t parent_id, uint32_t* buffer, size_t buffer_len) { std::vector\u0026lt;uint32_t\u0026gt; children_ids = as_manager(manager)-\u0026gt;findChildren(parent_id); size_t count = children_ids.size(); // buffer_len is the capacity in number of uint32_t elements if (buffer != nullptr \u0026amp;\u0026amp; buffer_len \u0026gt;= count \u0026amp;\u0026amp; count \u0026gt; 0) { memcpy(buffer, children_ids.data(), count * sizeof(uint32_t)); } return count; // Always return the actual count found } // ... other C API implementations ... } The C API layer adheres to several principles to ensure a stable and usable FFI bridge. It strictly follows the C Application Binary Interface (ABI), using extern \u0026quot;C\u0026quot; linkage to prevent C++ name mangling and guarantee standard C calling conventions, making it consumable from Rust and other languages. To hide the internal C++ implementation details of the EnttManager, the API operates on an opaque handle, EnttManagerHandle*, which is essentially treated as a void* pointer by callers. The interface itself is carefully restricted to use only fundamental C data types like integers (e.g., uint32_t), pointers (char*, uint32_t*), and size types (size_t), avoiding any direct exposure of C++ classes or complex structures. For boolean values, a common FFI convention is adopted where C++ bool is mapped to a C int, returning 1 for true and 0 for false. Consistent representation of the null entity state across the boundary is achieved using a predefined integer constant, FFI_NULL_ENTITY (defined as UINT32_MAX), which corresponds to the internal entt::null value. Handling variable-length data, such as strings or vectors of entity IDs, requires a specific strategy to manage memory safely across the WASM boundary. This layer employs the two-stage call pattern: the caller first invokes the function with a null buffer pointer to query the required buffer size (e.g., string length including the null terminator, or the number of elements in a vector). The caller (the WASM module in this case) then allocates a buffer of sufficient size within its own linear memory. Finally, the caller invokes the C API function again, this time providing the pointer to its allocated buffer and the buffer\u0026rsquo;s capacity. The C API function then copies the requested data into the provided buffer. As a verification step and to handle potential buffer size mismatches, the C API function returns the originally required size, allowing the caller to confirm if the provided buffer was adequate. This pattern effectively avoids complex memory management issues and lifetime tracking across the FFI boundary.\nWasmHost and Defining Host Functions via Lambdas The WasmHost orchestrates Wasmtime. The critical part now is how it exposes the C API functions to the WASM module. We settled on using the Wasmtime C++ API\u0026rsquo;s linker.func_new combined with C++ lambdas in main.\n// host.cpp (main function, key parts) #include \u0026#34;wasm_host.h\u0026#34; #include \u0026#34;entt_api.h\u0026#34; // ... other includes ... using namespace wasmtime; int main(int argc, char *argv[]) { // ... setup ... WasmHost host(wasm_path); EnttManager* manager_ptr = \u0026amp;host.getEnttManager(); // Pointer needed for capture Linker\u0026amp; linker = host.getLinker(); Store\u0026amp; store = host.getStore(); host.setupWasi(); std::cout \u0026lt;\u0026lt; \u0026#34;[Host Main] Defining host functions using lambdas...\u0026#34; \u0026lt;\u0026lt; std::endl; // Define Wasmtime function types auto void_to_i32_type = FuncType({}, {ValType(ValKind::I32)}); // ... other FuncType definitions ... // --- Example Lambda Definition (create_entity) --- linker.func_new( \u0026#34;env\u0026#34;, \u0026#34;host_create_entity\u0026#34;, // Module \u0026amp; Function name WASM expects void_to_i32_type, // The Wasmtime function type // The Lambda implementing the host function [manager_ptr]( // Capture the EnttManager pointer Caller caller, // Wasmtime provided caller context Span\u0026lt;const Val\u0026gt; args, // Arguments from WASM Span\u0026lt;Val\u0026gt; results // Where to put return values for WASM ) -\u0026gt; Result\u0026lt;std::monostate, Trap\u0026gt; // Required return signature { try { // Call the stable C API function uint32_t id = entt_manager_create_entity( reinterpret_cast\u0026lt;EnttManagerHandle*\u0026gt;(manager_ptr) ); // Convert result to wasmtime::Val and store in results span results[0] = Val(static_cast\u0026lt;int32_t\u0026gt;(id)); // Indicate success return std::monostate(); } catch (const std::exception\u0026amp; e) { // Convert C++ exceptions to WASM traps std::cerr \u0026lt;\u0026lt; \u0026#34;Host function host_create_entity failed: \u0026#34; \u0026lt;\u0026lt; e.what() \u0026lt;\u0026lt; std::endl; return Trap(\u0026#34;Host function host_create_entity failed.\u0026#34;); } } ).unwrap(); // unwrap for brevity, check Result in production // --- Example Lambda Definition (add_name, needs memory access) --- linker.func_new( \u0026#34;env\u0026#34;, \u0026#34;host_add_name\u0026#34;, /* i32ptrlen_to_void_type */..., [manager_ptr](Caller caller, Span\u0026lt;const Val\u0026gt; args, Span\u0026lt;Val\u0026gt; results) -\u0026gt; Result\u0026lt;std::monostate, Trap\u0026gt; { // 1. Extract args: entity_id, name_ptr, name_len // 2. Get memory: auto mem_opt = caller.get_export(\u0026#34;memory\u0026#34;); ... check ... Memory mem = ...; Span\u0026lt;uint8_t\u0026gt; data = ...; // 3. Bounds check ptr + len against data.size() // 4. Read string: std::string name_str(data.data() + name_ptr, name_len); // 5. Call C API: entt_manager_add_name(..., name_str.c_str()); return std::monostate(); } ).unwrap(); // ... Define lambdas for ALL functions in entt_api.h similarly ... host.initialize(); // Compile WASM, instantiate with linked functions host.callFunctionVoid(\u0026#34;test_relationships\u0026#34;); // Run the tests in WASM // ... rest of main ... } The integration within the WasmHost and the main function showcases several important techniques for exposing host functionality to WASM. C++ lambdas serve as the essential bridge, adapting Wasmtime\u0026rsquo;s specific calling convention, which involves receiving a wasmtime::Caller object and spans of wasmtime::Val for arguments and results ( Span\u0026lt;const Val\u0026gt;, Span\u0026lt;Val\u0026gt;), to the simpler, C-style signature of our C API functions which expect an EnttManagerHandle* and basic C types. State is managed through lambda captures; by capturing the pointer to the EnttManager instance (manager_ptr) obtained from the WasmHost, the lambda provides the necessary context to the otherwise stateless C API functions, enabling them to operate on the correct EnttManager instance. It\u0026rsquo;s critical, however, to be mindful of object lifetimes: the captured EnttManager pointer is only valid as long as the WasmHost instance exists, meaning the host object must outlive any potential WASM execution that might invoke these captured-pointer lambdas. For operations requiring interaction with WASM\u0026rsquo;s linear memory, such as passing strings or buffers, the lambda must explicitly retrieve the exported Memory object using the provided wasmtime::Caller. Once obtained, the lambda is responsible for accessing the memory data via the returned Span\u0026lt;uint8_t\u0026gt; and performing rigorous bounds checking before reading or writing to prevent memory corruption. The lambdas also take responsibility for data type marshalling, converting incoming wasmtime::Val arguments into the appropriate C types needed by the C API functions, and converting any C API return values back into wasmtime::Val objects to be placed in the results span for WASM. Finally, robust error handling is incorporated using try-catch blocks within each lambda. This ensures that any standard C++ exceptions thrown during the execution of the C API or the lambda\u0026rsquo;s internal logic are caught and gracefully converted into wasmtime::Trap objects, which are then returned to the WASM runtime, preventing host exceptions from crashing the entire process.\nBack to Rust: Consuming the Host API Safely The Rust side focuses on interacting with the stable C API provided by the host, hiding the unsafe details.\nThe FFI Layer (ffi.rs): Managing the unsafe Boundary This module is the gatekeeper between safe Rust and the potentially unsafe C world.\n// src/ffi.rs use std::ffi::{c_char, c_int, CStr, CString}; use std::ptr; use std::slice; // Constant for null entity pub const FFI_NULL_ENTITY_ID: u32 = u32::MAX; // Host function imports (extern \u0026#34;C\u0026#34; block) ##[link(wasm_import_module = \u0026#34;env\u0026#34;)] unsafe extern \u0026#34;C\u0026#34; { // fn host_create_entity() -\u0026gt; u32; ... (all C API functions declared here) fn host_is_entity_valid(entity_id: u32) -\u0026gt; c_int; fn host_get_profile_for_player(player_id: u32) -\u0026gt; u32; fn host_get_name(entity_id: u32, buffer_ptr: *mut c_char, buffer_len: usize) -\u0026gt; usize; fn host_find_children(parent_id: u32, buffer_ptr: *mut u32, buffer_len: usize) -\u0026gt; usize; // ... } // Safe wrappers pub fn is_entity_valid(entity_id: u32) -\u0026gt; bool { if entity_id == FFI_NULL_ENTITY_ID { return false; } unsafe { host_is_entity_valid(entity_id) != 0 } // Convert c_int to bool } pub fn get_profile_for_player(player_id: u32) -\u0026gt; Option\u0026lt;u32\u0026gt; { let profile_id = unsafe { host_get_profile_for_player(player_id) }; // Convert sentinel value to Option if profile_id == FFI_NULL_ENTITY_ID { None } else { Some(profile_id) } } // Wrapper using two-stage call for strings pub fn get_name(entity_id: u32) -\u0026gt; Option\u0026lt;String\u0026gt; { unsafe { let required_len = host_get_name(entity_id, ptr::null_mut(), 0); // Call 1: Get size if required_len == 0 { return None; } let mut buffer: Vec\u0026lt;u8\u0026gt; = vec![0; required_len]; // Allocate in Rust/WASM let written_len = host_get_name(entity_id, buffer.as_mut_ptr() as *mut c_char, buffer.len()); // Call 2: Fill buffer if written_len == required_len { // Verify host wrote expected amount // Safely convert buffer to String (handles null terminator) CStr::from_bytes_with_nul(\u0026amp;buffer[..written_len]).ok()? // Check for interior nulls .to_str().ok()?.to_owned().into() // Convert CStr -\u0026gt; \u0026amp;str -\u0026gt; String -\u0026gt; Option\u0026lt;String\u0026gt; } else { None } // Error case } } // Wrapper using two-stage call for Vec\u0026lt;u32\u0026gt; pub fn find_children(parent_id: u32) -\u0026gt; Vec\u0026lt;u32\u0026gt; { unsafe { let count = host_find_children(parent_id, ptr::null_mut(), 0); // Call 1 if count == 0 { return Vec::new(); } let mut buffer: Vec\u0026lt;u32\u0026gt; = vec![0; count]; // Allocate let written_count = host_find_children(parent_id, buffer.as_mut_ptr(), buffer.len()); // Call 2 if written_count == count { buffer } else { Vec::new() } // Verify and return } } // ... other safe wrappers ... The design of the Rust FFI layer (ffi.rs) prioritizes safety and ergonomics for the rest of the Rust codebase. A key principle is the isolation of unsafe code; all direct calls to the imported extern \u0026quot;C\u0026quot; host functions are strictly contained within unsafe {} blocks inside this specific module. This creates a clear boundary, allowing the core application logic in other modules to remain entirely within safe Rust. The wrappers actively promote type safety by translating between the C types used in the FFI signatures (like c_int) and idiomatic Rust types such as bool or, for potentially null values, Option\u0026lt;u32\u0026gt;. For instance, the C API\u0026rsquo;s integer constant FFI_NULL_ENTITY is consistently mapped to Rust\u0026rsquo;s None variant, providing a more expressive and safer way to handle potentially absent entity references. Memory management for data exchanged via the buffer pattern (used for strings and vectors) is handled entirely on the Rust/WASM side. The wrapper functions implement the two-stage call convention: they first call the host API to determine the required buffer size, then allocate the necessary memory (e.g., a Vec\u0026lt;u8\u0026gt; for strings or Vec\u0026lt;u32\u0026gt; for entity IDs) within WASM\u0026rsquo;s own linear memory space. This allocated buffer\u0026rsquo;s pointer and capacity are then passed to the second host API call, which fills the buffer. The Rust wrapper subsequently processes the data safely, for example, by using CStr::from_bytes_with_nul to correctly interpret potentially null-terminated strings received from the host. This approach confines memory allocation and interpretation to the Rust side, avoiding cross-boundary memory management complexities. Finally, basic error handling is integrated into the wrappers; C API conventions indicating failure (like returning a size of 0 when data was expected) are translated into appropriate Rust return types, typically Option or an empty Vec, signaling the absence of data or an unsuccessful operation to the calling Rust code.\nThe Core Logic (lib.rs::core): Safe Interaction With the FFI details abstracted away, the core Rust logic becomes clean and safe.\n// src/lib.rs::core use crate::ffi::{ /* Import the necessary safe wrappers */ }; pub fn run_entt_relationship_tests() { println!(\u0026#34;[WASM Core] === Starting EnTT Relationship Tests ===\u0026#34;); // --- Test 1:1 --- let player1 = create_entity(); // Calls safe ffi::create_entity() let profile1 = create_entity(); add_name(player1, \u0026#34;Alice_WASM\u0026#34;); // Calls safe ffi::add_name() assert!(link_player_profile(player1, profile1)); // Calls safe ffi::link_player_profile() let found_profile_opt = get_profile_for_player(player1); // Calls safe wrapper assert_eq!(found_profile_opt, Some(profile1)); // ... rest of the tests using safe wrappers ... println!(\u0026#34;[WASM Core] === EnTT Relationship Tests Completed ===\u0026#34;); } The core logic operates purely in terms of Rust types and safe function calls, interacting with the host\u0026rsquo;s EnTT world indirectly but effectively.\nExecution \u0026amp; Verification: Seeing it All Work Running the C++ host executable produces interleaved output from both the host and the WASM module, confirming the interactions:\n// [Host Setup] ... initialization ... // [Host Main] Defining host functions using lambdas... // [Host Setup] Initializing WasmHost... // ... compilation, instantiation ... [Host Setup] WasmHost initialization complete. --- Test: Running WASM Relationship Tests --- \u0026lt;-- Host calls WASM export [WASM Export] Running relationship tests... [WASM Core] === Starting EnTT Relationship Tests === [WASM Core] --- Testing 1:1 Relationships --- [EnttManager] Created entity: 0 \u0026lt;-- WASM calls host_create_entity -\u0026gt; C API -\u0026gt; Manager [EnttManager] Created entity: 1 // ... other calls ... [WASM Core] Unlinking Player 0 [EnttManager] Unlinking 1:1 for entity 0 \u0026lt;-- WASM calls host_unlink -\u0026gt; C API -\u0026gt; Manager [WASM Core] Destroying Player 0 and Profile 1 [EnttManager] Destroying entity: 0 \u0026lt;-- WASM calls host_destroy -\u0026gt; C API -\u0026gt; Manager [EnttManager::Cleanup] Cleaning ... FOR entity 0... \u0026lt;-- Host EnTT signal triggers cleanup *before* removal [EnttManager::Cleanup] Finished cleaning for entity 0. // ... more cleanup and tests ... [WASM Export] Relationship tests finished. [Host Main] WASM tests finished. [EnttManager] Shutting down. \u0026lt;-- Host application ends The logs clearly demonstrate the back-and-forth calls and, crucially, the execution of the EnttManager::Cleanup logic triggered by registry_.destroy(), ensuring relationship integrity is maintained automatically.\nKey Takeaways and Reflections This journey integrating EnTT and WebAssembly underscores several crucial architectural principles. Foremost among them is the need to consciously embrace the boundary between the C++ host and the WASM module. Instead of attempting to force complex C++ concepts like object orientation across this divide, the successful approach involves designing a well-defined, stable interface using the C ABI. This FFI layer should rely on simple, fundamental data types and establish clear communication protocols, such as the two-stage buffer pattern employed here for handling variable-length data like strings and vectors.\nEnTT\u0026rsquo;s inherent strengths proved particularly advantageous in overcoming the limitations faced by traditional OOP at the WASM boundary. Its data-driven philosophy, centered around portable entity identifiers (transmissible as simple integers) and data-only components, provides a natural and effective model for interaction. Entity IDs serve as reliable handles across the FFI, while component structures act as straightforward data contracts manageable within WASM\u0026rsquo;s linear memory.\nThe structural separation into distinct layers was also key to the project\u0026rsquo;s success and maintainability. Isolating the core C++ EnTT logic within the EnttManager, providing a clean C API facade, creating safe Rust FFI wrappers in ffi.rs, and implementing the main plugin logic in safe Rust within lib.rs::core results in a system that is easier to understand, test, and modify safely. Furthermore, automating essential maintenance tasks, like relationship cleanup, significantly enhances robustness. Leveraging EnTT\u0026rsquo;s signal system, specifically the on_destroy signal, allowed for the automatic removal of dangling references when entities were destroyed, drastically reducing the potential for runtime errors and simplifying the logic compared to manual tracking across the FFI.\nFinally, this integration highlights the importance of using the provided libraries idiomatically. For Wasmtime\u0026rsquo;s C++ API (wasmtime.hh), this meant utilizing the intended mechanisms like linker.func_new with C++ lambdas for defining host functions, rather than attempting to force the use of raw C function pointers with API overloads not designed for them. Adhering to the intended usage patterns of the tools generally leads to cleaner, more correct, and often more performant solutions.\nConclusion and Future Directions We\u0026rsquo;ve successfully built a system where a Rust WASM plugin can interact with and manage complex entity relationships stored within an EnTT registry managed by a C++ host. This demonstrates that even sophisticated data structures and logic can be effectively bridged across the WASM boundary by leaning into data-oriented design principles and carefully crafting the FFI layer.\nThis opens up exciting possibilities: building extensible game engines where gameplay logic resides in safe WASM plugins, creating simulation platforms with user-provided WASM modules, or offloading specific computations to sandboxed WASM components within larger C++ applications.\nWhile our example covers the fundamentals, there are several avenues for further exploration and refinement. Enhancing the robustness of error handling across the FFI, perhaps with more structured error codes or reporting mechanisms beyond simple boolean returns or traps, would be beneficial for production systems. Investigating alternative data serialization methods, such as Protocol Buffers or FlatBuffers, could offer more standardized or potentially more efficient ways to structure and transfer complex data structures through WASM\u0026rsquo;s linear memory compared to direct struct mapping. Furthermore, delving into advanced Wasmtime features like fuel metering for computation limiting or epoch-based interruption for cooperative multitasking could provide greater control over plugin resource consumption and responsiveness. Finally, staying informed about evolving WebAssembly standards, especially upcoming proposals like Interface Types, will be important, as these aim to substantially simplify the complexities of cross-language data exchange and function calls in the future.\nThe core takeaway remains: when object-oriented bridges struggle to cross the WASM chasm, EnTT\u0026rsquo;s data-driven philosophy paves a solid and efficient path forward. Happy coding in your bridged worlds!\n","permalink":"https://blog.tategotoazarasi.me/en/posts/bridging-the-gap-flexible-relationship-management-between-cpp-host-and-rust-wasm-plugins-using-entt/","summary":"Manage EnTT entity relationships in a C++ host from Rust WebAssembly (WASM) plugins using Wasmtime, a stable C FFI, and a data-driven approach to overcome WASM boundary limitations.","title":"Bridging the Gap: Flexible Relationship Management Between C++ Host and Rust WASM Plugins using EnTT"},{"content":"Today, let\u0026rsquo;s talk about an increasingly popular technology: WebAssembly (Wasm). However, we won\u0026rsquo;t confine it to the browser. Instead, we\u0026rsquo;ll explore how, on the server-side or in desktop applications, we can use the Wasmtime runtime to allow C++ programs to load and execute Rust-compiled Wasm modules. We\u0026rsquo;ll also delve into enabling complex interactions between them, such as bidirectional function calls, shared memory, passing structs, and even modifying each other\u0026rsquo;s state.\nA Brief Introduction to WebAssembly and Wasmtime First, let\u0026rsquo;s briefly explain what WebAssembly is. You can think of it as a portable binary instruction format designed for the modern web. It\u0026rsquo;s not meant to replace JavaScript but rather to act as a powerful complement, allowing code written in performance-sensitive or low-level languages like C, C++, or Rust to run in web environments (and other Wasm-supporting environments) at near-native speeds. Wasm\u0026rsquo;s core strengths lie in its sandboxed security model and * platform-agnostic* nature.\nWasmtime, on the other hand, is a standalone, efficient, and secure WebAssembly runtime developed by the Bytecode Alliance (a consortium including companies like Mozilla, Fastly, Intel, and Red Hat). It enables you to run Wasm modules outside the browser – for instance, on servers, in command-line tools, or on embedded devices. Wasmtime provides APIs for various languages, including C, C++, Python, Rust, and Go, making it convenient to integrate Wasm into existing applications.\nWhy Choose a C++ Host + Rust Wasm Combination? This combination offers several compelling advantages:\nMany mature projects have extensive C++ foundations. Wasm allows parts of these projects to be modularized, sandboxed, or exposed as a plugin system without rewriting the core logic. Rust is renowned for its memory and concurrency safety, making it an excellent choice for writing highly reliable Wasm modules. Rust adds another layer of assurance on top of Wasm\u0026rsquo;s sandbox. Both C++ and Rust are high-performance languages. When compiled to Wasm and executed with a JIT runtime like Wasmtime, they can achieve performance close to native code. Interaction between the Wasm module and the host must occur through explicitly defined interfaces (imports/exports), which helps maintain a clean architecture.\nThe goal of this article is to demonstrate, through a concrete example, how to use Wasmtime\u0026rsquo;s C++ API to build a C++ host application that loads a Rust-written Wasm module and facilitates various interesting interactions between them.\nCore Concepts: Bridging C++ and Wasm Before diving into the code, we need to understand a few key concepts:\nHost and Guest In this scenario, the C++ application is the host. It is responsible for loading, managing, and running the Wasm module. The Rust-compiled Wasm module is the guest. It runs within the Wasmtime runtime environment provided by the host, constrained by the sandbox.\nWasm Imports and Exports The primary way Wasm modules communicate with the outside world is through imports and exports.\nA Wasm module can export functions, memory, global variables, or tables, making them available for the host or other Wasm modules to call or access. In Rust, we typically use #[no_mangle] pub extern \u0026quot;C\u0026quot; to mark functions intended for export.\nA Wasm module can declare which functionalities (usually functions) it needs to import from the host environment. When the host instantiates the Wasm module, it must provide implementations for these imports. In Rust, we use an extern \u0026quot;C\u0026quot; { ... } block combined with #[link(wasm_import_module = \u0026quot;...\u0026quot;)] to declare imports.\nThis import/export mechanism forms the interface contract between the host and the Wasm module.\nLinear Memory Each Wasm instance (usually) has its own linear memory. This is a contiguous, mutable array of bytes that can be read and written by both the Wasm code and the host code. Pointers within Wasm code are essentially offsets ( typically 32-bit or 64-bit integers) into this memory region.\nCrucially, Wasm itself is sandboxed; it cannot directly access the host\u0026rsquo;s memory. Likewise, the host cannot arbitrarily access variables internal to the Wasm instance. However, the host can obtain access to the Wasm instance\u0026rsquo;s exported linear memory via Wasmtime APIs (often as a pointer or Span to the memory\u0026rsquo;s start). Once access is granted, the host can directly read from or write to this memory block. Similarly, Wasm code can indirectly interact with the host\u0026rsquo;s state or resources by calling host-provided functions (imported functions).\nThis method of data exchange via shared linear memory is central to Wasm interaction. Passing complex data structures ( like C++ structs or Rust structs) is typically achieved by serializing them into this memory and then passing pointers (offsets) to that location.\nWASI (WebAssembly System Interface) WASI is a set of standardized system interfaces designed to allow Wasm modules to interact with the underlying operating system in a secure and portable manner, covering functionalities like file system access, network communication, and standard I/O. While our example doesn\u0026rsquo;t involve complex file operations, Rust\u0026rsquo;s standard println! macro relies on underlying standard output capabilities. To make println! within the Wasm module work correctly (printing output to the host\u0026rsquo;s console), we need to configure and link WASI support in the host.\nBuilding the C++ Host: Setting the Stage with Wasmtime Now, let\u0026rsquo;s examine what the C++ host side needs to do. For better code organization, we often create a class (e.g., WasmHost) to encapsulate the interaction logic with Wasmtime.\nLoading and Compiling the Wasm Module The first step is to read the contents of the Wasm module file (the .wasm binary) and then use Wasmtime\u0026rsquo;s Engine to compile it. The Engine acts as Wasmtime\u0026rsquo;s core compilation and execution engine, responsible for transforming Wasm bytecode into executable machine code. The compilation result is a Module object. This Module object is thread-safe and can be reused by multiple Stores.\n// Pseudo-code example (Actual code in wasm_host.cpp) #include \u0026#34;wasmtime.hh\u0026#34; // Include Wasmtime C++ header #include \u0026lt;vector\u0026gt; #include \u0026lt;fstream\u0026gt; #include \u0026lt;stdexcept\u0026gt; using namespace wasmtime; // ... WasmHost class definition ... std::vector\u0026lt;uint8_t\u0026gt; WasmHost::readWasmFile() { std::ifstream file(wasm_path_, std::ios::binary | std::ios::ate); // ... Error handling ... std::streamsize size = file.tellg(); file.seekg(0, std::ios::beg); std::vector\u0026lt;uint8_t\u0026gt; buffer(static_cast\u0026lt;size_t\u0026gt;(size)); // ... Read file contents into buffer ... return buffer; } void WasmHost::loadAndCompile() { std::vector\u0026lt;uint8_t\u0026gt; wasm_bytes = readWasmFile(); std::cout \u0026lt;\u0026lt; \u0026#34;[Host Setup] Compiling WASM module...\u0026#34; \u0026lt;\u0026lt; std::endl; // engine_ is a member variable of WasmHost, type wasmtime::Engine Result\u0026lt;Module\u0026gt; module_res = Module::compile(engine_, wasm_bytes); if (!module_res) { throw std::runtime_error(\u0026#34;Module compilation failed: \u0026#34; + module_res.err().message()); } // module_ is also a WasmHost member, type std::optional\u0026lt;wasmtime::Module\u0026gt; module_ = std::move(module_res.ok()); std::cout \u0026lt;\u0026lt; \u0026#34;[Host Setup] Module compiled successfully.\u0026#34; \u0026lt;\u0026lt; std::endl; } // Call loadAndCompile() in the WasmHost constructor or an initialization function Engine and Store The Engine handles code compilation, while the Store represents the \u0026ldquo;world\u0026rdquo; or \u0026ldquo;context\u0026rdquo; of a Wasm instance. All data associated with a Wasm instance, such as its memory, global variables, tables, and the instance itself, belongs to a specific Store. One Engine can be associated with multiple Stores, but a Store is linked to only one Engine. Stores are not thread-safe; typically, one thread corresponds to one Store.\n// WasmHost class members Engine engine_; Store store_; // WasmHost constructor WasmHost::WasmHost(std::string wasm_path) : wasm_path_(std::move(wasm_path)), engine_(), // Create default Engine store_(engine_) // Create Store based on Engine { // ... } Configuring WASI As mentioned, if the Wasm module requires system interactions (like println!), we need to configure WASI for the Store. This is usually done before instantiating the module. Wasmtime provides the WasiConfig class to configure WASI behavior, such as inheriting the host\u0026rsquo;s standard input/output/error streams, environment variables, and command-line arguments. The configured WasiConfig must be set into the Store\u0026rsquo;s context.\n// WasmHost::setupWasi() method void WasmHost::setupWasi() { // ... Check if already initialized or configured ... std::cout \u0026lt;\u0026lt; \u0026#34;[Host Setup] Configuring WASI...\u0026#34; \u0026lt;\u0026lt; std::endl; WasiConfig wasi; wasi.inherit_stdout(); // Make Wasm\u0026#39;s stdout go to host\u0026#39;s stdout wasi.inherit_stderr(); // Same for stderr // store_ is a WasmHost member variable auto wasi_set_res = store_.context().set_wasi(std::move(wasi)); if (!wasi_set_res) { throw std::runtime_error(\u0026#34;Failed setting WASI config in store: \u0026#34; + wasi_set_res.err().message()); } wasi_configured_ = true; std::cout \u0026lt;\u0026lt; \u0026#34;[Host Setup] WASI configured for Store.\u0026#34; \u0026lt;\u0026lt; std::endl; // Also need to define WASI imports in the Linker linkWasiImports(); } // WasmHost::linkWasiImports() method void WasmHost::linkWasiImports() { // ... Check if WASI is configured ... std::cout \u0026lt;\u0026lt; \u0026#34;[Host Setup] Defining WASI imports in linker...\u0026#34; \u0026lt;\u0026lt; std::endl; // linker_ is a WasmHost member variable, type wasmtime::Linker auto linker_define_wasi_res = linker_.define_wasi(); if (!linker_define_wasi_res) { throw std::runtime_error(\u0026#34;Failed defining WASI imports in linker: \u0026#34; + linker_define_wasi_res.err().message()); } std::cout \u0026lt;\u0026lt; \u0026#34;[Host Setup] WASI imports defined.\u0026#34; \u0026lt;\u0026lt; std::endl; } Linker: The Bridge Connecting Host and Wasm The Linker is a Wasmtime utility for resolving module imports and connecting them to host-provided implementations. Before instantiating a module, we need to inform the Linker how to satisfy all of the Wasm module\u0026rsquo;s import requirements.\nThis involves two main parts:\nLinking WASI Imports: If we\u0026rsquo;ve configured WASI, we need to call linker_.define_wasi(). This automatically adds implementations for standard WASI functions to the Linker. Linking Custom Host Function Imports: The Wasm module might need to call our custom host functions. We must wrap these C++ functions (or lambdas) into a form Wasmtime understands and register them with the Linker using linker_.define() or linker_.func_wrap(). We specify the corresponding Wasm module name (defined by #[link(wasm_import_module = \u0026quot;...\u0026quot;)] in the Rust code) and the function name. Defining Host Functions Callable by Wasm This is crucial for enabling Wasm-to-Host calls. We need to write the implementation functions in C++. Their signatures must match the extern \u0026quot;C\u0026quot; function declarations in Rust (or be adaptable by Wasmtime C++ API template deduction).\nFor example, if Rust declares imports like this:\n// src/ffi.rs #[link(wasm_import_module = \u0026#34;env\u0026#34;)] // Module name is \u0026#34;env\u0026#34; unsafe extern \u0026#34;C\u0026#34; { fn host_log_value(value: i32); fn host_get_shared_value() -\u0026gt; i32; fn host_set_shared_value(value: i32); } Then, in the C++ host, we provide implementations for these three functions and register them with the Linker, associated with the \u0026ldquo;env\u0026rdquo; module.\n// host.cpp #include \u0026lt;iostream\u0026gt; #include \u0026lt;cstdint\u0026gt; // Host state int32_t shared_host_value = 42; // C++ implementation functions void host_log_value_impl_target(int32_t value) { std::cout \u0026lt;\u0026lt; \u0026#34;[Host Target] host_log_value called by WASM with value: \u0026#34; \u0026lt;\u0026lt; value \u0026lt;\u0026lt; std::endl; } int32_t host_get_shared_value_impl_target() { std::cout \u0026lt;\u0026lt; \u0026#34;[Host Target] host_get_shared_value called by WASM. Returning: \u0026#34; \u0026lt;\u0026lt; shared_host_value \u0026lt;\u0026lt; std::endl; return shared_host_value; } void host_set_shared_value_impl_target(int32_t new_value) { std::cout \u0026lt;\u0026lt; \u0026#34;[Host Target] host_set_shared_value called by WASM. Old host value: \u0026#34; \u0026lt;\u0026lt; shared_host_value \u0026lt;\u0026lt; \u0026#34;, New host value: \u0026#34; \u0026lt;\u0026lt; new_value \u0026lt;\u0026lt; std::endl; shared_host_value = new_value; // Modify host state } // In the WasmHost class or main function, register these using the Linker // (Simplified wrapper function within WasmHost class) template \u0026lt;typename FuncPtr\u0026gt; void WasmHost::defineHostFunction(std::string_view module_name, std::string_view func_name, FuncPtr func_ptr) { if (is_initialized_) { throw std::logic_error(\u0026#34;Cannot define host functions after initialization.\u0026#34;); } std::cout \u0026lt;\u0026lt; \u0026#34;[Host Setup] Defining host function: \u0026#34; \u0026lt;\u0026lt; module_name \u0026lt;\u0026lt; \u0026#34;::\u0026#34; \u0026lt;\u0026lt; func_name \u0026lt;\u0026lt; \u0026#34;...\u0026#34; \u0026lt;\u0026lt; std::endl; // linker_ is a WasmHost member variable auto result = linker_.func_wrap(module_name, func_name, func_ptr); if (!result) { throw std::runtime_error(\u0026#34;Failed to define host function \u0026#39;\u0026#34; + std::string(func_name) + \u0026#34;\u0026#39;: \u0026#34; + result.err().message()); } } // Called from main function host.defineHostFunction(\u0026#34;env\u0026#34;, \u0026#34;host_log_value\u0026#34;, host_log_value_impl_target); host.defineHostFunction(\u0026#34;env\u0026#34;, \u0026#34;host_get_shared_value\u0026#34;, host_get_shared_value_impl_target); host.defineHostFunction(\u0026#34;env\u0026#34;, \u0026#34;host_set_shared_value\u0026#34;, host_set_shared_value_impl_target); linker_.func_wrap() is a convenient template function. It automatically deduces the parameter and return types of the C++ function, converts them to the corresponding Wasm function type, and registers the function. This is often simpler than manually creating a FuncType and using linker_.define().\nInstantiating the Module Once all imports (WASI and custom functions) are defined in the Linker, we can use linker_.instantiate() to create an instance (Instance) of the Wasm module. The instantiation process connects the Wasm code with the host-provided implementations and allocates resources like memory and globals within the Store.\n// WasmHost::instantiateModule() method void WasmHost::instantiateModule() { // ... Check if module_ is valid ... std::cout \u0026lt;\u0026lt; \u0026#34;[Host Setup] Instantiating module...\u0026#34; \u0026lt;\u0026lt; std::endl; // store_ is a WasmHost member variable TrapResult\u0026lt;Instance\u0026gt; instance_res = linker_.instantiate(store_.context(), module_.value()); if (!instance_res) { // Handle instantiation error (could be linking error or Wasm start trap) throw std::runtime_error(\u0026#34;Module instantiation failed: \u0026#34; + instance_res.err().message()); } // instance_ is a WasmHost member, type std::optional\u0026lt;wasmtime::Instance\u0026gt; instance_ = std::move(instance_res.ok()); std::cout \u0026lt;\u0026lt; \u0026#34;[Host Setup] Module instantiated successfully.\u0026#34; \u0026lt;\u0026lt; std::endl; } Accessing Wasm Linear Memory To exchange complex data with the Wasm module or directly read/write its memory state, the host needs access to the Wasm instance\u0026rsquo;s linear memory. Wasm modules typically export a memory object named \u0026ldquo;memory\u0026rdquo;. We can retrieve it using instance_.get().\n// WasmHost::getMemory() method void WasmHost::getMemory() { // ... Check if instance_ is valid ... std::cout \u0026lt;\u0026lt; \u0026#34;[Host Setup] Getting exported memory \u0026#39;memory\u0026#39;...\u0026#34; \u0026lt;\u0026lt; std::endl; // store_ is a WasmHost member variable auto memory_export_opt = instance_.value().get(store_.context(), \u0026#34;memory\u0026#34;); if (memory_export_opt \u0026amp;\u0026amp; std::holds_alternative\u0026lt;Memory\u0026gt;(*memory_export_opt)) { // memory_ is a WasmHost member, type std::optional\u0026lt;wasmtime::Memory\u0026gt; memory_ = std::get\u0026lt;Memory\u0026gt;(*memory_export_opt); std::cout \u0026lt;\u0026lt; \u0026#34;[Host Setup] Found exported memory. Size: \u0026#34; \u0026lt;\u0026lt; memory_.value().data(store_.context()).size() \u0026lt;\u0026lt; \u0026#34; bytes.\u0026#34; \u0026lt;\u0026lt; std::endl; } else { std::cout \u0026lt;\u0026lt; \u0026#34;[Host Setup] Export \u0026#39;memory\u0026#39; not found or not a memory. Proceeding without memory access.\u0026#34; \u0026lt;\u0026lt; std::endl; } } // Get a Span\u0026lt;uint8_t\u0026gt; for the memory, providing a view into the memory region Span\u0026lt;uint8_t\u0026gt; WasmHost::getMemorySpan() { if (!is_initialized_ || !memory_.has_value()) { throw std::logic_error(\u0026#34;Memory not available or host not initialized.\u0026#34;); } return memory_.value().data(store_.context()); } The obtained wasmtime::Memory object has a data() method that returns a wasmtime::Span\u0026lt;uint8_t\u0026gt; (or std::span\u0026lt;uint8_t\u0026gt; if C++20 is available). This Span provides direct, low-level access (a pointer and size) to the Wasm linear memory region. With this Span, the host can directly read from and write to the Wasm\u0026rsquo;s memory.\nBuilding the Wasm Module: Rust\u0026rsquo;s Safe Territory Now let\u0026rsquo;s switch to the Rust side and see how the Wasm module is constructed.\nProject Structure Typically, FFI (Foreign Function Interface) related code is placed in a separate module (e.g., src/ffi.rs), while the core, safe Rust logic resides in another module (e.g., src/core.rs or directly within src/lib.rs).\nsrc/lib.rs serves as the library\u0026rsquo;s entry point. It declares and exports the interfaces from the ffi module needed by the host and might contain or invoke logic from the core module.\n// src/lib.rs mod ffi; // Declare the ffi module pub(crate) mod core; // Declare the internal core module // Re-export functions and types from the FFI layer needed by the host pub use ffi::{ Point, get_plugin_shared_value_ptr, just_add, point_add, simple_add, trigger_host_calls, }; FFI Layer (src/ffi.rs) This is the boundary where Rust interacts with the external world (the C++ host).\nDeclare Host Function Imports: Use extern \u0026quot;C\u0026quot; blocks and #[link(wasm_import_module = \u0026quot;env\u0026quot;)] to inform the Rust compiler and Wasm runtime about external functions provided by a module named \u0026ldquo;env\u0026rdquo;. The signatures must match the implementations provided by the C++ host. Note that extern \u0026quot;C\u0026quot; blocks are inherently unsafe because calling external functions cannot guarantee Rust\u0026rsquo;s memory safety rules.\n// src/ffi.rs #[link(wasm_import_module = \u0026#34;env\u0026#34;)] unsafe extern \u0026#34;C\u0026#34; { fn host_log_value(value: i32); fn host_get_shared_value() -\u0026gt; i32; fn host_set_shared_value(value: i32); } Provide Safe Wrappers: To avoid scattering unsafe blocks throughout the business logic, it\u0026rsquo;s common practice to provide safe Rust wrapper functions for the imported unsafe functions.\n// src/ffi.rs pub fn log_value_from_host(value: i32) { unsafe { host_log_value(value) } // The unsafe call is encapsulated inside } // ... other wrapper functions ... Export Wasm Functions: Use #[no_mangle] to prevent the Rust compiler from mangling function names, and use pub extern \u0026quot;C\u0026quot; to specify the C calling convention. This allows the C++ host to find and call these functions by name.\n// src/ffi.rs #[no_mangle] // Prevent name mangling pub extern \u0026#34;C\u0026#34; fn just_add(left: u64, right: u64) -\u0026gt; u64 { println!(\u0026#34;[WASM FFI] just_add called...\u0026#34;); // Using WASI\u0026#39;s println! core::perform_basic_add(left, right) // Call core logic } #[no_mangle] pub extern \u0026#34;C\u0026#34; fn trigger_host_calls(input_val: i32) { println!(\u0026#34;[WASM FFI] trigger_host_calls called...\u0026#34;); core::perform_host_calls_test(input_val); // Call core logic } // ... other exported functions ... Core Logic Layer (src/core.rs) This is where the actual functionality of the Wasm module is implemented, ideally using safe Rust code. It calls the safe wrappers provided by the FFI layer to interact with the host.\n// src/lib.rs (core module) pub(crate) mod core { use crate::ffi::{ // Import safe wrappers from the FFI layer Point, get_shared_value_from_host, log_value_from_host, set_shared_value_in_host, // ... }; pub fn perform_basic_add(left: u64, right: u64) -\u0026gt; u64 { println!(\u0026#34;[WASM Core] perform_basic_add: {} + {}\u0026#34;, left, right); left.wrapping_add(right) // Safe addition } pub fn perform_host_calls_test(input_val: i32) { println!(\u0026#34;[WASM Core] perform_host_calls_test with input: {}\u0026#34;, input_val); // Call host functions (via safe wrappers) log_value_from_host(input_val * 2); let host_val = get_shared_value_from_host(); set_shared_value_in_host(host_val + input_val + 5); // ... } // ... other core logic functions ... } Defining Shared Data Structures If complex data structures need to be passed between C++ and Rust, both sides must agree on the memory layout. In Rust, use the #[repr(C)] attribute to enforce a C-compatible memory layout for the struct. In C++, while compilers often lay out structs sequentially, using #pragma pack(push, 1) and #pragma pack(pop) ensures a packed (no padding) layout for absolute certainty, or ensures consistent alignment between both sides.\n// src/ffi.rs #[repr(C)] // Crucial: guarantees C-compatible layout #[derive(Debug, Copy, Clone, Default)] pub struct Point { pub x: i32, pub y: i32, } // host.cpp #pragma pack(push, 1) // Recommended: ensures packed layout consistent with Rust struct Point { int32_t x; int32_t y; }; #pragma pack(pop) Managing Wasm Internal State Wasm modules sometimes need to maintain their own state. One way is using Rust\u0026rsquo;s static mut variables. However, accessing static mut requires an unsafe block because it can potentially introduce data races (though the risk is lower in single-threaded Wasm environments, Rust still mandates unsafe).\n// src/ffi.rs static mut PLUGIN_SHARED_VALUE: i32 = 100; // Wasm module\u0026#39;s internal state // Internal FFI helper function for safe reading (still needs unsafe block) pub(crate) fn read_plugin_value_internal() -\u0026gt; i32 { unsafe { PLUGIN_SHARED_VALUE } } // Used in the core module // use crate::ffi::read_plugin_value_internal; // let val = read_plugin_value_internal(); If the host needs to modify this state directly, an exported function can return a pointer (memory offset) to the static mut variable.\n// src/ffi.rs #[no_mangle] pub unsafe extern \u0026#34;C\u0026#34; fn get_plugin_shared_value_ptr() -\u0026gt; *mut i32 { // Note: Requires `unsafe fn` and an inner `unsafe` block // Use `\u0026amp;raw mut` (newer Rust syntax) or direct cast to get the raw pointer // let ptr = unsafe { \u0026amp;mut PLUGIN_SHARED_VALUE as *mut i32 }; let ptr = { \u0026amp;raw mut PLUGIN_SHARED_VALUE as *mut i32 }; // Using \u0026amp;raw mut avoids Miri complaints println!(\u0026#34;[WASM FFI] get_plugin_shared_value_ptr() -\u0026gt; {:?}\u0026#34;, ptr); ptr } Warning: Exposing a pointer to internal mutable state directly to the host is a very dangerous practice! It breaks Wasm\u0026rsquo;s encapsulation, allowing the host to modify internal Wasm data directly, potentially leading to unexpected consequences or violating internal invariants. This pattern should be strongly avoided in practice unless there\u0026rsquo;s a very specific and controlled reason. A better approach is to modify internal state indirectly and safely via exported functions. It\u0026rsquo;s shown here primarily to demonstrate the possibilities of memory manipulation.\nDetailed Interaction Patterns Now let\u0026rsquo;s combine the C++ host and Rust Wasm module code to see how specific interaction flows are implemented.\nPattern One: Host Calls a Simple Wasm Function (just_add) This is the most basic interaction. The host needs to call a pure computation function exported by the Wasm module.\nC++ Host Side (host.cpp):\nGet Function: Obtain a type-safe Wasm function proxy (TypedFunc) using a method encapsulated in WasmHost ( which internally calls instance_.get() and func.typed()). Prepare Arguments: Wrap the C++ uint64_t arguments in an std::tuple. Call: Invoke the Wasm function using the typed_func.call() method. The Wasmtime C++ API handles argument and return value marshalling. Process Result: Extract the std::tuple containing the uint64_t return value from the returned Result. // host.cpp (inside main, Test 1) uint64_t arg1 = 15, arg2 = 27; auto args = std::make_tuple(arg1, arg2); std::cout \u0026lt;\u0026lt; \u0026#34;[Host Main] Calling Wasm function \u0026#39;just_add(\u0026#34; \u0026lt;\u0026lt; arg1 \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; arg2 \u0026lt;\u0026lt; \u0026#34;)\u0026#39;...\u0026#34; \u0026lt;\u0026lt; std::endl; // host is the WasmHost instance // Type deduction: Return is tuple\u0026lt;u64\u0026gt;, Params are tuple\u0026lt;u64, u64\u0026gt; auto result_tuple = host.callFunction\u0026lt;std::tuple\u0026lt;uint64_t\u0026gt;, std::tuple\u0026lt;uint64_t, uint64_t\u0026gt;\u0026gt;( \u0026#34;just_add\u0026#34;, args); // result_tuple is Result\u0026lt;std::tuple\u0026lt;uint64_t\u0026gt;, TrapError\u0026gt; if (!result_tuple) { /* Error handling */ } uint64_t result_val = std::get\u0026lt;0\u0026gt;(result_tuple.ok()); std::cout \u0026lt;\u0026lt; \u0026#34;[Host Main] \u0026#39;just_add\u0026#39; Result: \u0026#34; \u0026lt;\u0026lt; result_val \u0026lt;\u0026lt; std::endl; Here, host.callFunction is a wrapper within the WasmHost class that hides the details of getting the function, type-checking, and calling.\nRust Wasm Side (src/ffi.rs and src/lib.rs::core):\nThe #[no_mangle] pub extern \u0026quot;C\u0026quot; fn just_add function is exported. It receives two u64 parameters and calls core::perform_basic_add for the computation. It returns the u64 result. // src/ffi.rs #[no_mangle] pub extern \u0026#34;C\u0026#34; fn just_add(left: u64, right: u64) -\u0026gt; u64 { println!(\u0026#34;[WASM FFI] just_add called with: {} + {}\u0026#34;, left, right); let result = crate::core::perform_basic_add(left, right); // Call core logic println!(\u0026#34;[WASM FFI] just_add result: {}\u0026#34;, result); result } // src/lib.rs::core pub fn perform_basic_add(left: u64, right: u64) -\u0026gt; u64 { println!(\u0026#34;[WASM Core] perform_basic_add: {} + {}\u0026#34;, left, right); left.wrapping_add(right) // Use safe addition } This flow demonstrates the basic function call from C++ to Rust and the passing of simple data types.\nPattern Two: Wasm Calls Host Functions (trigger_host_calls) This pattern reverses the direction: the Wasm module needs to invoke functionality provided by the host.\nC++ Host Side:\nImplement Host Functions: Such as host_log_value_impl_target, host_get_shared_value_impl_target, host_set_shared_value_impl_target. These functions can directly access and modify the host\u0026rsquo;s state (like shared_host_value). Register with Linker: Use host.defineHostFunction(\u0026quot;env\u0026quot;, ...) to associate these C++ functions with the function names the Wasm module expects to import from the \u0026ldquo;env\u0026rdquo; module. Call Wasm Entry Point: The host calls the Wasm-exported trigger_host_calls function. This function will, in turn, trigger calls from within Wasm back to the host functions. Since this Wasm function returns void, host.callFunctionVoid can be used. // host.cpp (inside main, Test 2) int32_t trigger_arg = 7; int32_t host_value_before = shared_host_value; // Record state before call std::cout \u0026lt;\u0026lt; \u0026#34;[Host Main] Calling Wasm function \u0026#39;trigger_host_calls(\u0026#34; \u0026lt;\u0026lt; trigger_arg \u0026lt;\u0026lt; \u0026#34;)\u0026#39;...\u0026#34; \u0026lt;\u0026lt; std::endl; // host.callFunctionVoid wraps calling void Wasm functions // Params are tuple\u0026lt;i32\u0026gt; host.callFunctionVoid\u0026lt;std::tuple\u0026lt;int32_t\u0026gt;\u0026gt;( \u0026#34;trigger_host_calls\u0026#34;, std::make_tuple(trigger_arg)); std::cout \u0026lt;\u0026lt; \u0026#34;[Host Main] Returned from \u0026#39;trigger_host_calls\u0026#39;.\u0026#34; \u0026lt;\u0026lt; std::endl; // Check if host state was modified by Wasm after the call // ... Compare shared_host_value with the expected value ... Rust Wasm Side:\nDeclare Imports: Use extern \u0026quot;C\u0026quot; and #[link(wasm_import_module = \u0026quot;env\u0026quot;)] in src/ffi.rs to declare the functions needed from the host. Provide Safe Wrappers: Offer safe wrappers like log_value_from_host, get_shared_value_from_host, set_shared_value_in_host in src/ffi.rs. Export Trigger Function: The trigger_host_calls function is exported. Call Host Functions: In core::perform_host_calls_test (called by trigger_host_calls), invoke the C++ host functions indirectly by calling the safe wrappers from the FFI layer, thereby reading and modifying the host\u0026rsquo;s state. // src/ffi.rs - Import declarations and safe wrappers (shown previously) // src/ffi.rs - Export trigger function #[no_mangle] pub extern \u0026#34;C\u0026#34; fn trigger_host_calls(input_val: i32) { println!(\u0026#34;[WASM FFI] trigger_host_calls called with input: {}\u0026#34;, input_val); crate::core::perform_host_calls_test(input_val); // Call core logic println!(\u0026#34;[WASM FFI] trigger_host_calls finished.\u0026#34;); } // src/lib.rs::core - Core logic calling host functions pub fn perform_host_calls_test(input_val: i32) { println!(\u0026#34;[WASM Core] perform_host_calls_test with input: {}\u0026#34;, input_val); // 1. Call host_log_value log_value_from_host(input_val * 2); // 2. Call host_get_shared_value let host_val = get_shared_value_from_host(); println!(\u0026#34;[WASM Core] Received value from host: {}\u0026#34;, host_val); // 3. Call host_set_shared_value (modifying host state) let new_host_val = host_val.wrapping_add(input_val).wrapping_add(5); set_shared_value_in_host(new_host_val); // ... } This flow demonstrates calls from Wasm to C++ and how Wasm can influence the host\u0026rsquo;s state by invoking host functions.\nPattern Three: Sharing Structs via Memory (point_add) This is a more complex interaction involving passing struct data between the host and Wasm. Since C++ or Rust objects cannot be passed directly, we utilize the shared linear memory.\nC++ Host Side (host.cpp, Test 3):\nDefine Struct: Define the Point struct, using #pragma pack to ensure a controlled layout. Calculate Memory Offsets: Choose addresses (offsets) within the Wasm linear memory to store the input points p1, p2, and the result result. Ensure these addresses don\u0026rsquo;t overlap and have sufficient space. Write to Memory: Create C++ Point objects host_p1, host_p2. Use the host.writeMemory() method to copy the byte representation of these objects into the Wasm linear memory at the corresponding offsets offset_p1, offset_p2. writeMemory internally gets the memory Span and performs memcpy. Call Wasm Function: Invoke the Wasm-exported point_add function. Importantly, the arguments passed to Wasm are the previously calculated memory offsets (as int32_t pointers). Read from Memory: After the Wasm function executes, the result is written back to offset_result in Wasm memory. The host uses host.readMemory\u0026lt;Point\u0026gt;() to read the bytes from that offset and interpret them as a C++ Point object. readMemory also gets the memory Span and uses memcpy. Verify Result: Compare the result read back from Wasm memory with the expected result. // host.cpp (inside main, Test 3) const size_t point_size = sizeof(Point); const int32_t offset_p1 = 2048; // Example offset const int32_t offset_p2 = offset_p1 + point_size; const int32_t offset_result = offset_p2 + point_size; Point host_p1 = {100, 200}; Point host_p2 = {30, 70}; std::cout \u0026lt;\u0026lt; \u0026#34;[Host Main] Writing points to WASM memory...\u0026#34; \u0026lt;\u0026lt; std::endl; // host.writeMemory encapsulates getting Span and memcpy host.writeMemory(offset_p1, host_p1); // Write host_p1 to Wasm memory host.writeMemory(offset_p2, host_p2); // Write host_p2 to Wasm memory std::cout \u0026lt;\u0026lt; \u0026#34;[Host Main] Calling Wasm function \u0026#39;point_add\u0026#39; with offsets...\u0026#34; \u0026lt;\u0026lt; std::endl; // Args are offsets (i32), representing pointers auto point_add_args = std::make_tuple(offset_result, offset_p1, offset_p2); host.callFunctionVoid\u0026lt;std::tuple\u0026lt;int32_t, int32_t, int32_t\u0026gt;\u0026gt;(\u0026#34;point_add\u0026#34;, point_add_args); std::cout \u0026lt;\u0026lt; \u0026#34;[Host Main] Reading result struct from WASM memory...\u0026#34; \u0026lt;\u0026lt; std::endl; // host.readMemory encapsulates getting Span and memcpy Point result_point = host.readMemory\u0026lt;Point\u0026gt;(offset_result); // Read result from Wasm memory std::cout \u0026lt;\u0026lt; \u0026#34;[Host Main] \u0026#39;point_add\u0026#39; Result read from memory: { x: \u0026#34; \u0026lt;\u0026lt; result_point.x \u0026lt;\u0026lt; \u0026#34;, y: \u0026#34; \u0026lt;\u0026lt; result_point.y \u0026lt;\u0026lt; \u0026#34; }\u0026#34; \u0026lt;\u0026lt; std::endl; // ... Verify result ... // Simplified implementation of writeMemory/readMemory in WasmHost: template \u0026lt;typename T\u0026gt; void WasmHost::writeMemory(int32_t offset, const T\u0026amp; data) { auto memory_span = getMemorySpan(); size_t data_size = sizeof(T); if (offset \u0026lt; 0 || static_cast\u0026lt;size_t\u0026gt;(offset) + data_size \u0026gt; memory_span.size()) { throw std::out_of_range(\u0026#34;Memory write out of bounds\u0026#34;); } std::memcpy(memory_span.data() + offset, \u0026amp;data, data_size); } template \u0026lt;typename T\u0026gt; T WasmHost::readMemory(int32_t offset) { auto memory_span = getMemorySpan(); size_t data_size = sizeof(T); if (offset \u0026lt; 0 || static_cast\u0026lt;size_t\u0026gt;(offset) + data_size \u0026gt; memory_span.size()) { throw std::out_of_range(\u0026#34;Memory read out of bounds\u0026#34;); } T result; std::memcpy(\u0026amp;result, memory_span.data() + offset, data_size); return result; } Rust Wasm Side:\nDefine Struct: Define the Point struct using #[repr(C)] to ensure layout compatibility with the C++ side. Export Function: Export the point_add function. Its parameters are *mut Point and *const Point. These receive the 32-bit integers (memory offsets) from the host, which Wasmtime interprets as pointers into the Wasm linear memory. Use unsafe: Inside the function body, an unsafe block is mandatory to dereference these raw pointers ( *result_ptr, *p1_ptr, *p2_ptr). The Rust compiler cannot guarantee the validity of these pointers (they originate from the external world), so the developer must take responsibility. Perform Operation: Read the input Point data from the pointers, call core::add_points to compute the result. Write to Memory: Write the calculated result back to the memory location specified by the host using *result_ptr = result;. // src/ffi.rs - Point struct definition (shown previously) // src/ffi.rs - Export point_add function #[no_mangle] pub extern \u0026#34;C\u0026#34; fn point_add(result_ptr: *mut Point, p1_ptr: *const Point, p2_ptr: *const Point) { println!(\u0026#34;[WASM FFI] point_add called with pointers...\u0026#34;); unsafe { // Must use unsafe to dereference raw pointers if result_ptr.is_null() || p1_ptr.is_null() || p2_ptr.is_null() { println!(\u0026#34;[WASM FFI] Error: Received null pointer.\u0026#34;); return; } // Dereference input pointers to read data let p1 = *p1_ptr; let p2 = *p2_ptr; // Call core logic for calculation let result = crate::core::add_points(p1, p2); // Dereference output pointer to write the result *result_ptr = result; println!(\u0026#34;[WASM FFI] Wrote result to address {:?}\u0026#34;, result_ptr); } } // src/lib.rs::core - Core addition logic pub fn add_points(p1: Point, p2: Point) -\u0026gt; Point { println!(\u0026#34;[WASM Core] add_points called with p1: {:?}, p2: {:?}\u0026#34;, p1, p2); Point { x: p1.x.wrapping_add(p2.x), y: p1.y.wrapping_add(p2.y), } } This pattern forms the basis for complex data exchange between Wasm and the host. Key elements are agreed-upon memory layouts, access via pointers (offsets), and the correct use of unsafe in Rust.\nPattern Four: Host Directly Reads/Writes Wasm Internal State This pattern demonstrates (but does not recommend) how the host can directly modify internal static mut state within the Wasm module.\nC++ Host Side (host.cpp, Test 4):\nGet State Pointer: Call the Wasm-exported get_plugin_shared_value_ptr function. This function returns an int32_t, representing the offset of PLUGIN_SHARED_VALUE within Wasm linear memory. Read Initial Value: Use host.readMemory\u0026lt;int32_t\u0026gt;() to read the current value of the Wasm state from the obtained offset. Write New Value: Use host.writeMemory() to write a new int32_t value to that offset. Read Again to Verify: Use host.readMemory\u0026lt;int32_t\u0026gt;() again to confirm the write was successful. // host.cpp (inside main, Test 4) int32_t plugin_value_offset = -1; // ... std::cout \u0026lt;\u0026lt; \u0026#34;[Host Main] Calling Wasm \u0026#39;get_plugin_shared_value_ptr\u0026#39;...\u0026#34; \u0026lt;\u0026lt; std::endl; // getPluginDataOffset wraps calling the Wasm function to get the offset plugin_value_offset = host.getPluginDataOffset(\u0026#34;get_plugin_shared_value_ptr\u0026#34;); std::cout \u0026lt;\u0026lt; \u0026#34;[Host Main] Received offset: \u0026#34; \u0026lt;\u0026lt; plugin_value_offset \u0026lt;\u0026lt; std::endl; if (plugin_value_offset \u0026gt; 0) { // Basic validity check // Read Wasm state int32_t value_from_plugin_before = host.readMemory\u0026lt;int32_t\u0026gt;(plugin_value_offset); std::cout \u0026lt;\u0026lt; \u0026#34;[Host Main] Value read from plugin: \u0026#34; \u0026lt;\u0026lt; value_from_plugin_before \u0026lt;\u0026lt; std::endl; // Write new value to Wasm state const int32_t new_value_for_plugin = 777; std::cout \u0026lt;\u0026lt; \u0026#34;[Host Main] Writing new value (\u0026#34; \u0026lt;\u0026lt; new_value_for_plugin \u0026lt;\u0026lt; \u0026#34;) to plugin state...\u0026#34; \u0026lt;\u0026lt; std::endl; host.writeMemory(plugin_value_offset, new_value_for_plugin); // Read again to verify int32_t value_from_plugin_after = host.readMemory\u0026lt;int32_t\u0026gt;(plugin_value_offset); std::cout \u0026lt;\u0026lt; \u0026#34;[Host Main] Value read after host write: \u0026#34; \u0026lt;\u0026lt; value_from_plugin_after \u0026lt;\u0026lt; std::endl; // ... Verify value_from_plugin_after == new_value_for_plugin ... } // WasmHost::getPluginDataOffset implementation int32_t WasmHost::getPluginDataOffset(std::string_view func_name) { std::cout \u0026lt;\u0026lt; \u0026#34;[Host] Getting plugin data offset via \u0026#39;\u0026#34; \u0026lt;\u0026lt; func_name \u0026lt;\u0026lt; \u0026#34;\u0026#39;...\u0026#34; \u0026lt;\u0026lt; std::endl; // Wasm function takes no args, returns i32 (offset) auto result_tuple = callFunction\u0026lt;std::tuple\u0026lt;int32_t\u0026gt;\u0026gt;(func_name); if (!result_tuple) { /* Error handling */ return -1; } int32_t offset = std::get\u0026lt;0\u0026gt;(result_tuple.ok()); std::cout \u0026lt;\u0026lt; \u0026#34;[Host] Received offset from plugin: \u0026#34; \u0026lt;\u0026lt; offset \u0026lt;\u0026lt; std::endl; return offset; } Rust Wasm Side:\nDefine static mut State: static mut PLUGIN_SHARED_VALUE: i32 = 100; Export Pointer Function: Export the get_plugin_shared_value_ptr function, which, within an unsafe context, returns the raw pointer (offset) to PLUGIN_SHARED_VALUE. // src/ffi.rs static mut PLUGIN_SHARED_VALUE: i32 = 100; #[no_mangle] pub unsafe extern \u0026#34;C\u0026#34; fn get_plugin_shared_value_ptr() -\u0026gt; *mut i32 { let ptr = { \u0026amp;raw mut PLUGIN_SHARED_VALUE as *mut i32 }; println!(\u0026#34;[WASM FFI] get_plugin_shared_value_ptr() -\u0026gt; {:?}\u0026#34;, ptr); ptr } This pattern showcases the power of memory manipulation but also highlights the potential risks. The host can now directly interfere with Wasm\u0026rsquo;s internal implementation details.\nPattern Five: Wasm Verifies Internal State Change by Host To confirm that the host\u0026rsquo;s write in Pattern Four actually took effect, we let the Wasm module itself check the value of that static mut variable.\nC++ Host Side (host.cpp, Test 5):\nAfter modifying the Wasm state in Pattern Four, call another Wasm function (e.g., simple_add, repurposed here). We aren\u0026rsquo;t interested in this function\u0026rsquo;s return value, but rather in the log output it generates from within Wasm.\n// host.cpp (inside main, Test 5, assuming plugin_value_offset \u0026gt; 0) std::cout \u0026lt;\u0026lt; \u0026#34;[Host Main] Calling Wasm \u0026#39;simple_add\u0026#39; to verify internal state...\u0026#34; \u0026lt;\u0026lt; std::endl; // Call a Wasm function, allowing it to read and print its own state auto args = std::make_tuple(1ULL, 1ULL); host.callFunction\u0026lt;std::tuple\u0026lt;uint64_t\u0026gt;, std::tuple\u0026lt;uint64_t, uint64_t\u0026gt;\u0026gt;( \u0026#34;simple_add\u0026#34;, args); std::cout \u0026lt;\u0026lt; \u0026#34;[Host Main] Returned from \u0026#39;simple_add\u0026#39;. Check WASM output above.\u0026#34; \u0026lt;\u0026lt; std::endl; Rust Wasm Side:\nWe need to modify the simple_add function (or the core logic it calls, perform_simple_add_and_read_internal_state) so that before performing its main task, it reads the value of PLUGIN_SHARED_VALUE and prints it.\n// src/ffi.rs #[no_mangle] pub extern \u0026#34;C\u0026#34; fn simple_add(left: u64, right: u64) -\u0026gt; u64 { println!(\u0026#34;[WASM FFI] simple_add (verification step) called...\u0026#34;); crate::core::perform_simple_add_and_read_internal_state(left, right) } // Internal helper function to read static mut (requires unsafe) pub(crate) fn read_plugin_value_internal() -\u0026gt; i32 { unsafe { PLUGIN_SHARED_VALUE } } // src/lib.rs::core pub fn perform_simple_add_and_read_internal_state(left: u64, right: u64) -\u0026gt; u64 { // Read and print its own internal state let current_plugin_val = read_plugin_value_internal(); // Call FFI helper println!( \u0026#34;[WASM Core] Current plugin\u0026#39;s internal shared value: {}\u0026#34;, // Expecting 777 here current_plugin_val ); println!(\u0026#34;[WASM Core] Performing simple add: {} + {}\u0026#34;, left, right); // ... Perform original addition logic ... left + right // Assuming simple return } When the host executes Test 5, we should see output from [WASM Core] in the console showing Current plugin's internal shared value: 777 (or whatever value was written in Pattern Four). This verifies that the host successfully modified the Wasm\u0026rsquo;s internal state.\nKey Takeaways and Considerations This example highlights several crucial points when using Wasmtime for C++/Rust Wasm interactions:\nClear Interface Definition: The FFI layer is central. Rust\u0026rsquo;s extern \u0026quot;C\u0026quot; (for both imports and exports) and the C++ function signatures/linking must match precisely. Memory Operations are Fundamental: Passing complex data relies on reading and writing to Wasm\u0026rsquo;s linear memory. Understanding pointers as offsets and ensuring consistent data structure layouts (#[repr(C)], #pragma pack) is vital. Necessity of unsafe: In the Rust Wasm module, interacting with the FFI and static mut almost inevitably requires unsafe blocks. Use them cautiously and confine them to the FFI boundary layer whenever possible. Careful State Management: Both the host and Wasm can maintain state. They can influence each other\u0026rsquo;s state through function calls. Directly exposing pointers to Wasm\u0026rsquo;s internal state to the host, while technically feasible, breaks encapsulation and should generally be avoided. Prefer managing state through interface functions. Role of WASI: For Wasm modules needing standard I/O or other system interactions (even just println!), the host must configure and link WASI. Wasmtime API: Wasmtime provides a comprehensive C++ API (wasmtime.hh) featuring core classes like Engine, Store, Module, Linker, Instance, Memory, Func, TypedFunc, Val, and error handling mechanisms like Result and Trap. Understanding the roles and relationships of these classes is key to successful implementation. Conclusion WebAssembly and Wasmtime offer a powerful way to extend existing applications and achieve high-performance, secure, and portable modularity. The combination of C++ and Rust leverages C++\u0026rsquo;s ecosystem and performance while benefiting from Rust\u0026rsquo;s safety guarantees, making it particularly suitable for building plugin systems, handling performance-critical tasks, or scenarios requiring strong sandboxing.\nWhile the interaction patterns covered here are quite comprehensive, they represent just the tip of the iceberg. Wasmtime also supports more advanced features like epoch-based interruption, fuel metering for resource control, reference types, multiple memories, threading, and more.\nHopefully, this detailed walkthrough has helped you grasp the fundamental principles and practical methods for enabling interaction between a C++ host and a Rust Wasm module using Wasmtime. If this area interests you, I encourage you to experiment and integrate Wasm into your next project!\n","permalink":"https://blog.tategotoazarasi.me/en/posts/deep-dive-into-wasmtime-bidirectional-communication-and-memory-sharing-between-cpp-and-rust-wasm-modules/","summary":"A detailed technical guide on using the Wasmtime runtime to enable complex bidirectional communication, shared memory access, and struct passing between C++ host applications and Rust WebAssembly modules.","title":"Deep Dive into Wasmtime: Bidirectional Communication and Memory Sharing Between C++ and Rust Wasm Modules"},{"content":"If you\u0026rsquo;re involved in C++ game development or interested in high-performance Entity Component Systems (ECS), chances are you\u0026rsquo;ve heard of EnTT. It\u0026rsquo;s a highly popular, C++17-based, header-only library renowned for its outstanding performance, flexibility, and embrace of modern C++ features.\nThe ECS pattern itself is a powerful architectural paradigm. It promotes data-driven design by decoupling \u0026ldquo;things\u0026rdquo; ( Entities), their \u0026ldquo;data\u0026rdquo; (Components), and their \u0026ldquo;behavior\u0026rdquo; (Systems). This leads to scalable, high-performance, and maintainable applications, especially in scenarios like games that handle vast numbers of dynamic objects and complex interactions.\nHowever, when transitioning from traditional relational databases or other object-oriented design patterns to ECS, a common question arises: How do you represent and manage relationships between entities within an ECS? For instance, how does a player character (entity) link to their account information (another entity)? How does a parent node (entity) know all its child nodes (multiple entities)? How should the many-to-many enrollment relationship between students ( entities) and courses (entities) be handled?\nIn relational databases, we have well-established mechanisms like foreign keys and join tables to manage these connections. But in EnTT, or indeed many ECS implementations, there isn\u0026rsquo;t a built-in, first-class concept of \u0026ldquo;foreign keys\u0026rdquo; or \u0026ldquo;join tables.\u0026rdquo; This doesn\u0026rsquo;t mean it\u0026rsquo;s impossible; rather, it requires us to leverage the core mechanics of ECS – entities, components, and the registry – to cleverly construct these relationships.\nThe purpose of this blog post is to take you on a deep dive into how to represent and manage the three most common types of entity relationships in EnTT using components as the vehicle: one-to-one (1:1), one-to-many (1:N), and many-to-many ( N:N). We won\u0026rsquo;t just discuss how to \u0026ldquo;represent\u0026rdquo; these relationships, but also how to implement their basic operations: Create, Read, Update, and Delete – commonly known as CRUD.\nWe\u0026rsquo;ll start with some fundamental EnTT concepts, particularly what an entity (entt::entity) truly is and how it works, as this is crucial for understanding relationship management. Then, we\u0026rsquo;ll progressively delve into the specific implementation strategies for each relationship type, discussing the pros and cons of different approaches, and illustrating practical operations through dissected code examples. We\u0026rsquo;ll pay special attention to potential pitfalls encountered during implementation, such as a subtle issue discovered in the N:N relationship implementation (and its solution) during previous discussions, and how to safely handle potential \u0026ldquo;dangling references\u0026rdquo; (i.e., relationships pointing to destroyed entities).\nReady? Let\u0026rsquo;s journey into the world of EnTT and see how we can elegantly weave a network of relationships between entities using components.\nEnTT Fundamentals: Registry, Entities, and Components Before we dive into relationships, it\u0026rsquo;s essential to have a clear understanding of EnTT\u0026rsquo;s core concepts.\nThe Registry entt::registry is the heart of EnTT. Think of it as the central manager of your ECS \u0026ldquo;world,\u0026rdquo; or a highly flexible \u0026quot; database.\u0026quot; All entities, components, and their associations are stored and maintained by the registry. Creating one is straightforward:\n#include \u0026lt;entt/entt.hpp\u0026gt; entt::registry my_world; // Just like that, an empty ECS world is born This registry object will be our entry point for all subsequent operations, such as creating entities, adding components, querying, etc. One of EnTT\u0026rsquo;s design philosophies is \u0026ldquo;pay for what you use\u0026rdquo;; the registry itself is lightweight, only allocating storage for specific component types internally when you start using them.\nEntities An entity, represented by the entt::entity type in EnTT, is the \u0026ldquo;E\u0026rdquo; in ECS. But be aware: it\u0026rsquo;s not a traditional C++ object. You can\u0026rsquo;t add methods or member variables to an entt::entity. It\u0026rsquo;s essentially just a lightweight identifier, a unique \u0026ldquo;ID card,\u0026rdquo; used to mark a \u0026ldquo;thing\u0026rdquo; in your game world. This thing could be a player character, a bullet, a UI element, or anything you need to track independently.\nCreating entities is simple, done via the registry:\nentt::entity player_entity = my_world.create(); entt::entity enemy_entity = my_world.create(); The entt::entity value returned by create() is the unique identifier for this new entity.\nNow, let\u0026rsquo;s delve deeper into the \u0026ldquo;identity\u0026rdquo; of an entt::entity, which is particularly important when discussing relationships. In previous discussions, we saw usage like (uint32_t)some_entity, seemingly implying it\u0026rsquo;s just a simple 32-bit unsigned integer ID. But it\u0026rsquo;s more nuanced than that.\nentt::entity (by default) is based on uint32_t, but it encodes two pieces of information within those 32 bits (or other sizes; 32 is default):\nEntity Index (or Slot): This part can be viewed as the entity\u0026rsquo;s position or slot number within some internal storage structure (like an array). Entity Version: This is a counter associated with a specific index/slot. Why this design? Imagine we create entity A, assigned index 5 and version 1. Later, we destroy entity A. Its index 5 becomes available for reuse. Sometime after, we create a new entity B, and EnTT happens to reuse index 5. However, to distinguish the new entity B from the destroyed entity A, EnTT increments the version number associated with index 5, perhaps to 2. So, entity A\u0026rsquo;s entt::entity value represents (index 5, version 1), while entity B\u0026rsquo;s represents (index 5, version 2). These translate to different underlying uint32_t values.\nThe core purpose of this \u0026ldquo;index + version\u0026rdquo; design is safety. If you hold onto an old entity handle entityA_handle (representing index 5, version 1), and before you use it again, entity A is destroyed and index 5 is reused by the new entity B (version 2). When you try to access components using entityA_handle, EnTT can use the registry.valid(entityA_handle) function to detect that the version in your handle (1) doesn\u0026rsquo;t match the current version stored for index 5 (2). It thus knows your handle is stale (points to a \u0026ldquo;zombie\u0026rdquo; entity) and can prevent you from incorrectly accessing data belonging to entity B. This is known as dangling handle detection.\nSo, back to the (uint32_t)some_entity cast. It does extract the underlying 32-bit integer value, which contains the combined index and version information. In our example code, it\u0026rsquo;s primarily used to conveniently print a number for logging or debugging. But it\u0026rsquo;s crucial to understand:\nThis specific uint32_t value, for a particular entity instance (like entity A or entity B in the example), is immutable during its lifetime. After an entity is destroyed, the exact uint32_t value that represented it (e.g., the value for \u0026ldquo;index 5, version 1\u0026rdquo;) will not be assigned to a new, different entity instance. Even if index 5 is reused, the new entity will have a different version number, resulting in a different uint32_t value. In this sense, the uint32_t value acts as an \u0026ldquo;immutable identifier\u0026rdquo; for that specific entity instance. It forever refers to that instance, whether it\u0026rsquo;s alive or destroyed. It won\u0026rsquo;t \u0026ldquo;drift\u0026rdquo; to point to another instance. However, it differs from concepts like UUIDs or database auto-increment primary keys (which are never reused and entirely independent), because its \u0026ldquo;index\u0026rdquo; part can be reused. EnTT officially recommends treating entt::entity as an opaque handle. Its internal structure might change, and we should rely on registry.valid() to check its validity rather than attempting to parse it.\nWith a solid grasp of entt::entity\u0026rsquo;s nature, we can build relationships with more confidence.\nComponents Components are the \u0026ldquo;C\u0026rdquo; in ECS, representing the data owned by entities. In EnTT, components can be any C++ struct or class, typically Plain Old Data Structures (PODS) or PODS-like types containing only data. They don\u0026rsquo;t need to inherit from any specific base class or be pre-registered with the registry.\nstruct Position { float x = 0.0f; float y = 0.0f; }; struct Velocity { float dx = 0.0f; float dy = 0.0f; }; struct Renderable { std::string sprite_id; int z_order = 0; }; struct PlayerTag {}; // Empty structs can also be components, often used for tagging entities To add components to an entity, we use the registry\u0026rsquo;s emplace or emplace_or_replace methods:\nentt::entity player = my_world.create(); // Add Position and Velocity components, initializing them directly in emplace my_world.emplace\u0026lt;Position\u0026gt;(player, 100.0f, 50.0f); my_world.emplace\u0026lt;Velocity\u0026gt;(player, 5.0f, 0.0f); // Add a Renderable component my_world.emplace\u0026lt;Renderable\u0026gt;(player, \u0026#34;player_sprite\u0026#34;, 10); // Add a tag component my_world.emplace\u0026lt;PlayerTag\u0026gt;(player); Core Operation Overview Besides creating entities (create) and adding components (emplace, emplace_or_replace), here are some core operations we\u0026rsquo;ll frequently use:\nDestroy Entity: my_world.destroy(player); Destroys the entity and all its components. Get Component: Position\u0026amp; pos = my_world.get\u0026lt;Position\u0026gt;(player); Gets a component reference. Undefined behavior (usually assertion failure or crash) if the entity doesn\u0026rsquo;t have the component. Position* pos_ptr = my_world.try_get\u0026lt;Position\u0026gt;(player); Attempts to get a component pointer. Returns nullptr if the entity doesn\u0026rsquo;t have the component. This is the safer approach. Modify Component: my_world.patch\u0026lt;Position\u0026gt;(player, [](auto\u0026amp; p) { p.x += 10.0f; }); Gets the component (creating it if it doesn\u0026rsquo;t exist) and modifies it via a lambda. Modify directly after getting a reference or pointer via get or try_get. Remove Component: my_world.remove\u0026lt;Velocity\u0026gt;(player); Check Component Existence: bool has_pos = my_world.all_of\u0026lt;Position\u0026gt;(player); Check Entity Validity: bool is_valid = my_world.valid(player); The Null Entity EnTT provides a special constant entt::null, which represents an invalid entity. You can use it to signify \u0026ldquo;no entity\u0026rdquo; or the absence of a relationship. my_world.valid(entt::null) always returns false.\nentt::entity no_entity = entt::null; if (my_world.valid(no_entity)) { // This code will never execute } Alright, equipped with these fundamentals, we can start building entity relationships.\nThe Core Principle: Representing Relationships with Components As mentioned earlier, EnTT doesn\u0026rsquo;t have built-in relationship types. Our core strategy is: use components to store relationship information. Specifically, we typically store the entt::entity identifier(s) of related entities within a component attached to one or both entities involved in the relationship.\nBelow, we\u0026rsquo;ll explore the specific implementations for 1:1, 1:N, and N:N relationships.\nImplementing 1:1 Relationships (e.g., Player \u0026lt;-\u0026gt; Player Profile) A one-to-one relationship means one entity is precisely linked to another, and vice versa. For example, a player entity corresponds to a player profile entity.\nStrategy Selection The most direct way to represent this relationship is to add a component to entities on both ends of the relationship, with each component storing the entt::entity ID of the other party.\nOn the Player entity, add a PlayerRelation component containing a profileEntity member (of type entt::entity). On the Player Profile entity, add a ProfileRelation component containing a playerEntity member (of type entt::entity). If an entity hasn\u0026rsquo;t established a relationship yet, or the relationship is severed, the corresponding entt::entity member can be set to entt::null.\n// Component on the player pointing to their profile struct PlayerRelation { entt::entity profileEntity = entt::null; // Points to the associated Profile entity }; // Component on the profile pointing back to its player struct ProfileRelation { entt::entity playerEntity = entt::null; // Points to the associated Player entity }; // Some auxiliary data components to make the example concrete struct PlayerName { std::string name; }; struct ProfileData { std::string bio; }; This bidirectional linking makes looking up the counterpart from either end very convenient.\nCreate (Establishing the Relationship / Linking) We need a function to establish this link. This function requires the registry and the IDs of the two entities to be linked.\n#include \u0026lt;cassert\u0026gt; // For assertion checks #include \u0026lt;iostream\u0026gt; // For logging #include \u0026lt;cstdint\u0026gt; // For uint32_t cast void linkPlayerProfile(entt::registry\u0026amp; registry, entt::entity player, entt::entity profile) { // Ensure the passed entity IDs are valid assert(registry.valid(player) \u0026amp;\u0026amp; \u0026#34;Invalid player entity\u0026#34;); assert(registry.valid(profile) \u0026amp;\u0026amp; \u0026#34;Invalid profile entity\u0026#34;); // (Optional but recommended) Check and clean up potentially existing old links. // If \u0026#39;player\u0026#39; is already linked to another profile, or \u0026#39;profile\u0026#39; is linked to another player, // you might need to unlink the old relationship first. Here, we simplify by overwriting. // Real applications might need more complex logic to decide if overwriting is allowed. // Use emplace_or_replace to add or update the relationship components. // If the component exists, it\u0026#39;s replaced; if not, it\u0026#39;s created. registry.emplace_or_replace\u0026lt;PlayerRelation\u0026gt;(player, profile); registry.emplace_or_replace\u0026lt;ProfileRelation\u0026gt;(profile, player); // (For demonstration) Print a log message // Note: Directly printing entt::entity might not output a number, requires casting. std::cout \u0026lt;\u0026lt; \u0026#34;Linked Player \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(player) \u0026lt;\u0026lt; \u0026#34; with Profile \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(profile) \u0026lt;\u0026lt; std::endl; } // Example Usage: // entt::registry registry; // entt::entity player1 = registry.create(); // registry.emplace\u0026lt;PlayerName\u0026gt;(player1, \u0026#34;Alice\u0026#34;); // entt::entity profile1 = registry.create(); // registry.emplace\u0026lt;ProfileData\u0026gt;(profile1, \u0026#34;Loves coding.\u0026#34;); // linkPlayerProfile(registry, player1, profile1); Read (Reading the Relationship / Finding the Partner) We need functions to find one entity based on the other.\nentt::entity getProfileForPlayer(entt::registry\u0026amp; registry, entt::entity player) { if (!registry.valid(player)) return entt::null; // Check input entity validity // Use try_get to get the relationship component pointer safely auto* relation = registry.try_get\u0026lt;PlayerRelation\u0026gt;(player); // Check if the component exists AND if the partner ID stored within it is still valid if (relation \u0026amp;\u0026amp; registry.valid(relation-\u0026gt;profileEntity)) { return relation-\u0026gt;profileEntity; } return entt::null; // Not found or partner is stale } entt::entity getPlayerForProfile(entt::registry\u0026amp; registry, entt::entity profile) { if (!registry.valid(profile)) return entt::null; auto* relation = registry.try_get\u0026lt;ProfileRelation\u0026gt;(profile); if (relation \u0026amp;\u0026amp; registry.valid(relation-\u0026gt;playerEntity)) { return relation-\u0026gt;playerEntity; } return entt::null; } // Example Usage: // entt::entity foundProfile = getProfileForPlayer(registry, player1); // if (registry.valid(foundProfile)) { // // Get partner\u0026#39;s data // auto\u0026amp; data = registry.get\u0026lt;ProfileData\u0026gt;(foundProfile); // std::cout \u0026lt;\u0026lt; \u0026#34;Found profile for Player \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(player1) // \u0026lt;\u0026lt; \u0026#34;, Bio: \u0026#34; \u0026lt;\u0026lt; data.bio \u0026lt;\u0026lt; std::endl; // } else { // std::cout \u0026lt;\u0026lt; \u0026#34;Player \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(player1) \u0026lt;\u0026lt; \u0026#34; has no valid associated profile.\u0026#34; \u0026lt;\u0026lt; std::endl; // } Key Point: After retrieving a partner entity\u0026rsquo;s ID, always use registry.valid() to re-check if that partner entity itself is still valid. The partner could have been destroyed between the time you retrieved the ID and when you try to use it.\nUpdate (Updating the Relationship or Associated Data) Updating can refer to two scenarios:\nChanging the Relationship Target: Make Player A associate with Profile Y instead of Profile X. This usually involves first dissolving the old link (see Delete operation below) and then calling linkPlayerProfile to establish the new one. Modifying the Associated Entity\u0026rsquo;s Data via the Relationship: This is more common. For example, updating the Bio information of a profile associated with a player entity. void updateProfileBio(entt::registry\u0026amp; registry, entt::entity player, const std::string\u0026amp; newBio) { entt::entity profile = getProfileForPlayer(registry, player); // First, find the associated profile if (registry.valid(profile)) { // Ensure the profile entity is valid // Use patch or try_get/get to modify the ProfileData component on the profile // patch is concise; it creates ProfileData if absent (maybe not desired) // try_get is safer, only modifying if the component exists if (auto* data = registry.try_get\u0026lt;ProfileData\u0026gt;(profile)) { data-\u0026gt;bio = newBio; std::cout \u0026lt;\u0026lt; \u0026#34;Updated Bio for Profile \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(profile) \u0026lt;\u0026lt; \u0026#34; associated with Player \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(player) \u0026lt;\u0026lt; \u0026#34;.\u0026#34; \u0026lt;\u0026lt; std::endl; } else { std::cerr \u0026lt;\u0026lt; \u0026#34;Error: Profile \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(profile) \u0026lt;\u0026lt; \u0026#34; has no ProfileData component.\u0026#34; \u0026lt;\u0026lt; std::endl; } } else { std::cerr \u0026lt;\u0026lt; \u0026#34;Error: Player \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(player) \u0026lt;\u0026lt; \u0026#34; has no valid associated profile.\u0026#34; \u0026lt;\u0026lt; std::endl; } } // Example Usage: // updateProfileBio(registry, player1, \u0026#34;Loves coding and EnTT!\u0026#34;); Delete (Deleting the Relationship / Unlinking) Dissolving a 1:1 relationship requires updating the relationship components on both entities.\nvoid unlinkPlayerProfile(entt::registry\u0026amp; registry, entt::entity entity) { if (!registry.valid(entity)) return; // Check input entity entt::entity partner = entt::null; bool was_player = false; // Flag to know if the input was Player or Profile, for correct partner component removal // Try to unlink from the Player\u0026#39;s perspective if (auto* playerRel = registry.try_get\u0026lt;PlayerRelation\u0026gt;(entity)) { partner = playerRel-\u0026gt;profileEntity; registry.remove\u0026lt;PlayerRelation\u0026gt;(entity); // Remove relation component from player was_player = true; std::cout \u0026lt;\u0026lt; \u0026#34;Unlinking from Player \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(entity) \u0026lt;\u0026lt; \u0026#34;...\u0026#34;; } // Otherwise, try to unlink from the Profile\u0026#39;s perspective else if (auto* profileRel = registry.try_get\u0026lt;ProfileRelation\u0026gt;(entity)) { partner = profileRel-\u0026gt;playerEntity; registry.remove\u0026lt;ProfileRelation\u0026gt;(entity); // Remove relation component from profile std::cout \u0026lt;\u0026lt; \u0026#34;Unlinking from Profile \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(entity) \u0026lt;\u0026lt; \u0026#34;...\u0026#34;; } else { // This entity has no 1:1 relationship component, nothing to do std::cout \u0026lt;\u0026lt; \u0026#34;Entity \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(entity) \u0026lt;\u0026lt; \u0026#34; has no 1:1 relationship to unlink.\u0026#34; \u0026lt;\u0026lt; std::endl; return; } // If a partner was found and the partner entity is still valid, remove the relationship component from the partner too if (registry.valid(partner)) { std::cout \u0026lt;\u0026lt; \u0026#34; and from partner \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(partner) \u0026lt;\u0026lt; \u0026#34;.\u0026#34; \u0026lt;\u0026lt; std::endl; if (was_player) { // If input was player, partner is profile, remove ProfileRelation registry.remove\u0026lt;ProfileRelation\u0026gt;(partner); } else { // If input was profile, partner is player, remove PlayerRelation registry.remove\u0026lt;PlayerRelation\u0026gt;(partner); } } else { std::cout \u0026lt;\u0026lt; \u0026#34; (Partner entity already invalid)\u0026#34; \u0026lt;\u0026lt; std::endl; } } // Example Usage: // unlinkPlayerProfile(registry, player1); // assert(getProfileForPlayer(registry, player1) == entt::null); // Verify unlinking worked // assert(getPlayerForProfile(registry, profile1) == entt::null); Note that this unlink function only removes the relationship; it doesn\u0026rsquo;t destroy the entities themselves.\nImplementing 1:N Relationships (e.g., Parent Node -\u0026gt; Child Nodes) One-to-many relationships, like parent-child nodes in a scene graph, or a team entity linked to multiple member entities.\nStrategy Selection There are two primary strategies here:\nParent-Centric: Add a component to the parent entity containing a list of child entity IDs (e.g., std::vector\u0026lt;entt::entity\u0026gt;). Child-Centric: Add a component to each child entity containing the ID of its parent. Which is better?\nParent-Centric: Finding all children from the parent is simple (direct list access). However, finding the parent from a child is difficult (requires iterating through all potential parents and checking their lists). If a parent has many children, the list component can become large, potentially impacting cache efficiency. Adding/removing children requires modifying the parent\u0026rsquo;s component. Child-Centric: Finding the parent from a child is very simple (direct component access). Finding all children of a parent requires iterating through all entities that have the \u0026ldquo;parent component\u0026rdquo; and checking if their parent ID matches (which EnTT\u0026rsquo;s view can do efficiently). Adding/removing a child only requires modifying the child\u0026rsquo;s own component. This approach generally aligns better with ECS principles of data locality and often performs better when querying the \u0026ldquo;N\u0026rdquo; side (children). Therefore, we typically recommend and will use the Child-Centric strategy.\n// Component on the child pointing to its parent struct ParentComponent { entt::entity parentEntity = entt::null; // Points to the parent entity }; // Auxiliary data component struct NodeLabel { std::string label; }; Create (Establishing the Relationship / Setting the Parent) Add or update the ParentComponent on the child entity.\nvoid setParent(entt::registry\u0026amp; registry, entt::entity child, entt::entity parent) { assert(registry.valid(child) \u0026amp;\u0026amp; \u0026#34;Invalid child entity\u0026#34;); // \u0026#39;parent\u0026#39; is allowed to be entt::null, indicating removal of parent relationship assert((parent == entt::null || registry.valid(parent)) \u0026amp;\u0026amp; \u0026#34;Invalid parent entity\u0026#34;); registry.emplace_or_replace\u0026lt;ParentComponent\u0026gt;(child, parent); // Add or update the parent ID if (parent != entt::null) { std::cout \u0026lt;\u0026lt; \u0026#34;Set Parent of Child \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(child) \u0026lt;\u0026lt; \u0026#34; to \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(parent) \u0026lt;\u0026lt; std::endl; } else { std::cout \u0026lt;\u0026lt; \u0026#34;Removed Parent from Child \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(child) \u0026lt;\u0026lt; \u0026#34;.\u0026#34; \u0026lt;\u0026lt; std::endl; } } // Example Usage: // entt::entity parentNode = registry.create(); // registry.emplace\u0026lt;NodeLabel\u0026gt;(parentNode, \u0026#34;Root\u0026#34;); // entt::entity child1 = registry.create(); // registry.emplace\u0026lt;NodeLabel\u0026gt;(child1, \u0026#34;Child A\u0026#34;); // setParent(registry, child1, parentNode); Read (Reading the Relationship) Finding the Parent from a Child: entt::entity getParent(entt::registry\u0026amp; registry, entt::entity child) { if (!registry.valid(child)) return entt::null; auto* parentComp = registry.try_get\u0026lt;ParentComponent\u0026gt;(child); // Again, check if the parent entity is still valid if (parentComp \u0026amp;\u0026amp; registry.valid(parentComp-\u0026gt;parentEntity)) { return parentComp-\u0026gt;parentEntity; } return entt::null; } // Example Usage: // entt::entity foundParent = getParent(registry, child1); Finding All Children from a Parent: This requires leveraging EnTT\u0026rsquo;s Views. Views allow us to efficiently iterate over all entities possessing specific components (or combinations thereof).\n#include \u0026lt;vector\u0026gt; std::vector\u0026lt;entt::entity\u0026gt; findChildren(entt::registry\u0026amp; registry, entt::entity parent) { std::vector\u0026lt;entt::entity\u0026gt; children; if (!registry.valid(parent)) return children; // Return empty if parent is invalid // Create a view to iterate over all entities with a ParentComponent auto view = registry.view\u0026lt;ParentComponent\u0026gt;(); // Iterate through each entity in the view (these are potential children) for (entt::entity child_entity : view) { // Get the ParentComponent for this entity // Inside a view loop, view.get is often more efficient than registry.get const auto\u0026amp; p_comp = view.get\u0026lt;ParentComponent\u0026gt;(child_entity); // Check if its parent is the one we\u0026#39;re looking for if (p_comp.parentEntity == parent) { // If yes, add it to the results list // child_entity is guaranteed to be valid within the view iteration, no need for another valid() check children.push_back(child_entity); } } return children; } // Example Usage: // std::vector\u0026lt;entt::entity\u0026gt; kids = findChildren(registry, parentNode); // std::cout \u0026lt;\u0026lt; \u0026#34;Children of Parent \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(parentNode) \u0026lt;\u0026lt; \u0026#34;: \u0026#34;; // for(entt::entity k : kids) { std::cout \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(k) \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } // std::cout \u0026lt;\u0026lt; std::endl; Update (Updating the Relationship or Associated Data) Changing the Parent: Simply call setParent(registry, child, newParent);. Updating the Child\u0026rsquo;s Own Data: Directly get the child\u0026rsquo;s other components and modify them. void updateChildLabel(entt::registry\u0026amp; registry, entt::entity child, const std::string\u0026amp; newLabel) { if (registry.valid(child)) { // Use patch or try_get/get to modify NodeLabel if (auto* label = registry.try_get\u0026lt;NodeLabel\u0026gt;(child)) { label-\u0026gt;label = newLabel; std::cout \u0026lt;\u0026lt; \u0026#34;Updated label for Child \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(child) \u0026lt;\u0026lt; \u0026#34; to: \u0026#34; \u0026lt;\u0026lt; newLabel \u0026lt;\u0026lt; std::endl; } else { std::cout \u0026lt;\u0026lt; \u0026#34;Child \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(child) \u0026lt;\u0026lt; \u0026#34; has no NodeLabel to update.\u0026#34; \u0026lt;\u0026lt; std::endl; } } } // Example Usage: // updateChildLabel(registry, child1, \u0026#34;Child A Modified\u0026#34;); Delete (Deleting the Relationship) To sever the parent-child relationship for a specific child, simply remove its ParentComponent.\nvoid removeChildRelationship(entt::registry\u0026amp; registry, entt::entity child) { if (registry.valid(child)) { // Removing the ParentComponent breaks the link // remove() is safe even if the component doesn\u0026#39;t exist registry.remove\u0026lt;ParentComponent\u0026gt;(child); std::cout \u0026lt;\u0026lt; \u0026#34;Removed parent relationship from Child \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(child) \u0026lt;\u0026lt; \u0026#34;.\u0026#34; \u0026lt;\u0026lt; std::endl; } } // Example Usage: // removeChildRelationship(registry, child1); // assert(getParent(registry, child1) == entt::null); // Check successful removal Again, this only deletes the relationship, not the child entity itself.\nImplementing N:N Relationships (e.g., Student \u0026lt;-\u0026gt; Course) Many-to-many relationships, like students enrolling in courses – a student can take multiple courses, and a course can have multiple students.\nStrategy Selection Bidirectional Lists: Add a CoursesAttended component (containing std::vector\u0026lt;entt::entity\u0026gt; of course IDs) to student entities, and a StudentsEnrolled component (containing std::vector\u0026lt;entt::entity\u0026gt; of student IDs) to course entities. Relationship Entity: Create a separate \u0026ldquo;Enrollment\u0026rdquo; entity for each student-course link. This entity would contain entt::entity IDs pointing to the student and the course, and potentially data specific to the relationship itself (like a Grade component). Which is better?\nBidirectional Lists: Relatively straightforward to implement. Finding all courses for a student or all students for a course is convenient (access respective lists). However, requires maintaining synchronization between two lists; adding/deleting links modifies components on both entities. If relationships are very dense, the lists can become large. Relationship Entity: Closer to a relational database\u0026rsquo;s join table. Excellent when the relationship itself needs to carry data (e.g., grades). Querying specific relationship details (like a student\u0026rsquo;s grade in a specific course) is easy. However, finding all courses for a student (or all students for a course) requires iterating over all \u0026quot; Enrollment\u0026quot; entities, which might be slower than direct list access (unless optimized with views/indices). Can generate many small entities. For scenarios where the relationship itself doesn\u0026rsquo;t carry data, and the primary query pattern is \u0026ldquo;given one side, find all entities on the other side,\u0026rdquo; the Bidirectional Lists strategy is often simpler and more intuitive. We\u0026rsquo;ll use this approach.\n#include \u0026lt;vector\u0026gt; #include \u0026lt;algorithm\u0026gt; // For std::find, std::remove // Component on a student containing a list of course IDs they attend struct CoursesAttended { std::vector\u0026lt;entt::entity\u0026gt; courseEntities; }; // Component on a course containing a list of student IDs enrolled struct StudentsEnrolled { std::vector\u0026lt;entt::entity\u0026gt; studentEntities; }; // Auxiliary data components struct StudentInfo { std::string name; }; struct CourseInfo { std::string title; }; Create (Establishing the Relationship / Student Enrollment) This requires adding the other entity\u0026rsquo;s ID to the component list on both the student and the course. Here, we must be mindful of the debugging issue encountered previously. Directly using registry.patch and modifying the vector within its lambda could potentially lead to internal state inconsistencies in EnTT, especially when the component is being created for the first time.\nA more robust approach is to use registry.get_or_emplace to ensure the component exists, and then modify its vector.\nvoid enrollStudent(entt::registry\u0026amp; registry, entt::entity student, entt::entity course) { assert(registry.valid(student) \u0026amp;\u0026amp; \u0026#34;Invalid student entity\u0026#34;); assert(registry.valid(course) \u0026amp;\u0026amp; \u0026#34;Invalid course entity\u0026#34;); // --- Use get_or_emplace to avoid potential issues with patch --- // 1. Add course ID to the student\u0026#39;s list // Get or create the student\u0026#39;s course list component auto\u0026amp; courses_attended = registry.get_or_emplace\u0026lt;CoursesAttended\u0026gt;(student); // Check if already enrolled to prevent duplicates auto\u0026amp; student_courses_vec = courses_attended.courseEntities; if (std::find(student_courses_vec.begin(), student_courses_vec.end(), course) == student_courses_vec.end()) { student_courses_vec.push_back(course); // Add course ID } // 2. Add student ID to the course\u0026#39;s list // Get or create the course\u0026#39;s student list component auto\u0026amp; students_enrolled = registry.get_or_emplace\u0026lt;StudentsEnrolled\u0026gt;(course); // Check if already enrolled to prevent duplicates auto\u0026amp; course_students_vec = students_enrolled.studentEntities; if (std::find(course_students_vec.begin(), course_students_vec.end(), student) == course_students_vec.end()) { course_students_vec.push_back(student); // Add student ID } // --- End safe update --- std::cout \u0026lt;\u0026lt; \u0026#34;Enrolled Student \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(student) \u0026lt;\u0026lt; \u0026#34; in Course \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(course) \u0026lt;\u0026lt; std::endl; } // Example Usage: // entt::entity studentA = registry.create(); // registry.emplace\u0026lt;StudentInfo\u0026gt;(studentA, \u0026#34;Bob\u0026#34;); // entt::entity courseMath = registry.create(); // registry.emplace\u0026lt;CourseInfo\u0026gt;(courseMath, \u0026#34;Math 101\u0026#34;); // enrollStudent(registry, studentA, courseMath); Read (Reading the Relationship) Finding All Courses for a Student: std::vector\u0026lt;entt::entity\u0026gt; getCoursesForStudent(entt::registry\u0026amp; registry, entt::entity student) { if (!registry.valid(student)) return {}; auto* courses_comp = registry.try_get\u0026lt;CoursesAttended\u0026gt;(student); if (courses_comp) { std::vector\u0026lt;entt::entity\u0026gt; valid_courses; // !! Important: Filter out course entities that might have been destroyed !! for (entt::entity course_entity : courses_comp-\u0026gt;courseEntities) { if (registry.valid(course_entity)) { valid_courses.push_back(course_entity); } else { // Optional: Log a warning here indicating a dangling reference was found // std::cerr \u0026lt;\u0026lt; \u0026#34;Warning: Student \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(student) // \u0026lt;\u0026lt; \u0026#34; course list contains invalid course ID \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(course_entity) \u0026lt;\u0026lt; std::endl; } } // Optional: If invalid IDs were found, consider updating the original component // to remove them. This modifies state, depends if your read function allows side effects. // if(valid_courses.size() != courses_comp-\u0026gt;courseEntities.size()) { // registry.patch\u0026lt;CoursesAttended\u0026gt;(student, [\u0026amp;](auto\u0026amp; c){ c.courseEntities = valid_courses; }); // } return valid_courses; } return {}; // Student doesn\u0026#39;t have a CoursesAttended component } Finding All Students for a Course: std::vector\u0026lt;entt::entity\u0026gt; getStudentsForCourse(entt::registry\u0026amp; registry, entt::entity course) { if (!registry.valid(course)) return {}; auto* students_comp = registry.try_get\u0026lt;StudentsEnrolled\u0026gt;(course); if (students_comp) { std::vector\u0026lt;entt::entity\u0026gt; valid_students; // !! Important: Filter out student entities that might have been destroyed !! for (entt::entity student_entity : students_comp-\u0026gt;studentEntities) { if (registry.valid(student_entity)) { valid_students.push_back(student_entity); } else { // Optional: Log warning } } // Optional: Update original component return valid_students; } return {}; // Course doesn\u0026#39;t have a StudentsEnrolled component } // Example Usage: // std::vector\u0026lt;entt::entity\u0026gt; bobs_courses = getCoursesForStudent(registry, studentA); // std::vector\u0026lt;entt::entity\u0026gt; math_students = getStudentsForCourse(registry, courseMath); Emphasis Again: Filtering out invalid entities using registry.valid() before returning the ID list is crucial!\nUpdate (Updating Associated Data) Updating the student\u0026rsquo;s or course\u0026rsquo;s own data is straightforward; just get the respective entity\u0026rsquo;s component and modify it.\nvoid updateStudentName(entt::registry\u0026amp; registry, entt::entity student, const std::string\u0026amp; newName) { if(registry.valid(student)) { if(auto* info = registry.try_get\u0026lt;StudentInfo\u0026gt;(student)) { info-\u0026gt;name = newName; std::cout \u0026lt;\u0026lt; \u0026#34;Updated name for Student \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(student) \u0026lt;\u0026lt; \u0026#34; to: \u0026#34; \u0026lt;\u0026lt; newName \u0026lt;\u0026lt; std::endl; } } } // Example Usage: // updateStudentName(registry, studentA, \u0026#34;Bobby\u0026#34;); Delete (Deleting the Relationship / Student Withdraws) This also requires updating the components on both entities, removing the other\u0026rsquo;s ID from their respective vectors.\nvoid withdrawStudent(entt::registry\u0026amp; registry, entt::entity student, entt::entity course) { if (!registry.valid(student) || !registry.valid(course)) return; // Check validity of both bool changed = false; // Flag if any actual removal happened // 1. Remove course ID from the student\u0026#39;s course list if (auto* courses = registry.try_get\u0026lt;CoursesAttended\u0026gt;(student)) { auto\u0026amp; vec = courses-\u0026gt;courseEntities; // Use the C++ standard library remove-erase idiom auto original_size = vec.size(); vec.erase(std::remove(vec.begin(), vec.end(), course), vec.end()); if (vec.size() != original_size) { changed = true; } } // 2. Remove student ID from the course\u0026#39;s student list if (auto* students = registry.try_get\u0026lt;StudentsEnrolled\u0026gt;(course)) { auto\u0026amp; vec = students-\u0026gt;studentEntities; auto original_size = vec.size(); vec.erase(std::remove(vec.begin(), vec.end(), student), vec.end()); if (vec.size() != original_size) { changed = true; } } if(changed) { std::cout \u0026lt;\u0026lt; \u0026#34;Withdrew Student \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(student) \u0026lt;\u0026lt; \u0026#34; from Course \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(course) \u0026lt;\u0026lt; \u0026#34;.\u0026#34; \u0026lt;\u0026lt; std::endl; } else { std::cout \u0026lt;\u0026lt; \u0026#34;Student \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(student) \u0026lt;\u0026lt; \u0026#34; was not enrolled in Course \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(course) \u0026lt;\u0026lt; \u0026#34; or components missing; withdrawal failed.\u0026#34; \u0026lt;\u0026lt; std::endl; } } // Example Usage: // entt::entity coursePhys = registry.create(); registry.emplace\u0026lt;CourseInfo\u0026gt;(coursePhys, \u0026#34;Physics 101\u0026#34;); // enrollStudent(registry, studentA, coursePhys); // Ensure A is enrolled in Physics first // withdrawStudent(registry, studentA, coursePhys); // Then withdraw // assert(/* Check if A\u0026#39;s course list and Physics\u0026#39; student list are updated */); Important Considerations and Nuances Handling Dangling References This is the most common pitfall when using ID-based relationship representation. When you destroy an entity (like a course), EnTT does not automatically find all CoursesAttended components referencing that course ID and remove the ID from them. These references become \u0026ldquo;dangling.\u0026rdquo;\nOur primary defense mechanism is to always check the validity of a stored entity ID using registry.valid() before using it. This was demonstrated in our Read function examples above (e.g., filtering invalid course IDs in getCoursesForStudent).\nIf you require more automated cleanup, consider using EnTT\u0026rsquo;s signal system. You can listen for the on_destroy signal for specific entity types (e.g., Course). When a course is destroyed, the triggered callback receives the destroyed course\u0026rsquo;s ID. You can then write logic to iterate through all students, check their CoursesAttended components, and remove the just-destroyed course ID. This approach is more complex but guarantees relationship data consistency. For many cases, checking valid() on read is sufficient.\nPerformance Considerations 1:1 and 1:N (Child-to-Parent): Queries are very fast, typically O(1) component access. 1:N (Parent-to-Children): Requires using a view to iterate over all potential child-type entities, then comparing the parent ID. EnTT\u0026rsquo;s view performance is excellent and generally fast enough. If parent-to-children lookups are extremely frequent and become a bottleneck, consider caching results or using the parent-centric strategy (but weigh its drawbacks). N:N (Bidirectional Lists): Querying all related entities for one side requires accessing a vector. Traversing large vectors has a cost. Adding/removing links requires modifying two vectors, and std::vector::erase(std::remove(...)) itself isn\u0026rsquo;t an O(1) operation. If relationships are extremely dense (like a social network\u0026rsquo;s friend graph) or if the relationship itself needs data, the \u0026ldquo;Relationship Entity\u0026rdquo; strategy might be superior. Alternatives Revisited For 1:N, the parent-stores-child-list approach can be an option if retrieving all children from the parent is frequent and the number of children is manageable. For N:N, the relationship entity approach offers better scalability when relationships have attributes (like grades) or the number of relationships is massive. The choice of strategy depends on your specific application scenario, query patterns, and performance needs. There\u0026rsquo;s no single \u0026ldquo;best\u0026rdquo; solution.\nComplexity It\u0026rsquo;s evident that manually managing relationships in ECS is somewhat more complex than relying on database foreign key constraints. You are responsible for maintaining relationship integrity, especially during updates and deletions, ensuring information is synchronized on both ends, and handling the dangling reference problem gracefully.\nConclusion We\u0026rsquo;ve journeyed together through implementing 1:1, 1:N, and N:N entity relationships in the powerful and flexible EnTT ECS library using a component-based approach. The core idea revolves around using components to store the entt::entity identifiers of related entities and utilizing registry operations (create, destroy, try_get, get_or_emplace, remove, view, etc.) to achieve relationship creation, querying, updates, and deletion.\nWe also delved into the nature of entt::entity itself, understanding how its embedded index and version information aids in safely handling entity handles. Furthermore, we stressed the critical importance of checking registry.valid() before using stored entity IDs to prevent issues arising from dangling references. For N:N relationship implementation, drawing from previous debugging experience, we opted for get_or_emplace over patch to enhance stability during component creation and modification.\nWhile EnTT doesn\u0026rsquo;t provide built-in relationship primitives, it equips us with sufficient tools and flexibility to design efficient relationship management solutions tailored to our specific needs, all while adhering to the ECS philosophy. Hopefully, this comprehensive guide helps you better understand how to handle entity associations within EnTT, laying a solid foundation for building complex and vibrant virtual worlds.\nRemember, practice is the best teacher. Try applying these patterns in your own projects, adapting and optimizing them based on your findings. Happy exploring in the world of EnTT!\n","permalink":"https://blog.tategotoazarasi.me/en/posts/weaving-the-web-managing-entity-relationships-in-entt/","summary":"Manage 1:1, 1:N, \u0026amp; N:N entity relationships in C++ EnTT ECS using component-based CRUD strategies and best practices.","title":"Weaving the Web: Managing Entity Relationships in EnTT"},{"content":"I was tinkering with Breezy Weather, the open-source weather app, the other day. It\u0026rsquo;s got a decent collection of widgets, but I felt like something was missing – one of those \u0026ldquo;kitchen sink\u0026rdquo; widgets that just throws everything you need onto your home screen. You know, the current time, what the weather\u0026rsquo;s doing right now, what it\u0026rsquo;s gonna do in the next few hours, AND the outlook for the next few days. I got tired of either opening the app or juggling multiple widgets to get the full picture. Naturally, the itch to code kicked in, and I decided to build it myself. Let\u0026rsquo;s call it the ClockDayHourWeekWidget.\nThis blog post is basically my development log. I\u0026rsquo;m jotting down the thought process, the steps I took, and a few bumps I hit along the way. It\u0026rsquo;s mainly for my future self, but hopefully, it might be useful for anyone else interested in Android widget development or maybe even contributing to Breezy Weather. The style\u0026rsquo;s going to be pretty casual – think of it as dev notes – but I\u0026rsquo;ll make sure to include the key technical bits and enough code snippets so you can understand what\u0026rsquo;s going on and potentially replicate it.\nThe Goal:\nCreate a new Android App Widget that displays:\nCurrent Time: Just like your standard clock. Current Weather: Icon, location name, current temperature. Hourly Forecast: A glimpse of the weather (icon, time, temp) for the next few hours (e.g., the next 5). Daily Forecast: The usual suspects (icon, day of the week, high/low temp) for the next few days (e.g., the next 5). Configurability: Following the Breezy Weather pattern, allow users to customize background style, transparency, text color, text size, clock font, etc., via a configuration screen. Alright, goal set. Let\u0026rsquo;s dive in!\nThe Big Picture: Standing on the Shoulders of Giants Thankfully, Breezy Weather has a pretty well-defined structure, especially for adding new widgets. Looking at existing files like WidgetClockDayWeekProvider.kt and HourlyTrendWidgetIMP.kt, the pattern becomes clear. To add a new widget, you generally need these pieces:\nAppWidgetProvider (e.g., XxxWidgetProvider.kt): This is the widget\u0026rsquo;s entry point. It extends AppWidgetProvider and receives system broadcasts, most importantly onUpdate. Its main job is to kick off the real work of loading data and updating the view. Widget Implementation (e.g., XxxWidgetIMP.kt): Often an object (Kotlin singleton) inheriting from AbstractRemoteViewsPresenter. This is where the magic happens: fetching data, loading user configuration, building the RemoteViews object (which defines the widget\u0026rsquo;s UI), and handling click intents. Configuration Activity (e.g., XxxWidgetConfigActivity.kt): An Activity extending AbstractWidgetConfigActivity. It pops up when the user adds the widget, allowing them to customize its appearance ( background, colors, etc.). It also needs to show a live preview of the settings. XML Layout Files (widget_xxx.xml, widget_xxx_card.xml): These define the static structure of the widget\u0026rsquo;s UI. Typically, there\u0026rsquo;s a version without a background card and one with it. Widget Definition XML (xml/widget_xxx.xml, xml/v28/widget_xxx.xml): This metadata file tells the Android system about the widget – its minimum size, preview image, the configuration activity to launch, update frequency ( usually 0 here, as updates are triggered programmatically), etc. The v28 version usually adds widgetFeatures=\u0026quot;reconfigurable\u0026quot;. Resource Updates: You\u0026rsquo;ll need to touch several resource files: dimens.xml: Possibly define new dimensions if needed. keys.xml: Add a unique SharedPreferences key for storing the widget\u0026rsquo;s settings. strings.xml: Add the user-visible name for the widget. AndroidManifest.xml: Register the new Provider and Config Activity. Widgets.kt: Add unique request codes for PendingIntents. Basically, follow this recipe, create or modify each part, and voilà – a new widget is born. For our ClockDayHourWeekWidget, the existing ClockDayWeekWidget is a great starting point. It already handles the clock, date, current weather, and daily forecast. Our main task is to surgically insert the \u0026ldquo;hourly forecast\u0026rdquo; section into it.\nGetting Our Hands Dirty: Creating the Components Let\u0026rsquo;s build this thing piece by piece.\nWidget Provider (ClockDayHourWeekWidgetProvider.kt) This one\u0026rsquo;s relatively straightforward. We can copy WidgetClockDayWeekProvider.kt and make a few tweaks:\nRename the class to ClockDayHourWeekWidgetProvider. Inside the onUpdate method, make sure it calls the updateWidgetView method of our new implementation class, ClockDayHourWeekWidgetIMP. Key Point: When calling weatherRepository.getWeatherByLocationId, we absolutely must set both withDaily = true and withHourly = true. Our widget needs both sets of forecast data. // src/main/java/org/breezyweather/background/receiver/widget/ClockDayHourWeekWidgetProvider.kt package org.breezyweather.background.receiver.widget // ... other imports ... import org.breezyweather.remoteviews.presenters.ClockDayHourWeekWidgetIMP // Reference the new IMP import javax.inject.Inject @AndroidEntryPoint // Hilt annotation is crucial class ClockDayHourWeekWidgetProvider : AppWidgetProvider() { @Inject lateinit var locationRepository: LocationRepository @Inject lateinit var weatherRepository: WeatherRepository @OptIn(DelicateCoroutinesApi::class) // Note: Using GlobalScope here, a common but not ideal practice in Providers override fun onUpdate( context: Context, appWidgetManager: AppWidgetManager, appWidgetIds: IntArray, ) { super.onUpdate(context, appWidgetManager, appWidgetIds) // Check if any widget of this type is still in use if (ClockDayHourWeekWidgetIMP.isInUse(context)) { // Launch a coroutine on the IO dispatcher to fetch data GlobalScope.launch(Dispatchers.IO) { // Get the first location (without parameters) val location = locationRepository.getFirstLocation(withParameters = false) // Call the IMP to update the view ClockDayHourWeekWidgetIMP.updateWidgetView( context, location?.copy( // Use copy to create a new object and fill in the weather weather = weatherRepository.getWeatherByLocationId( location.formattedId, withDaily = true, // Needed for daily data (isDaylight, daily forecast) withHourly = true, // !! Must be true, we need hourly data !! withMinutely = false, withAlerts = false ) ) ) } } } } A quick note on GlobalScope.launch(Dispatchers.IO): In the onUpdate method of an AppWidgetProvider, which runs on the main thread and has a short lifespan, this is a fairly common way to handle potentially long-running operations like network requests or database access. While GlobalScope isn\u0026rsquo;t generally recommended (its coroutines are tied to the application\u0026rsquo;s lifecycle and harder to manage), it\u0026rsquo;s a simpler solution in this specific context. More robust approaches might involve goAsync() paired with a Hilt-injected CoroutineScope or even WorkManager, but sticking to the existing pattern keeps things simpler here.\nWidget Implementation (ClockDayHourWeekWidgetIMP.kt) This is the beast. Most of the UI construction logic lives here. Again, copying ClockDayWeekWidgetIMP.kt gives us a solid foundation to build upon.\nIts Main Responsibilities:\nupdateWidgetView: Called by the Provider. Gets the config, calls getRemoteViews to build the UI, and finally updates the widget via AppWidgetManager. getRemoteViews: The core method. Takes Context, Location data, and various config parameters, returning a fully constructed RemoteViews object. isInUse: Checks if any instances of this specific widget type exist. setOnClickPendingIntent: Sets up the actions (like opening the app or calendar) when users click on different parts of the widget. Breaking Down getRemoteViews:\nGet Config \u0026amp; Colors: Use getWidgetConfig to load saved settings and initialize WidgetColor to handle color logic based on config and day/night status.\nChoose Layout: Based on WidgetColor\u0026rsquo;s judgment (whether to show a card background), load either R.layout.widget_clock_day_hour_week or R.layout.widget_clock_day_hour_week_card.\nPrepare Data: Extract weather data from the Location object, get instances of SettingsManager, ResourcesProviderFactory, etc.\nPopulate Sections (using views.setXXX methods):\nClock: Set the TextClock timezone (setTimeZone). Control the visibility (setViewVisibility) of the different font-styled TextClock views based on the clockFont config. Date: Set the TextClock timezone and date format (setCharSequence with format12Hour/format24Hour). Current Weather: Icon: Get the icon URI using ResourceHelper.getWidgetNotificationIconUri and set it with setImageViewUri. Handle potential nulls (weather.current or weatherCode) by hiding the view ( setViewVisibility(View.INVISIBLE)). Alternate Calendar: Set the TextView text based on CalendarHelper settings and the hideAlternateCalendar config. Place \u0026amp; Current Temp: Concatenate the strings and set the text for the corresponding TextView. Hourly Forecast (The New Bit): This is the core addition. We need the LinearLayout container designated for the hourly forecast in our layout. Define an array of IDs to easily access the time TextView, temperature TextView, and weather ImageView for each hourly item. Get the weather.nextHourlyForecast list, limiting it to a maximum number (e.g., MAX_HOURLY_ITEMS = 5). Loop Through Data: Iterate min(MAX_HOURLY_ITEMS, weather.nextHourlyForecast.size) times. Get the HourlyForecast object for the current hour. Set the time TextView\u0026rsquo;s text (using hourly.date.getHour(location, context)). Set the temperature TextView\u0026rsquo;s text (using temperatureUnit.getShortValueText), handling potential nulls. Set the weather ImageView\u0026rsquo;s icon (using ResourceHelper.getWidgetNotificationIconUri), again handling potential nulls for weatherCode and using hourly.isDaylight to pick the correct day/night icon. Control Visibility: Ensure this forecast item is visible (setVisibility(View.VISIBLE)). Handle Excess Views: For any placeholder views in the layout beyond the available data (e.g., layout has 5 slots, API gives 3 hours), hide them (setVisibility(View.GONE)). It\u0026rsquo;s best to hide the entire parent LinearLayout or RelativeLayout for that item. Container Visibility: If there\u0026rsquo;s no hourly data at all (hourlyItemCount == 0), hide the entire hourly forecast container LinearLayout (widget_clock_day_hour_week_hourly_container). // Inside ClockDayHourWeekWidgetIMP.kt -\u0026gt; getRemoteViews() (Hourly Forecast Snippet) // --- Hourly Forecast --- val hourlyIds = arrayOf( // ... (Define 2D array of TextView and ImageView IDs) ... arrayOf(R.id.widget_clock_day_hour_week_hour_time_1, R.id.widget_clock_day_hour_week_hour_temp_1, R.id.widget_clock_day_hour_week_hour_icon_1), // ... other hours ... ) val hourlyItemCount = min(MAX_HOURLY_ITEMS, weather.nextHourlyForecast.size) hourlyIds.forEachIndexed { i, hourlyId -\u0026gt; if (i \u0026lt; hourlyItemCount) { val hourly = weather.nextHourlyForecast[i] views.setTextViewText(hourlyId[0], hourly.date.getHour(location, context)) // Set time views.setTextViewText( hourlyId[1], // Set temperature hourly.temperature?.temperature?.let { temperatureUnit.getShortValueText(context, it) } ?: \u0026#34;...\u0026#34; ) hourly.weatherCode?.let { // Set icon views.setViewVisibility(hourlyId[2], View.VISIBLE) views.setImageViewUri( hourlyId[2], ResourceHelper.getWidgetNotificationIconUri( provider, it, hourly.isDaylight ?: dayTime, minimalIcon, color.minimalIconColor ) ) } ?: views.setViewVisibility(hourlyId[2], View.INVISIBLE) // Make sure the parent item container is visible (assuming parent ID is widget_clock_day_hour_week_hour_item_x) val parentId = context.resources.getIdentifier(\u0026#34;widget_clock_day_hour_week_hour_item_${i + 1}\u0026#34;, \u0026#34;id\u0026#34;, context.packageName) if (parentId != 0) views.setInt(parentId, \u0026#34;setVisibility\u0026#34;, View.VISIBLE) } else { // Hide unused items (preferably the parent container) val parentId = context.resources.getIdentifier(\u0026#34;widget_clock_day_hour_week_hour_item_${i + 1}\u0026#34;, \u0026#34;id\u0026#34;, context.packageName) if (parentId != 0) views.setInt(parentId, \u0026#34;setVisibility\u0026#34;, View.GONE) // Fallback: If parent ID isn\u0026#39;t found, hide individual elements // else { views.setInt(hourlyId[0], \u0026#34;setVisibility\u0026#34;, View.GONE); ... } } } // If no hourly data, hide the entire hourly section views.setViewVisibility( R.id.widget_clock_day_hour_week_hourly_container, if (hourlyItemCount \u0026gt; 0) View.VISIBLE else View.GONE ) Daily Forecast: This logic is very similar to the original ClockDayWeekWidgetIMP, just make sure to use the new IDs from our modified layout. It also needs the same treatment for handling insufficient data (hiding extra views) and hiding the entire daily container if no data exists. Apply Styles: Text Color: If a specific text color is configured (textColor != Color.TRANSPARENT), loop through all relevant TextViews (including the newly added hourly ones!) and use setTextColor. Text Size: If a non-100% size is set (textSize != 100), calculate the scale, get base dimensions ( R.dimen.xxx), multiply by scale, and then loop through all relevant TextViews, setting the size with setTextViewTextSize(TypedValue.COMPLEX_UNIT_PX, size). Remember the new hourly TextViews! You might need different base dimensions for different parts (clock vs. content vs. hourly time vs. daily day name). Clock Font: Use a when statement on clockFont to set the visibility of the appropriate TextClock container. Card Background: If color.showCard is true, set the background drawable (setImageViewResource) and its alpha (setInt(id, \u0026quot;setImageAlpha\u0026quot;, alpha)). Set Click Actions: Call the setOnClickPendingIntent method, passing the context, views, and location.\nsetOnClickPendingIntent:\nThis method wires up the clickable elements (weather icon, date, clock, daily icons) to perform actions. It creates PendingIntents and binds them using views.setOnClickPendingIntent(viewId, pendingIntent).\nThe crucial part is giving each PendingIntent a unique Request Code. We define these constants centrally in Widgets.kt. Breezy Weather provides helpers for common intents: getWeatherPendingIntent: Opens the main app screen. getDailyForecastPendingIntent: Opens the app scrolled to the specific forecast day. getAlarmPendingIntent: Tries to open the system alarm/clock app. getCalendarPendingIntent: Tries to open the system calendar app. We need to define a new block of non-conflicting request codes in Widgets.kt for ClockDayHourWeekWidget (e.g., starting with 14x). // Inside ClockDayHourWeekWidgetIMP.kt private fun setOnClickPendingIntent(context: Context, views: RemoteViews, location: Location) { // Click main weather area -\u0026gt; Open App views.setOnClickPendingIntent( R.id.widget_clock_day_hour_week_weather, // ID of the main content container getWeatherPendingIntent(context, location, Widgets.CLOCK_DAY_HOUR_WEEK_PENDING_INTENT_CODE_WEATHER) // Use new code ) // Click daily forecast icon -\u0026gt; Open App to that day val todayIndex = location.weather?.todayIndex ?: 0 views.setOnClickPendingIntent( R.id.widget_clock_day_hour_week_day_icon_1, // Day 1 icon ID getDailyForecastPendingIntent(context, location, todayIndex, Widgets.CLOCK_DAY_HOUR_WEEK_PENDING_INTENT_CODE_DAILY_FORECAST_1) // New code ) // ... Set similar PendingIntents for day_icon_2 to day_icon_5 ... // Click clock -\u0026gt; Open Alarm/Clock App views.setOnClickPendingIntent( R.id.widget_clock_day_hour_week_clock_light, // Light font clock ID getAlarmPendingIntent(context, Widgets.CLOCK_DAY_HOUR_WEEK_PENDING_INTENT_CODE_CLOCK_LIGHT) // New code ) // ... Set similar PendingIntents for normal and black font clocks ... // Click date -\u0026gt; Open Calendar App views.setOnClickPendingIntent( R.id.widget_clock_day_hour_week_title, // Date TextClock ID getCalendarPendingIntent(context, Widgets.CLOCK_DAY_HOUR_WEEK_PENDING_INTENT_CODE_CALENDAR) // New code ) // Clicks for hourly forecast items could be added here if needed, // but the current design doesn\u0026#39;t seem to require them. /* views.setOnClickPendingIntent( R.id.widget_clock_day_hour_week_hour_icon_1, // getHourlyForecastPendingIntent(...) // Would need a helper and codes ) */ } Configuration Activity (ClockDayHourWeekWidgetConfigActivity.kt) This activity lets users tweak the widget when they first add it. Copying ClockDayWeekWidgetConfigActivity.kt is the path of least resistance.\nModifications Needed:\nRename the class to ClockDayHourWeekWidgetConfigActivity. initLocations(): Ensure withHourly = true when fetching weather data, just like in the Provider. Even if the preview doesn\u0026rsquo;t show hourly details, the underlying data might be needed for other logic (like determining isDaylight accurately for icons if the current condition isn\u0026rsquo;t available). // Inside ClockDayHourWeekWidgetConfigActivity.kt override suspend fun initLocations() { val location = locationRepository.getFirstLocation(withParameters = false) locationNow = location?.copy( weather = weatherRepository.getWeatherByLocationId( location.formattedId, withDaily = true, withHourly = true, // Ensure hourly data is fetched withMinutely = false, withAlerts = false ) ) } initData(): Set default configuration values, like the initial clock font (clockFontValueNow). The base class AbstractWidgetConfigActivity handles defaults for card style, color, alpha, etc. initView(): Control which configuration options are visible on the screen. For this widget, options for card style, alpha, text color, text size, clock font, and hiding the alternate calendar should all be visible. updateWidgetView(): When the user changes a setting in the config UI, this method calls ClockDayHourWeekWidgetIMP.updateWidgetView to immediately update the widget instance on the home screen (live preview effect). remoteViews (getter): This property provides the RemoteViews for the preview area within the config screen. It must call ClockDayHourWeekWidgetIMP.getRemoteViews, passing the current selections from the config UI ( cardStyleValueNow, cardAlpha, textColorValueNow, etc.). configStoreName (getter): Returns the unique SharedPreferences key used to store this widget\u0026rsquo;s settings. Must be unique! We\u0026rsquo;ll define this key in keys.xml. // Inside ClockDayHourWeekWidgetConfigActivity.kt override val configStoreName: String get() { // Return the new key we define in keys.xml return getString(R.string.sp_widget_clock_day_hour_week_setting) } XML Layout Files We need two layout files: layout/widget_clock_day_hour_week.xml (no background) and layout/widget_clock_day_hour_week_card.xml (with background).\nCopy widget_clock_day_week.xml and widget_clock_day_week_card.xml and then modify them.\nKey Modifications:\nRename Root Layout and ALL View IDs: To prevent clashes, systematically rename all IDs. A good practice is to replace widget_clock_day_week_ with widget_clock_day_hour_week_. Add Hourly Forecast Section: Between the \u0026ldquo;Date/Place/Current Temp\u0026rdquo; section and the \u0026ldquo;Daily Forecast\u0026rdquo; section, insert a new LinearLayout. Give it the ID android:id=\u0026quot;@+id/widget_clock_day_hour_week_hourly_container\u0026quot;. Set its orientation=\u0026quot;horizontal\u0026quot;. Inside it, place 5 child LinearLayouts (or RelativeLayouts), each representing one hour\u0026rsquo;s forecast. Set each hourly item\u0026rsquo;s LinearLayout to orientation=\u0026quot;vertical\u0026quot;, layout_width=\u0026quot;0dp\u0026quot;, layout_height=\u0026quot;wrap_content\u0026quot;, layout_weight=\u0026quot;1\u0026quot;, gravity=\u0026quot;center_horizontal\u0026quot;. Give them unique IDs like widget_clock_day_hour_week_hour_item_1 through item_5. Inside each hourly item LinearLayout, place the three necessary views: A TextView for the time (widget_clock_day_hour_week_hour_time_x). An ImageView for the weather icon (widget_clock_day_hour_week_hour_icon_x). A TextView for the temperature (widget_clock_day_hour_week_hour_temp_x). Use dimensions from dimens.xml, like @dimen/widget_time_text_size for the time, @dimen/widget_content_text_size for the temp, and @dimen/widget_little_weather_icon_size for the icon. Modify Daily Forecast IDs: Rename the original daily forecast IDs (like widget_clock_day_week_week_x, _temp_x, _icon_x) to widget_clock_day_hour_week_day_week_x, _day_temp_x, _day_icon_x. Also, give the parent LinearLayout container for the daily forecast an ID, like widget_clock_day_hour_week_daily_container. widget_clock_day_hour_week_card.xml: This file is essentially a copy of widget_clock_day_hour_week.xml, but with an ImageView added as the first child inside the root RelativeLayout. This ImageView will display the card background; give it the ID widget_clock_day_hour_week_card. \u0026lt;!-- layout/widget_clock_day_hour_week.xml (Snippet showing new hourly structure) --\u0026gt; \u0026lt;RelativeLayout ...\u0026gt; \u0026lt;LinearLayout android:id=\u0026#34;@+id/widget_clock_day_hour_week_weather\u0026#34; ...\u0026gt; \u0026lt;!-- ... (Clock, Date, Current Weather sections - IDs modified) ... --\u0026gt; \u0026lt;!-- Hourly Forecast --\u0026gt; \u0026lt;LinearLayout android:id=\u0026#34;@+id/widget_clock_day_hour_week_hourly_container\u0026#34; android:orientation=\u0026#34;horizontal\u0026#34; android:layout_width=\u0026#34;match_parent\u0026#34; android:layout_height=\u0026#34;wrap_content\u0026#34; android:layout_marginTop=\u0026#34;@dimen/little_margin\u0026#34; android:layout_marginBottom=\u0026#34;@dimen/little_margin\u0026#34; android:baselineAligned=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;!-- Hour 1 --\u0026gt; \u0026lt;LinearLayout android:id=\u0026#34;@+id/widget_clock_day_hour_week_hour_item_1\u0026#34; android:orientation=\u0026#34;vertical\u0026#34; android:layout_width=\u0026#34;0dp\u0026#34; android:layout_height=\u0026#34;wrap_content\u0026#34; android:layout_weight=\u0026#34;1\u0026#34; android:gravity=\u0026#34;center_horizontal\u0026#34;\u0026gt; \u0026lt;TextView android:id=\u0026#34;@+id/widget_clock_day_hour_week_hour_time_1\u0026#34; android:textSize=\u0026#34;@dimen/widget_time_text_size\u0026#34; ... /\u0026gt; \u0026lt;ImageView android:id=\u0026#34;@+id/widget_clock_day_hour_week_hour_icon_1\u0026#34; android:layout_width=\u0026#34;@dimen/widget_little_weather_icon_size\u0026#34; android:layout_height=\u0026#34;@dimen/widget_little_weather_icon_size\u0026#34; ... /\u0026gt; \u0026lt;TextView android:id=\u0026#34;@+id/widget_clock_day_hour_week_hour_temp_1\u0026#34; android:textSize=\u0026#34;@dimen/widget_content_text_size\u0026#34; ... /\u0026gt; \u0026lt;/LinearLayout\u0026gt; \u0026lt;!-- Hour 2 to 5 (Similar structure) --\u0026gt; \u0026lt;!-- ... --\u0026gt; \u0026lt;/LinearLayout\u0026gt; \u0026lt;!-- Daily Forecast --\u0026gt; \u0026lt;LinearLayout android:id=\u0026#34;@+id/widget_clock_day_hour_week_daily_container\u0026#34; android:orientation=\u0026#34;horizontal\u0026#34; ... \u0026gt; \u0026lt;!-- Day 1 --\u0026gt; \u0026lt;LinearLayout android:id=\u0026#34;@+id/widget_clock_day_hour_week_day_item_1\u0026#34; ...\u0026gt; \u0026lt;TextView android:id=\u0026#34;@+id/widget_clock_day_hour_week_day_week_1\u0026#34; ... /\u0026gt; \u0026lt;ImageView android:id=\u0026#34;@+id/widget_clock_day_hour_week_day_icon_1\u0026#34; ... /\u0026gt; \u0026lt;TextView android:id=\u0026#34;@+id/widget_clock_day_hour_week_day_temp_1\u0026#34; ... /\u0026gt; \u0026lt;/LinearLayout\u0026gt; \u0026lt;!-- Day 2 to 5 (Similar structure, IDs modified) --\u0026gt; \u0026lt;!-- ... --\u0026gt; \u0026lt;/LinearLayout\u0026gt; \u0026lt;/LinearLayout\u0026gt; \u0026lt;/RelativeLayout\u0026gt; Widget Definition XML Create widget_clock_day_hour_week.xml in res/xml/ and a corresponding version in res/xml-v28/ (create the directory if it doesn\u0026rsquo;t exist).\nCopy xml/widget_clock_day_week.xml and xml-v28/widget_clock_day_week.xml.\nChanges to Make:\nandroid:minWidth / android:minHeight: Since we added the hourly forecast row, the widget needs more vertical space. Increase minHeight, for example, from @dimen/widget_grid_2 (110dp) to @dimen/widget_grid_3 (180dp). Keep minWidth at @dimen/widget_grid_4 (250dp). android:minResizeHeight: The minimum resize height also needs to increase accordingly, perhaps to @dimen/widget_grid_2. android:initialLayout: Point this to our new layout: @layout/widget_clock_day_hour_week. android:previewImage: Point this to a new preview drawable: @drawable/widget_clock_day_hour_week. Remember, you need to create this image yourself and place it in the drawable folders. android:configure: Point this to our new configuration activity: org.breezyweather.remoteviews.config.ClockDayHourWeekWidgetConfigActivity. v28 Version: Make the same changes, and ensure android:widgetFeatures=\u0026quot;reconfigurable\u0026quot; is present. \u0026lt;!-- res/xml/widget_clock_day_hour_week.xml --\u0026gt; \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;appwidget-provider xmlns:android=\u0026#34;http://schemas.android.com/apk/res/android\u0026#34; android:minWidth=\u0026#34;@dimen/widget_grid_4\u0026#34; android:minHeight=\u0026#34;@dimen/widget_grid_3\u0026#34; \u0026lt;!-- Increased height --\u0026gt; android:minResizeWidth=\u0026#34;@dimen/widget_grid_3\u0026#34; android:minResizeHeight=\u0026#34;@dimen/widget_grid_2\u0026#34; \u0026lt;!-- Increased resize height --\u0026gt; android:updatePeriodMillis=\u0026#34;0\u0026#34; android:initialLayout=\u0026#34;@layout/widget_clock_day_hour_week\u0026#34; \u0026lt;!-- Point to new layout --\u0026gt; android:previewImage=\u0026#34;@drawable/widget_clock_day_hour_week\u0026#34; \u0026lt;!-- Point to new preview --\u0026gt; android:resizeMode=\u0026#34;horizontal|vertical\u0026#34; android:configure=\u0026#34;org.breezyweather.remoteviews.config.ClockDayHourWeekWidgetConfigActivity\u0026#34; \u0026lt;!-- Point to new config activity --\u0026gt; android:widgetCategory=\u0026#34;home_screen|keyguard\u0026#34; /\u0026gt; Stitching It All Together: Resources \u0026amp; Registration The final step is to make sure all the necessary resource definitions and registrations are in place.\ndimens.xml: Double-check the dimensions used in the layout. Existing ones like @dimen/widget_time_text_size ( 10sp), @dimen/widget_content_text_size (14sp), @dimen/widget_little_weather_icon_size (36dp) seem appropriate. If you feel the hourly or daily sections need specific adjustments, define new dimensions here and reference them. For now, reusing existing ones should be fine.\nkeys.xml: Add the new string for the configuration storage key.\n\u0026lt;!-- res/values/keys.xml --\u0026gt; \u0026lt;resources ...\u0026gt; ... \u0026lt;string name=\u0026#34;sp_widget_clock_day_hour_week_setting\u0026#34; translatable=\u0026#34;false\u0026#34;\u0026gt;widget_clock_day_hour_week_setting\u0026lt;/string\u0026gt; ... \u0026lt;/resources\u0026gt; strings.xml: Add the user-visible name for the widget.\n\u0026lt;!-- res/values/strings.xml --\u0026gt; \u0026lt;resources ...\u0026gt; ... \u0026lt;string name=\u0026#34;widget_clock_day_hour_week\u0026#34;\u0026gt;Clock + Day + Hour + Week\u0026lt;/string\u0026gt; \u0026lt;!-- Or your preferred name --\u0026gt; ... \u0026lt;/resources\u0026gt; (Don\u0026rsquo;t forget translations in other values-*/strings.xml files if necessary!)\nAndroidManifest.xml: Inside the \u0026lt;application\u0026gt; tag, register the new Provider (\u0026lt;receiver\u0026gt;) and Config Activity (\u0026lt;activity\u0026gt;). It\u0026rsquo;s good practice to group them with the other widget declarations.\n\u0026lt;!-- AndroidManifest.xml --\u0026gt; \u0026lt;application ...\u0026gt; ... \u0026lt;!-- ClockDayHourWeek Widget Configuration Activity --\u0026gt; \u0026lt;activity android:name=\u0026#34;.remoteviews.config.ClockDayHourWeekWidgetConfigActivity\u0026#34; android:theme=\u0026#34;@style/BreezyWeatherTheme\u0026#34; android:exported=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;intent-filter\u0026gt; \u0026lt;action android:name=\u0026#34;android.appwidget.action.APPWIDGET_CONFIGURE\u0026#34; /\u0026gt; \u0026lt;/intent-filter\u0026gt; \u0026lt;/activity\u0026gt; ... \u0026lt;!-- ClockDayHourWeek Widget Provider --\u0026gt; \u0026lt;receiver android:name=\u0026#34;.background.receiver.widget.ClockDayHourWeekWidgetProvider\u0026#34; android:label=\u0026#34;@string/widget_clock_day_hour_week\u0026#34; \u0026lt;!-- Reference the name from strings.xml --\u0026gt; android:exported=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;meta-data android:name=\u0026#34;android.appwidget.provider\u0026#34; android:resource=\u0026#34;@xml/widget_clock_day_hour_week\u0026#34; /\u0026gt; \u0026lt;!-- Reference the definition xml --\u0026gt; \u0026lt;intent-filter\u0026gt; \u0026lt;action android:name=\u0026#34;android.appwidget.action.APPWIDGET_UPDATE\u0026#34; /\u0026gt; \u0026lt;action android:name=\u0026#34;android.appwidget.action.ACTION_APPWIDGET_DISABLED\u0026#34; /\u0026gt; \u0026lt;/intent-filter\u0026gt; \u0026lt;/receiver\u0026gt; ... \u0026lt;/application\u0026gt; Widgets.kt: Add the new block of PendingIntent Request Code constants. Pick an unused range (like 14x).\n// src/main/java/org/breezyweather/remoteviews/Widgets.kt object Widgets { ... // other constants // clock + day + hour + week. (Using 14x block) const val CLOCK_DAY_HOUR_WEEK_PENDING_INTENT_CODE_WEATHER = 141 const val CLOCK_DAY_HOUR_WEEK_PENDING_INTENT_CODE_DAILY_FORECAST_1 = 1421 const val CLOCK_DAY_HOUR_WEEK_PENDING_INTENT_CODE_DAILY_FORECAST_2 = 1422 const val CLOCK_DAY_HOUR_WEEK_PENDING_INTENT_CODE_DAILY_FORECAST_3 = 1423 const val CLOCK_DAY_HOUR_WEEK_PENDING_INTENT_CODE_DAILY_FORECAST_4 = 1424 const val CLOCK_DAY_HOUR_WEEK_PENDING_INTENT_CODE_DAILY_FORECAST_5 = 1425 const val CLOCK_DAY_HOUR_WEEK_PENDING_INTENT_CODE_CALENDAR = 143 const val CLOCK_DAY_HOUR_WEEK_PENDING_INTENT_CODE_CLOCK_LIGHT = 144 const val CLOCK_DAY_HOUR_WEEK_PENDING_INTENT_CODE_CLOCK_NORMAL = 145 const val CLOCK_DAY_HOUR_WEEK_PENDING_INTENT_CODE_CLOCK_BLACK = 146 // Add codes here if hourly forecast items become clickable // const val CLOCK_DAY_HOUR_WEEK_PENDING_INTENT_CODE_HOURLY_FORECAST_1 = 1471 // ... ... // rest of the constants } Wrapping Up \u0026amp; Final Thoughts And\u0026hellip; that should be it! After adding all these files and making the necessary resource changes, rebuild the project. The new \u0026ldquo;Clock + Day + Hour + Week\u0026rdquo; widget should now appear in your system\u0026rsquo;s widget picker. When you add it to your home screen, the configuration activity will launch, and once configured, you should see your brand new, all-in-one weather widget!\nQuick Recap of the Process:\nDefine the Goal: Create a comprehensive weather widget. Analyze Existing Patterns: Identify the Provider -\u0026gt; IMP -\u0026gt; Config -\u0026gt; Layout -\u0026gt; Definition XML workflow. Copy \u0026amp; Modify: Leverage existing code (ClockDayWeek components) as a base, then modify extensively, especially the IMP and Layout files. Core Addition: Design and implement the hourly forecast section in the layout and add the corresponding data-binding and visibility logic in the IMP\u0026rsquo;s getRemoteViews. Attention to Detail: Systematically update all relevant IDs, configuration keys, widget names, and request codes for uniqueness. Adjust widget dimensions (minHeight, minResizeHeight). Resource Integration: Add the necessary declarations and definitions in AndroidManifest.xml, keys.xml, strings.xml, and Widgets.kt. Potential Gotchas:\nRemoteViews Limitations: Remember RemoteViews only supports a limited set of Views and methods. Complex interactions or custom drawing are tricky. We stuck to basics like TextView, ImageView, LinearLayout, RelativeLayout, and TextClock, which works fine. ID Conflicts: Forgetting to rename IDs after copying is an easy mistake that can lead to update errors or crashes. Double-check them! Data Fetching: Ensure the Provider requests withHourly = true, otherwise, the hourly section will be empty. Layout Adaptability: Widget appearance might need fine-tuning with dimens.xml values to look good across different screen sizes and densities. Overall, adding the ClockDayHourWeekWidget was a relatively smooth process, largely thanks to Breezy Weather\u0026rsquo;s clean structure and consistent widget implementation pattern. It involved a fair amount of code, but much of it was following the established template. The key was understanding how RemoteViews works and carefully handling the data binding and view states in the IMP class, especially for the newly added hourly section and the visibility logic for dynamic content.\nHope this rambling dev log is helpful to someone out there! Until the next coding adventure\u0026hellip; Cheers!\nSource Code\n","permalink":"https://blog.tategotoazarasi.me/en/posts/clock-day-hour-week-widget/","summary":"A detailed guide on adding a comprehensive \u0026ldquo;ClockDayHourWeekWidget\u0026rdquo; to the Breezy Weather app, combining clock, daily, and hourly forecasts into one Android widget.","title":"Dev Log: Adding a All-in-One Widget to Breezy Weather - The ClockDayHourWeekWidget Journey"}]