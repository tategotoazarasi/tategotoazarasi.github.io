<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Building a TypeScript Video Downloader for Complex, Anti-Scraping Websites | Tategoto Azarasi</title>
<meta name=keywords content="typescript,nodejs,playwright,web-scraping,automation,video-downloader,anti-scraping,race-condition,network-interception,browser-automation,typescript-video-downloader,nodejs-playwright-tutorial,bypass-anti-scraping,handle-dynamic-content,playwright-network-interception,browser-automation-scripting,download-streaming-video,race-condition-in-web-scraping,page-evaluate-fetch,robust-downloader-nodejs,typescript-project-from-scratch,debugging-playwright,advanced-web-scraping-techniques,playwright-vs-puppeteer,content-delivery-network-cdn-scraping"><meta name=description content="How to build a robust video downloader from scratch with TypeScript, Node.js, and Playwright, capable of handling complex anti-scraping mechanisms, dynamic content loading, and network race conditions."><meta name=author content="Tategoto Azarasi"><link rel=canonical href=https://tategotoazarasi.github.io/en/posts/building-a-resilient-typescript-video-downloader-against-advanced-anti-scraping/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=https://tategotoazarasi.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tategotoazarasi.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tategotoazarasi.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://tategotoazarasi.github.io/apple-touch-icon.png><link rel=mask-icon href=https://tategotoazarasi.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://tategotoazarasi.github.io/en/posts/building-a-resilient-typescript-video-downloader-against-advanced-anti-scraping/><link rel=alternate hreflang=zh href=https://tategotoazarasi.github.io/zh/posts/building-a-resilient-typescript-video-downloader-against-advanced-anti-scraping/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link crossorigin=anonymous href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ rel=stylesheet><script crossorigin=anonymous defer integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js></script><script crossorigin=anonymous defer integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR onload=renderMathInElement(document.body) src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js></script>>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:url" content="https://tategotoazarasi.github.io/en/posts/building-a-resilient-typescript-video-downloader-against-advanced-anti-scraping/"><meta property="og:site_name" content="Tategoto Azarasi"><meta property="og:title" content="Building a TypeScript Video Downloader for Complex, Anti-Scraping Websites"><meta property="og:description" content="How to build a robust video downloader from scratch with TypeScript, Node.js, and Playwright, capable of handling complex anti-scraping mechanisms, dynamic content loading, and network race conditions."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-09T19:33:20+00:00"><meta property="article:modified_time" content="2025-11-09T19:33:20+00:00"><meta property="article:tag" content="Typescript"><meta property="article:tag" content="Nodejs"><meta property="article:tag" content="Playwright"><meta property="article:tag" content="Web-Scraping"><meta property="article:tag" content="Automation"><meta property="article:tag" content="Video-Downloader"><meta name=twitter:card content="summary"><meta name=twitter:title content="Building a TypeScript Video Downloader for Complex, Anti-Scraping Websites"><meta name=twitter:description content="How to build a robust video downloader from scratch with TypeScript, Node.js, and Playwright, capable of handling complex anti-scraping mechanisms, dynamic content loading, and network race conditions."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tategotoazarasi.github.io/en/posts/"},{"@type":"ListItem","position":2,"name":"Building a TypeScript Video Downloader for Complex, Anti-Scraping Websites","item":"https://tategotoazarasi.github.io/en/posts/building-a-resilient-typescript-video-downloader-against-advanced-anti-scraping/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Building a TypeScript Video Downloader for Complex, Anti-Scraping Websites","name":"Building a TypeScript Video Downloader for Complex, Anti-Scraping Websites","description":"How to build a robust video downloader from scratch with TypeScript, Node.js, and Playwright, capable of handling complex anti-scraping mechanisms, dynamic content loading, and network race conditions.","keywords":["typescript","nodejs","playwright","web-scraping","automation","video-downloader","anti-scraping","race-condition","network-interception","browser-automation","typescript-video-downloader","nodejs-playwright-tutorial","bypass-anti-scraping","handle-dynamic-content","playwright-network-interception","browser-automation-scripting","download-streaming-video","race-condition-in-web-scraping","page-evaluate-fetch","robust-downloader-nodejs","typescript-project-from-scratch","debugging-playwright","advanced-web-scraping-techniques","playwright-vs-puppeteer","content-delivery-network-cdn-scraping"],"articleBody":"In modern web development, we often encounter tasks that seem simple on the surface but are fraught with hidden challenges. I recently faced such a task: downloading a video from a specific website. This was no simple static page; it heavily utilized JavaScript for dynamic content loading and was fortified with rather sophisticated anti-scraping and content protection mechanisms. This blog post will chronicle the entire journey of how I built a robust download tool from scratch using Node.js, TypeScript, and Playwright, step-by-step, to overcome these challenges.\nThe Initial Idea and Technology Selection The mission began with a clear objective: given a webpage URL, I needed to download the main video on that page to my local machine. My first instinct was to inspect the page’s source code, hoping to find a direct link to an .mp4 file. However, reality quickly set in. Upon opening the browser’s developer tools, I discovered that the initial HTML document contained no direct URLs to any video files. The video player, its source information, and everything related to it were dynamically generated and injected into the page only after a series of complex JavaScript scripts had finished executing.\nThis immediately ruled out using traditional HTTP clients like curl or wget, or even simple Node.js libraries like axios or node-fetch. These tools can only retrieve the initial HTML skeleton, which is devoid of any video information. They cannot emulate a real user environment, execute JavaScript, and therefore, can never “see” the dynamically generated video player.\nI knew at once that the key to solving this problem was to simulate a complete browser environment. I needed a tool that could not only load a webpage but also possess a JavaScript engine to execute all the scripts on the page, render the DOM, and respond to user interactions, just like Chrome or Firefox.\nMy technology selection process quickly honed in on a few key components:\nNode.js: As the backend runtime, its powerful file system capabilities and rich ecosystem make it the ideal platform for building this kind of automation tool.\nTypeScript: I chose TypeScript over native JavaScript primarily for the long-term maintainability and robustness of the project. TypeScript’s static type checking can catch a vast number of potential errors during the development phase, such as typos and type mismatches. For a project dealing with complex browser APIs and network data streams, type safety dramatically improves development efficiency and code quality. This was not just a personal preference, but a sound engineering decision.\nPlaywright: In the realm of browser automation, two main contenders stand out: Puppeteer and Playwright. While both are excellent, I ultimately opted for Playwright. As a more modern tool from Microsoft, it offers superior cross-browser support (Chromium, Firefox, WebKit), a cleaner and more powerful API, and provides incredibly flexible and reliable interfaces for handling waits, interactions, and network interception. I believed that for a site with potentially complex anti-scraping measures, Playwright’s advanced network control capabilities would be crucial.\nWith the tech stack decided, I began setting up the project. This process itself is a showcase of an engineer’s foundational skills. I created a new project directory, initialized a package.json with npm init -y, and installed the core dependency playwright along with development dependencies like typescript, ts-node, and @types/node. Next, I generated a tsconfig.json file using npx tsc --init.\nI meticulously configured the tsconfig.json file to align with modern Node.js best practices. For instance, I set the target to ES2020 to leverage modern syntax like async/await, configured the module to CommonJS (our initial choice on this journey, which would later evolve), and clearly defined rootDir and outDir to separate source code from compiled output. These seemingly minor configurations are the first step toward a well-structured and professional project.\nHeadless Browser and a Simulated Click With the project skeleton in place, I started writing the core logic. The core idea of my first version was straightforward: “emulate the user, listen to the network.”\nI began by designing a VideoDownloader class to encapsulate all related operations. This is a good engineering practice that leads to a cleaner, more extensible, and maintainable code structure.\nMy first step was to implement the most basic user behavior simulation. I wrote several core private methods: initialize to launch Playwright and create a new browser page instance; navigateToPage to navigate to the user-provided target URL; and clickPlayButton to find and click the play button on the page.\nThe key assumption here was that the real video URL would only begin to load after the user clicks the play button. Therefore, my primary task was to locate that button. By inspecting the page’s DOM, I identified the CSS selector for the play button, which was a button element with specific class names.\nBut simply clicking the button wasn’t enough. The click would trigger a network request for the video file, and I needed to capture it. Playwright’s powerful network listening capabilities came into play here. In the initialize method, immediately after creating the page instance, I registered a network response listener:\n// Core Snippet - Registering the response listener this.page.on('response', this.handleResponse.bind(this)); This listener would capture every single network response generated by the page. The handleResponse method was where I implemented the core filtering and data processing logic.\nInside this method, I had to accurately identify the video segments from potentially hundreds of network requests. By observing the network panel in the browser’s developer tools, I discovered several key characteristics of the video requests:\nFirst, the request URL contained a specific file extension, such as .mp4. Second, the video was typically loaded in chunks to support seeking and save bandwidth. This chunked loading is implemented in the HTTP protocol via the Range request header and the status code 206 Partial Content. The server’s response headers would include a Content-Range field, which explicitly states the segment’s position within the entire video file and the total file size.\nBased on these observations, I established my filtering logic: I would only process responses whose URLs contained a specific keyword (like .mp4) and had an HTTP status code of 206.\nThe next challenge was how to handle these captured video chunks, which could arrive out of order. Simply pushing them into an array would likely result in a corrupted final file. The correct approach was to store them based on their position in the complete video. The Content-Range header provided this crucial piece of information, for example, bytes 0-5242879/906098800, indicating a data block starting at byte 0.\nConsequently, I decided to use a Map data structure to store the segments, using the starting byte of the chunk as the key and the Buffer containing the binary data as the value.\n// Core Snippet - Storing chunks using a Map private videoChunks: Map\u003cnumber, Buffer\u003e = new Map(); // Inside handleResponse const rangeMatch = contentRange.match(/bytes (\\d+)-(\\d+)\\/(\\d+)/); if (rangeMatch) { const start = Number.parseInt(rangeMatch[1], 10); const buffer = await response.body(); // This was an early mistake, as we'll see this.videoChunks.set(start, buffer); } The advantage of this approach is that regardless of the order in which the chunks arrive, I can place them precisely where they belong.\nThe final step was file assembly. Once all chunks were downloaded, I needed to write them to a local file in the correct order. For efficiency and memory conservation, I opted against concatenating all chunks into one giant Buffer in memory. Instead, I leveraged Node.js’s Stream API. I created an fs.createWriteStream, then iterated over the sorted keys of my Map (the chunk start bytes), and sequentially fetched the corresponding Buffer for each key to write to the stream. This stream-based writing approach has a very low memory footprint and can handle massive files, even tens of gigabytes, with ease.\nWith that, the logic for my first version was complete. It seemed perfect: simulate a user click, listen to the network, filter for video chunks, store them in order, and write them efficiently. I ran the script with confidence.\nTimeouts and the networkidle Trap After running the program, the terminal output stalled at “Navigating to page…”, and a minute later, a bright red error message announced the failure of my first attempt: page.goto: Timeout 60000ms exceeded. waiting until \"networkidle\".\nA timeout error. This is an extremely common issue in automation and web scraping. I immediately reviewed my navigateToPage method:\n// Early, incorrect code await this.page.goto(this.url, { waitUntil: 'networkidle', timeout: 60000 }); The problem was waitUntil: 'networkidle'. This option tells Playwright to consider the navigation successful only when the page has loaded and there has been no new network activity for at least 500 milliseconds.\nThis option is useful for simple, static pages that become “quiet” after loading. However, for the modern, dynamic video site I was dealing with, this was an almost impossible condition to meet. The frontend applications of such sites are highly complex and engage in continuous background network activity:\nSending user behavior analytics and tracking data. Periodically refreshing ad content. Maintaining connections with the server via long-polling or WebSockets for real-time updates. Pre-loading images or other media resources. In this environment, the page’s network activity almost never goes “idle” for more than 500 milliseconds. My program waited patiently for the full 60 seconds, only to inevitably time out.\nThis failure made me realize that my waiting strategy had to be more precise. I didn’t need to wait for the entire page to fall silent; I only needed to wait until the key element I intended to interact with—the play button—was present and available.\nI quickly adjusted my strategy. First, I relaxed the page.goto waiting condition from networkidle to domcontentloaded. This option tells Playwright to proceed as soon as the core HTML document is loaded and parsed, without waiting for all images, stylesheets, and async scripts to finish. This made the navigation step fast and reliable.\nThen, I placed the real, precise waiting logic inside the clickPlayButton method. This method already contained a call to await this.page.waitForSelector(playButtonSelector, ...). This was the correct form of “waiting,” as it targeted a specific goal rather than an ambiguous and unattainable network state.\nIn the process of fixing this issue, I made an additional optimization. I realized that much of the continuous background network activity was related to ad and tracking scripts. These requests not only slowed down the page load and increased the risk of networkidle timeouts but were also completely useless for my download task. So, I decided to set up a request interceptor before navigation to proactively block these unnecessary requests.\n// Core Snippet - Blocking unnecessary requests await this.page.route('**/*', (route) =\u003e { const url = route.request().url(); if (url.includes('google-analytics.com') || url.includes('doubleclick.net')) { return route.abort(); } return route.continue(); }); This small change not only made the page load faster but also made my automation script’s environment cleaner and more predictable. It reflected an engineer’s mindset shift from just “making it work” to “making it work better.”\nA Race Condition After resolving the navigation timeout, I ran the program again. This time, the logs showed the page loaded successfully, and the play button was clicked. I could almost hear the trumpets of victory. However, a few seconds later, a new, more bizarre error appeared: Protocol error (Network.getResponseBody): No data found for resource with given identifier.\nThe program had successfully captured the video chunk’s response (response object), but it failed when attempting to retrieve its content (await response.body()). The error message was very low-level, pointing directly at the Chrome DevTools Protocol (CDP) layer.\nI spent a significant amount of time debugging and understanding this error. At first, I suspected a Playwright bug or some special encryption on the website’s part. But after repeated experiments and research, I finally identified the root cause: a classic and tricky “Race Condition.”\nMy code and the browser’s internal media processing pipeline were in a high-speed race, and I was losing.\nHere’s what was happening:\nThe browser initiates a request for a video chunk. The server returns data. Playwright’s backend detects this response via the CDP and notifies my Node.js process. The Node.js event loop places a 'response' event onto its event queue. Meanwhile, the browser, a highly optimized C++ application, doesn’t wait for my Node.js script. Its network stack receives the video data and may immediately send it via “zero-copy” or similar mechanisms directly to the GPU or media decoder for playback. For maximum performance, it clears this data from memory as soon as this operation is complete, assuming the data has been “consumed.” When the Node.js event loop finally gets around to processing my 'response' event, my handleResponse callback function begins to execute. When the code reaches await response.body(), it sends a command back to the browser via CDP: “Please give me the data for that response you just told me about.” The browser replies: “Sorry, the thing you’re asking for? I already processed and discarded it. The data is gone.” And thus, the No data found for resource error was thrown. The delay in this process might be mere milliseconds or even microseconds, but in the face of a high-performance browser kernel, that’s more than enough time for the data to vanish. I realized that any form of “post-mortem” listening—any approach that tries to get the data after the response has already happened—was inherently at risk of failing.\nTo win this race, I needed a more proactive, more invasive method. I turned once again to page.route. This time, I wouldn’t just use it to abort() requests; I would use it to completely “proxy” the video requests. My new plan was:\nIntercept an outgoing video request. Instead of letting the browser make the request, I would call route.fetch(), which instructs Playwright’s backend to perform the request on the browser’s behalf. After getting the APIResponse object back from route.fetch(), I could safely await response.body() from it, because this was my request, and the data was fully preserved. After capturing and storing the data in my videoChunks Map, I would then call route.fulfill({ response }) to pass a “forged” copy of this response back to the browser, making the page’s video player think everything was normal. This solution was, in theory, perfect. It transformed passive listening into active proxying, fundamentally solving the race condition. I refactored the code and ran it again, full of hope.\nIt worked! I successfully captured the first, and then the second, chunk of data! However, the joy was short-lived. After downloading a few chunks (about 9MB), the network requests stopped dead. The program timed out after waiting 20 seconds, leaving me with an incomplete file.\nWhy did the download chain break? Had my route.fulfill() call failed to properly “deceive” the player?\nTo find the truth, I added extremely verbose logging to my interceptor, printing the URL, method, type of every intercepted request, and the complete headers of every video response.\nThe logs quickly revealed the first stunning discovery: after the play button was clicked, the player initiated concurrent requests for multiple different video resolutions! For example, 720P.mp4, 1440P.mp4, and 2048P.mp4. It seemed to be probing to determine the best bitrate for the current network speed.\nMy code had a critical flaw: I was using a single activeVideoUrl variable to lock onto the first video stream I downloaded. When the request for 720P.mp4 arrived first, my program “decided” this was the target and ruthlessly ignored all subsequent requests for 1440P and 2048P. However, the player, after its brief probing, may have ultimately decided to continue streaming the 2048P version. Because my program didn’t correctly fulfill the 2048P requests (since it ignored them), the player’s state machine was broken, and it ceased all subsequent requests.\nI immediately corrected this logic. I stopped being “first-come, first-served” and instead maintained a list of all candidate streams. After the download finished, I would then select the “best” stream to assemble. My initial criterion for “best” was “the one with the most chunks.”\nYet, upon running it again, the problem persisted. The logs showed that every resolution stream was requested only for 2 chunks, and then they all stopped. My “select the most chunks” strategy degenerated into “select the first one” in this scenario, which was still wrong.\nIt was then that I finally grasped the true essence of the problem. It wasn’t my selection logic that was flawed; it was that route.fulfill() itself was the problem. The APIResponse object I got from route.fetch(), while containing the data, was ultimately a “clone.” When passed back to the browser via route.fulfill(), it may have lost certain low-level connection handles, timing information, or other metadata critical to the player. This “imperfect” response, while providing the initial few chunks of data, was enough to corrupt the player’s delicate internal state, causing it to refuse to request subsequent segments.\nActive Control over Passive Listening After the failures of both the page.on('response') race condition and the route.fulfill() state corruption, I was at an impasse. I seemed to be caught in a dilemma: either be too slow, or be too intrusive.\nIt was at that moment that a completely new idea took shape in my mind. All my previous attempts had revolved around a central theme: how to “steal” data from requests initiated by the browser. Whether listening or proxying, I was a “parasite” attached to the browser’s behavior.\nWhy not flip the script? Why couldn’t I be the one driving the download process?\nThis idea completely transformed my architecture. The new, final solution was born, divided into two distinct phases: Reconnaissance and Takeover.\nReconnaissance I still used page.route to intercept network requests. But this time, its purpose was incredibly simple: look, don’t touch. When a video chunk request was intercepted, I did nothing but immediately call route.continue(), giving the browser an unobstructed path to conduct its own network communications. This ensured the player’s state remained absolutely pure and continuous. Simultaneously, in the background, I quietly used page.waitForResponse() to await the completion of the very request I had just allowed to pass. When the response arrived, I parsed from it all the information I needed: the full URL (including the dynamically generated token), the total video size, and the resolution. I stored this information in my discoveredStreams Map. I set a 5-10 second reconnaissance window. During this time, the player would probe all the resolution streams as usual. My recon detector would gather information on all of them. At the end of the window, I would examine the discoveredStreams Map and select the stream with the highest resolution. With that, I had all the critical intelligence needed to download the full, high-quality video.\nTakeover With the recon mission complete, I no longer cared about the browser’s subsequent network activities. I initiated a brand new download loop, one that I was in complete control of. In this loop, I would execute the most core and elegant operation of the entire solution: making the browser work for me.\nI used the page.evaluate() function. This function can execute arbitrary JavaScript code within the context of the browser page. From my Node.js side, I calculated the range of the next chunk I wanted to download, for example, bytes=0-5242879, and then passed this range along with the high-resolution URL from the recon phase as arguments to page.evaluate.\n// Core Snippet - Initiating a fetch request inside the browser const chunkData = await this.page.evaluate(async ({ url, range }) =\u003e { const response = await fetch(url, { headers: { 'Range': range } }); if (!response.ok) throw new Error(`Fetch failed: ${response.status}`); const buffer = await response.arrayBuffer(); return { data: Array.from(new Uint8Array(buffer)) }; }, { url, range }); The magic of this code lies in the fact that fetch(url, ...) is executed inside the browser. This means the request:\nAutomatically carries all the current page’s cookies and session information. Originates from the same IP address and browser fingerprint. Has completely passed any Cloudflare or other JavaScript-based human verification checks the site might have deployed. It was a perfect, legitimate request originating from a real user session. The server could not distinguish it from the player’s own requests.\nThe response from fetch is an ArrayBuffer, an object that cannot be directly passed from the browser back to Node.js. Therefore, I cleverly converted it into a plain JavaScript array of numbers using Array.from(new Uint8Array(buffer)), a data structure that can be serialized as JSON and passed across process boundaries.\nBack in my Node.js environment, I received this array of numbers, converted it back into a Node.js Buffer object with Buffer.from(chunkData.data), and then steadily wrote it to my file stream.\nI repeated this process in a for loop, sequentially requesting the entire video file chunk by chunk, according to the size I defined, until every last byte was downloaded.\nThis solution proved to be unassailable. It completely sidestepped the race condition because it was no longer passively listening. It also entirely avoided interfering with the player’s state, because it let the browser’s business be the browser’s, and the download’s be mine. I was simply borrowing the browser’s “identity” to make my own requests.\nPolishing the Product Having solved the core download challenge, I didn’t stop there. A great engineer doesn’t just solve problems; they deliver a great tool. I began to add a series of professional features to the script.\nCommand-Line Argument Support I removed the hardcoded TARGET_URL and switched to using Node.js’s process.argv to parse command-line arguments. Now, a user could simply append any video URL they wanted to download after npm start, vastly improving the tool’s flexibility. I also added parameter checking; if no URL was provided, the program would print a usage hint and exit gracefully.\nRich Progress Display A long download process without any feedback is a terrible user experience. I created a separate DownloadTracker helper class dedicated to handling download statistics. It records a timestamp at the start of the download and updates the number of downloaded bytes each time a data chunk is received. To avoid spamming the console with frequent updates, I used a simple throttling technique: the progress information would update at most once per second. With each update, it would calculate:\nAverage Download Speed (Mbps): (Total Bytes Downloaded * 8) / Seconds Elapsed / 1024 / 1024 Time Remaining (seconds): (Total Size - Bytes Downloaded) / Average Speed Estimated Time of Arrival (ETA): Current Time + Time Remaining I then formatted this information into a single, clear, continuously refreshing status line, such as: Progress: 25.13% | 216.05MB / 860.00MB | Speed: 123.45 Mbps | ETA: 19:30:45 (55s remaining) This provided excellent real-time feedback to the user. Download Retry Mechanism Networks are unstable. During the download of a large file, the failure of a single chunk due to a temporary network glitch should not terminate the entire task. I added a while loop-based retry mechanism to the downloadChunkWithRetries logic. If a fetch failed, it wouldn’t give up immediately. It would enter a catch block where I would increment a retry counter and calculate a wait time using an “exponential backoff” strategy (e.g., wait 2s on the 1st retry, 4s on the 2nd, 8s on the 3rd…). After a short wait, it would attempt to download the exact same chunk again. Only after reaching the maximum number of retries (e.g., 5) would the program finally give up and abort the entire download. This mechanism dramatically enhanced the program’s robustness in unstable network environments.\nIntelligent File Naming Finally, I implemented a feature you might have seen in another one of my projects: extracting a meaningful title from the page to use as the filename. I wrote an extractVideoTitle method that would search for specific elements on the page in order of priority (first p[lang=\"ja\"], then h2.mt-16). If neither was found, it would fall back to using a part of the URL. I also cleaned the extracted title, removing any characters that are illegal in filenames. In the end, the downloaded file was named [Video Title]-[Resolution].mp4, making it instantly identifiable.\n","wordCount":"3970","inLanguage":"en","datePublished":"2025-11-09T19:33:20Z","dateModified":"2025-11-09T19:33:20Z","author":{"@type":"Person","name":"Tategoto Azarasi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://tategotoazarasi.github.io/en/posts/building-a-resilient-typescript-video-downloader-against-advanced-anti-scraping/"},"publisher":{"@type":"Organization","name":"Tategoto Azarasi","logo":{"@type":"ImageObject","url":"https://tategotoazarasi.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://tategotoazarasi.github.io/en/ accesskey=h title="Tategoto Azarasi (Alt + H)">Tategoto Azarasi</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://tategotoazarasi.github.io/zh/ title=中文 aria-label=中文>Zh</a></li></ul></div></div><ul id=menu><li><a href=https://tategotoazarasi.github.io/en/ title=Home><span>Home</span></a></li><li><a href=https://tategotoazarasi.github.io/en/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://tategotoazarasi.github.io/en/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://tategotoazarasi.github.io/en/>Home</a>&nbsp;»&nbsp;<a href=https://tategotoazarasi.github.io/en/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Building a TypeScript Video Downloader for Complex, Anti-Scraping Websites</h1><div class=post-meta><span title='2025-11-09 19:33:20 +0000 UTC'>November 9, 2025</span>&nbsp;·&nbsp;19 min&nbsp;·&nbsp;3970 words&nbsp;·&nbsp;Tategoto Azarasi&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://tategotoazarasi.github.io/zh/posts/building-a-resilient-typescript-video-downloader-against-advanced-anti-scraping/>Zh</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ol><li><a href=#the-initial-idea-and-technology-selection>The Initial Idea and Technology Selection</a></li><li><a href=#headless-browser-and-a-simulated-click>Headless Browser and a Simulated Click</a></li><li><a href=#timeouts-and-the-networkidle-trap>Timeouts and the <code>networkidle</code> Trap</a></li><li><a href=#a-race-condition>A Race Condition</a></li><li><a href=#active-control-over-passive-listening>Active Control over Passive Listening</a><ol><li><a href=#reconnaissance>Reconnaissance</a></li><li><a href=#takeover>Takeover</a></li></ol></li><li><a href=#polishing-the-product>Polishing the Product</a><ol><li><a href=#command-line-argument-support>Command-Line Argument Support</a></li><li><a href=#rich-progress-display>Rich Progress Display</a></li><li><a href=#download-retry-mechanism>Download Retry Mechanism</a></li><li><a href=#intelligent-file-naming>Intelligent File Naming</a></li></ol></li></ol></nav></div></details></div><div class=post-content><p>In modern web development, we often encounter tasks that seem simple on the surface but are fraught with hidden challenges. I recently faced such a task: downloading a video from a specific website. This was no simple static page; it heavily utilized JavaScript for dynamic content loading and was fortified with rather sophisticated anti-scraping and content protection mechanisms. This blog post will chronicle the entire journey of how I built a robust download tool from scratch using Node.js, TypeScript, and Playwright, step-by-step, to overcome these challenges.</p><h2 id=the-initial-idea-and-technology-selection>The Initial Idea and Technology Selection<a hidden class=anchor aria-hidden=true href=#the-initial-idea-and-technology-selection>#</a></h2><p>The mission began with a clear objective: given a webpage URL, I needed to download the main video on that page to my local machine. My first instinct was to inspect the page&rsquo;s source code, hoping to find a direct link to an <code>.mp4</code> file. However, reality quickly set in. Upon opening the browser&rsquo;s developer tools, I discovered that the initial HTML document contained no direct URLs to any video files. The video player, its source information, and everything related to it were dynamically generated and injected into the page only after a series of complex JavaScript scripts had finished executing.</p><p>This immediately ruled out using traditional HTTP clients like <code>curl</code> or <code>wget</code>, or even simple Node.js libraries like <code>axios</code> or <code>node-fetch</code>. These tools can only retrieve the initial HTML skeleton, which is devoid of any video information. They cannot emulate a real user environment, execute JavaScript, and therefore, can never &ldquo;see&rdquo; the dynamically generated video player.</p><p>I knew at once that the key to solving this problem was to simulate a complete browser environment. I needed a tool that could not only load a webpage but also possess a JavaScript engine to execute all the scripts on the page, render the DOM, and respond to user interactions, just like Chrome or Firefox.</p><p>My technology selection process quickly honed in on a few key components:</p><p><strong>Node.js</strong>: As the backend runtime, its powerful file system capabilities and rich ecosystem make it the ideal platform for building this kind of automation tool.</p><p><strong>TypeScript</strong>: I chose TypeScript over native JavaScript primarily for the long-term maintainability and robustness of the project. TypeScript&rsquo;s static type checking can catch a vast number of potential errors during the development phase, such as typos and type mismatches. For a project dealing with complex browser APIs and network data streams, type safety dramatically improves development efficiency and code quality. This was not just a personal preference, but a sound engineering decision.</p><p><strong>Playwright</strong>: In the realm of browser automation, two main contenders stand out: Puppeteer and Playwright. While both are excellent, I ultimately opted for Playwright. As a more modern tool from Microsoft, it offers superior cross-browser support (Chromium, Firefox, WebKit), a cleaner and more powerful API, and provides incredibly flexible and reliable interfaces for handling waits, interactions, and network interception. I believed that for a site with potentially complex anti-scraping measures, Playwright&rsquo;s advanced network control capabilities would be crucial.</p><p>With the tech stack decided, I began setting up the project. This process itself is a showcase of an engineer&rsquo;s foundational skills. I created a new project directory, initialized a <code>package.json</code> with <code>npm init -y</code>, and installed the core dependency <code>playwright</code> along with development dependencies like <code>typescript</code>, <code>ts-node</code>, and <code>@types/node</code>. Next, I generated a <code>tsconfig.json</code> file using <code>npx tsc --init</code>.</p><p>I meticulously configured the <code>tsconfig.json</code> file to align with modern Node.js best practices. For instance, I set the <code>target</code> to <code>ES2020</code> to leverage modern syntax like <code>async/await</code>, configured the <code>module</code> to <code>CommonJS</code> (our initial choice on this journey, which would later evolve), and clearly defined <code>rootDir</code> and <code>outDir</code> to separate source code from compiled output. These seemingly minor configurations are the first step toward a well-structured and professional project.</p><h2 id=headless-browser-and-a-simulated-click>Headless Browser and a Simulated Click<a hidden class=anchor aria-hidden=true href=#headless-browser-and-a-simulated-click>#</a></h2><p>With the project skeleton in place, I started writing the core logic. The core idea of my first version was straightforward: &ldquo;emulate the user, listen to the network.&rdquo;</p><p>I began by designing a <code>VideoDownloader</code> class to encapsulate all related operations. This is a good engineering practice that leads to a cleaner, more extensible, and maintainable code structure.</p><p>My first step was to implement the most basic user behavior simulation. I wrote several core private methods: <code>initialize</code> to launch Playwright and create a new browser page instance; <code>navigateToPage</code> to navigate to the user-provided target URL; and <code>clickPlayButton</code> to find and click the play button on the page.</p><p>The key assumption here was that the real video URL would only begin to load <em>after</em> the user clicks the play button. Therefore, my primary task was to locate that button. By inspecting the page&rsquo;s DOM, I identified the CSS selector for the play button, which was a <code>button</code> element with specific class names.</p><p>But simply clicking the button wasn&rsquo;t enough. The click would trigger a network request for the video file, and I needed to capture it. Playwright&rsquo;s powerful network listening capabilities came into play here. In the <code>initialize</code> method, immediately after creating the page instance, I registered a network response listener:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-typescript data-lang=typescript><span style=display:flex><span><span style=color:#75715e>// Core Snippet - Registering the response listener
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>this</span>.<span style=color:#a6e22e>page</span>.<span style=color:#a6e22e>on</span>(<span style=color:#e6db74>&#39;response&#39;</span>, <span style=color:#66d9ef>this</span>.<span style=color:#a6e22e>handleResponse</span>.<span style=color:#a6e22e>bind</span>(<span style=color:#66d9ef>this</span>));
</span></span></code></pre></div><p>This listener would capture every single network response generated by the page. The <code>handleResponse</code> method was where I implemented the core filtering and data processing logic.</p><p>Inside this method, I had to accurately identify the video segments from potentially hundreds of network requests. By observing the network panel in the browser&rsquo;s developer tools, I discovered several key characteristics of the video requests:</p><p>First, the request URL contained a specific file extension, such as <code>.mp4</code>.
Second, the video was typically loaded in chunks to support seeking and save bandwidth. This chunked loading is implemented in the HTTP protocol via the <code>Range</code> request header and the status code <code>206 Partial Content</code>. The server&rsquo;s response headers would include a <code>Content-Range</code> field, which explicitly states the segment&rsquo;s position within the entire video file and the total file size.</p><p>Based on these observations, I established my filtering logic: I would only process responses whose URLs contained a specific keyword (like <code>.mp4</code>) and had an HTTP status code of <code>206</code>.</p><p>The next challenge was how to handle these captured video chunks, which could arrive out of order. Simply pushing them into an array would likely result in a corrupted final file. The correct approach was to store them based on their position in the complete video. The <code>Content-Range</code> header provided this crucial piece of information, for example, <code>bytes 0-5242879/906098800</code>, indicating a data block starting at byte <code>0</code>.</p><p>Consequently, I decided to use a <code>Map</code> data structure to store the segments, using the starting byte of the chunk as the <code>key</code> and the <code>Buffer</code> containing the binary data as the <code>value</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-typescript data-lang=typescript><span style=display:flex><span><span style=color:#75715e>// Core Snippet - Storing chunks using a Map
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>private</span> <span style=color:#a6e22e>videoChunks</span>: <span style=color:#66d9ef>Map</span>&lt;<span style=color:#f92672>number</span>, <span style=color:#a6e22e>Buffer</span>&gt; <span style=color:#f92672>=</span> <span style=color:#66d9ef>new</span> <span style=color:#a6e22e>Map</span>();
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// Inside handleResponse
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>rangeMatch</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>contentRange</span>.<span style=color:#a6e22e>match</span>(<span style=color:#e6db74>/bytes (\d+)-(\d+)\/(\d+)/</span>);
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>rangeMatch</span>) {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>start</span> <span style=color:#f92672>=</span> Number.parseInt(<span style=color:#a6e22e>rangeMatch</span>[<span style=color:#ae81ff>1</span>], <span style=color:#ae81ff>10</span>);
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>buffer</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> <span style=color:#a6e22e>response</span>.<span style=color:#a6e22e>body</span>(); <span style=color:#75715e>// This was an early mistake, as we&#39;ll see
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>this</span>.<span style=color:#a6e22e>videoChunks</span>.<span style=color:#66d9ef>set</span>(<span style=color:#a6e22e>start</span>, <span style=color:#a6e22e>buffer</span>);
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>The advantage of this approach is that regardless of the order in which the chunks arrive, I can place them precisely where they belong.</p><p>The final step was file assembly. Once all chunks were downloaded, I needed to write them to a local file in the correct order. For efficiency and memory conservation, I opted against concatenating all chunks into one giant <code>Buffer</code> in memory. Instead, I leveraged Node.js&rsquo;s Stream API. I created an <code>fs.createWriteStream</code>, then iterated over the sorted keys of my <code>Map</code> (the chunk start bytes), and sequentially fetched the corresponding <code>Buffer</code> for each key to write to the stream. This stream-based writing approach has a very low memory footprint and can handle massive files, even tens of gigabytes, with ease.</p><p>With that, the logic for my first version was complete. It seemed perfect: simulate a user click, listen to the network, filter for video chunks, store them in order, and write them efficiently. I ran the script with confidence.</p><h2 id=timeouts-and-the-networkidle-trap>Timeouts and the <code>networkidle</code> Trap<a hidden class=anchor aria-hidden=true href=#timeouts-and-the-networkidle-trap>#</a></h2><p>After running the program, the terminal output stalled at &ldquo;Navigating to page&mldr;&rdquo;, and a minute later, a bright red error message announced the failure of my first attempt: <code>page.goto: Timeout 60000ms exceeded. waiting until "networkidle"</code>.</p><p>A timeout error. This is an extremely common issue in automation and web scraping. I immediately reviewed my <code>navigateToPage</code> method:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-typescript data-lang=typescript><span style=display:flex><span><span style=color:#75715e>// Early, incorrect code
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>await</span> <span style=color:#66d9ef>this</span>.<span style=color:#a6e22e>page</span>.<span style=color:#66d9ef>goto</span>(<span style=color:#66d9ef>this</span>.<span style=color:#a6e22e>url</span>, { <span style=color:#a6e22e>waitUntil</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;networkidle&#39;</span>, <span style=color:#a6e22e>timeout</span>: <span style=color:#66d9ef>60000</span> });
</span></span></code></pre></div><p>The problem was <code>waitUntil: 'networkidle'</code>. This option tells Playwright to consider the navigation successful only when the page has loaded and there has been no new network activity for at least 500 milliseconds.</p><p>This option is useful for simple, static pages that become &ldquo;quiet&rdquo; after loading. However, for the modern, dynamic video site I was dealing with, this was an almost impossible condition to meet. The frontend applications of such sites are highly complex and engage in continuous background network activity:</p><ul><li>Sending user behavior analytics and tracking data.</li><li>Periodically refreshing ad content.</li><li>Maintaining connections with the server via long-polling or WebSockets for real-time updates.</li><li>Pre-loading images or other media resources.</li></ul><p>In this environment, the page&rsquo;s network activity almost never goes &ldquo;idle&rdquo; for more than 500 milliseconds. My program waited patiently for the full 60 seconds, only to inevitably time out.</p><p>This failure made me realize that my waiting strategy had to be more precise. I didn&rsquo;t need to wait for the entire page to fall silent; I only needed to wait until the <strong>key element</strong> I intended to interact with—the play button—was present and available.</p><p>I quickly adjusted my strategy. First, I relaxed the <code>page.goto</code> waiting condition from <code>networkidle</code> to <code>domcontentloaded</code>. This option tells Playwright to proceed as soon as the core HTML document is loaded and parsed, without waiting for all images, stylesheets, and async scripts to finish. This made the navigation step fast and reliable.</p><p>Then, I placed the real, precise waiting logic inside the <code>clickPlayButton</code> method. This method already contained a call to <code>await this.page.waitForSelector(playButtonSelector, ...)</code>. This was the correct form of &ldquo;waiting,&rdquo; as it targeted a specific goal rather than an ambiguous and unattainable network state.</p><p>In the process of fixing this issue, I made an additional optimization. I realized that much of the continuous background network activity was related to ad and tracking scripts. These requests not only slowed down the page load and increased the risk of <code>networkidle</code> timeouts but were also completely useless for my download task. So, I decided to set up a request interceptor <em>before</em> navigation to proactively block these unnecessary requests.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-typescript data-lang=typescript><span style=display:flex><span><span style=color:#75715e>// Core Snippet - Blocking unnecessary requests
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>await</span> <span style=color:#66d9ef>this</span>.<span style=color:#a6e22e>page</span>.<span style=color:#a6e22e>route</span>(<span style=color:#e6db74>&#39;**/*&#39;</span>, (<span style=color:#a6e22e>route</span>) <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>url</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>route</span>.<span style=color:#a6e22e>request</span>().<span style=color:#a6e22e>url</span>();
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>url</span>.<span style=color:#a6e22e>includes</span>(<span style=color:#e6db74>&#39;google-analytics.com&#39;</span>) <span style=color:#f92672>||</span> <span style=color:#a6e22e>url</span>.<span style=color:#a6e22e>includes</span>(<span style=color:#e6db74>&#39;doubleclick.net&#39;</span>)) {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#a6e22e>route</span>.<span style=color:#a6e22e>abort</span>();
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#a6e22e>route</span>.<span style=color:#66d9ef>continue</span>();
</span></span><span style=display:flex><span>});
</span></span></code></pre></div><p>This small change not only made the page load faster but also made my automation script&rsquo;s environment cleaner and more predictable. It reflected an engineer&rsquo;s mindset shift from just &ldquo;making it work&rdquo; to &ldquo;making it work better.&rdquo;</p><h2 id=a-race-condition>A Race Condition<a hidden class=anchor aria-hidden=true href=#a-race-condition>#</a></h2><p>After resolving the navigation timeout, I ran the program again. This time, the logs showed the page loaded successfully, and the play button was clicked. I could almost hear the trumpets of victory. However, a few seconds later, a new, more bizarre error appeared: <code>Protocol error (Network.getResponseBody): No data found for resource with given identifier</code>.</p><p>The program had successfully captured the video chunk&rsquo;s response (<code>response</code> object), but it failed when attempting to retrieve its content (<code>await response.body()</code>). The error message was very low-level, pointing directly at the Chrome DevTools Protocol (CDP) layer.</p><p>I spent a significant amount of time debugging and understanding this error. At first, I suspected a Playwright bug or some special encryption on the website&rsquo;s part. But after repeated experiments and research, I finally identified the root cause: a classic and tricky &ldquo;Race Condition.&rdquo;</p><p>My code and the browser&rsquo;s internal media processing pipeline were in a high-speed race, and I was losing.</p><p>Here&rsquo;s what was happening:</p><ol><li>The browser initiates a request for a video chunk.</li><li>The server returns data. Playwright&rsquo;s backend detects this response via the CDP and notifies my Node.js process.</li><li>The Node.js event loop places a <code>'response'</code> event onto its event queue.</li><li>Meanwhile, the browser, a highly optimized C++ application, doesn&rsquo;t wait for my Node.js script. Its network stack receives the video data and may immediately send it via &ldquo;zero-copy&rdquo; or similar mechanisms directly to the GPU or media decoder for playback. For maximum performance, it clears this data from memory as soon as this operation is complete, assuming the data has been &ldquo;consumed.&rdquo;</li><li>When the Node.js event loop finally gets around to processing my <code>'response'</code> event, my <code>handleResponse</code> callback function begins to execute. When the code reaches <code>await response.body()</code>, it sends a command back to the browser via CDP: &ldquo;Please give me the data for that response you just told me about.&rdquo;</li><li>The browser replies: &ldquo;Sorry, the thing you&rsquo;re asking for? I already processed and discarded it. The data is gone.&rdquo; And thus, the <code>No data found for resource</code> error was thrown.</li></ol><p>The delay in this process might be mere milliseconds or even microseconds, but in the face of a high-performance browser kernel, that&rsquo;s more than enough time for the data to vanish. I realized that any form of &ldquo;post-mortem&rdquo; listening—any approach that tries to get the data <em>after</em> the response has already happened—was inherently at risk of failing.</p><p>To win this race, I needed a more proactive, more invasive method. I turned once again to <code>page.route</code>. This time, I wouldn&rsquo;t just use it to <code>abort()</code> requests; I would use it to completely &ldquo;proxy&rdquo; the video requests. My new plan was:</p><ol><li>Intercept an outgoing video request.</li><li><strong>Instead of letting the browser make the request</strong>, I would call <code>route.fetch()</code>, which instructs Playwright&rsquo;s backend to perform the request on the browser&rsquo;s behalf.</li><li>After getting the <code>APIResponse</code> object back from <code>route.fetch()</code>, I could safely <code>await response.body()</code> from it, because this was <em>my</em> request, and the data was fully preserved.</li><li>After capturing and storing the data in my <code>videoChunks</code> Map, I would then call <code>route.fulfill({ response })</code> to pass a &ldquo;forged&rdquo; copy of this response back to the browser, making the page&rsquo;s video player think everything was normal.</li></ol><p>This solution was, in theory, perfect. It transformed passive listening into active proxying, fundamentally solving the race condition. I refactored the code and ran it again, full of hope.</p><p>It worked! I successfully captured the first, and then the second, chunk of data! However, the joy was short-lived. After downloading a few chunks (about 9MB), the network requests stopped dead. The program timed out after waiting 20 seconds, leaving me with an incomplete file.</p><p>Why did the download chain break? Had my <code>route.fulfill()</code> call failed to properly &ldquo;deceive&rdquo; the player?</p><p>To find the truth, I added extremely verbose logging to my interceptor, printing the URL, method, type of every intercepted request, and the complete headers of every video response.</p><p>The logs quickly revealed the first stunning discovery: after the play button was clicked, the player initiated <strong>concurrent requests for multiple different video resolutions</strong>! For example, <code>720P.mp4</code>, <code>1440P.mp4</code>, and <code>2048P.mp4</code>. It seemed to be probing to determine the best bitrate for the current network speed.</p><p>My code had a critical flaw: I was using a single <code>activeVideoUrl</code> variable to lock onto the first video stream I downloaded. When the request for <code>720P.mp4</code> arrived first, my program &ldquo;decided&rdquo; this was the target and ruthlessly ignored all subsequent requests for <code>1440P</code> and <code>2048P</code>. However, the player, after its brief probing, may have ultimately decided to continue streaming the <code>2048P</code> version. Because my program didn&rsquo;t correctly <code>fulfill</code> the <code>2048P</code> requests (since it ignored them), the player&rsquo;s state machine was broken, and it ceased all subsequent requests.</p><p>I immediately corrected this logic. I stopped being &ldquo;first-come, first-served&rdquo; and instead maintained a list of all candidate streams. After the download finished, I would then select the &ldquo;best&rdquo; stream to assemble. My initial criterion for &ldquo;best&rdquo; was &ldquo;the one with the most chunks.&rdquo;</p><p>Yet, upon running it again, the problem persisted. The logs showed that every resolution stream was requested only for 2 chunks, and then they all stopped. My &ldquo;select the most chunks&rdquo; strategy degenerated into &ldquo;select the first one&rdquo; in this scenario, which was still wrong.</p><p>It was then that I finally grasped the true essence of the problem. It wasn&rsquo;t my selection logic that was flawed; it was that <strong><code>route.fulfill()</code> itself was the problem</strong>. The <code>APIResponse</code> object I got from <code>route.fetch()</code>, while containing the data, was ultimately a &ldquo;clone.&rdquo; When passed back to the browser via <code>route.fulfill()</code>, it may have lost certain low-level connection handles, timing information, or other metadata critical to the player. This &ldquo;imperfect&rdquo; response, while providing the initial few chunks of data, was enough to corrupt the player&rsquo;s delicate internal state, causing it to refuse to request subsequent segments.</p><h2 id=active-control-over-passive-listening>Active Control over Passive Listening<a hidden class=anchor aria-hidden=true href=#active-control-over-passive-listening>#</a></h2><p>After the failures of both the <code>page.on('response')</code> race condition and the <code>route.fulfill()</code> state corruption, I was at an impasse. I seemed to be caught in a dilemma: either be too slow, or be too intrusive.</p><p>It was at that moment that a completely new idea took shape in my mind. All my previous attempts had revolved around a central theme: <strong>how to &ldquo;steal&rdquo; data from requests initiated by the browser</strong>. Whether listening or proxying, I was a &ldquo;parasite&rdquo; attached to the browser&rsquo;s behavior.</p><p>Why not flip the script? Why couldn&rsquo;t <em>I</em> be the one driving the download process?</p><p>This idea completely transformed my architecture. The new, final solution was born, divided into two distinct phases: <strong>Reconnaissance</strong> and <strong>Takeover</strong>.</p><h3 id=reconnaissance>Reconnaissance<a hidden class=anchor aria-hidden=true href=#reconnaissance>#</a></h3><p>I still used <code>page.route</code> to intercept network requests. But this time, its purpose was incredibly simple: <strong>look, don&rsquo;t touch</strong>. When a video chunk request was intercepted, I did nothing but immediately call <code>route.continue()</code>, giving the browser an unobstructed path to conduct its own network communications. This ensured the player&rsquo;s state remained absolutely pure and continuous.
Simultaneously, in the background, I quietly used <code>page.waitForResponse()</code> to await the completion of the very request I had just allowed to pass. When the response arrived, I parsed from it all the information I needed: the full URL (including the dynamically generated token), the total video size, and the resolution. I stored this information in my <code>discoveredStreams</code> Map.
I set a 5-10 second reconnaissance window. During this time, the player would probe all the resolution streams as usual. My recon detector would gather information on all of them. At the end of the window, I would examine the <code>discoveredStreams</code> Map and select the stream with the highest resolution. With that, I had all the critical intelligence needed to download the full, high-quality video.</p><h3 id=takeover>Takeover<a hidden class=anchor aria-hidden=true href=#takeover>#</a></h3><p>With the recon mission complete, I no longer cared about the browser&rsquo;s subsequent network activities. I initiated a brand new download loop, one that I was in complete control of. In this loop, I would execute the most core and elegant operation of the entire solution: making the browser work for me.</p><p>I used the <code>page.evaluate()</code> function. This function can execute arbitrary JavaScript code within the context of the browser page. From my Node.js side, I calculated the range of the next chunk I wanted to download, for example, <code>bytes=0-5242879</code>, and then passed this range along with the high-resolution URL from the recon phase as arguments to <code>page.evaluate</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-typescript data-lang=typescript><span style=display:flex><span><span style=color:#75715e>// Core Snippet - Initiating a fetch request inside the browser
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>chunkData</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> <span style=color:#66d9ef>this</span>.<span style=color:#a6e22e>page</span>.<span style=color:#a6e22e>evaluate</span>(<span style=color:#66d9ef>async</span> ({ <span style=color:#a6e22e>url</span>, <span style=color:#a6e22e>range</span> }) <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>response</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> <span style=color:#a6e22e>fetch</span>(<span style=color:#a6e22e>url</span>, { <span style=color:#a6e22e>headers</span><span style=color:#f92672>:</span> { <span style=color:#e6db74>&#39;Range&#39;</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>range</span> } });
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span><span style=color:#a6e22e>response</span>.<span style=color:#a6e22e>ok</span>) <span style=color:#66d9ef>throw</span> <span style=color:#66d9ef>new</span> Error(<span style=color:#e6db74>`Fetch failed: </span><span style=color:#e6db74>${</span><span style=color:#a6e22e>response</span>.<span style=color:#a6e22e>status</span><span style=color:#e6db74>}</span><span style=color:#e6db74>`</span>);
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>buffer</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> <span style=color:#a6e22e>response</span>.<span style=color:#a6e22e>arrayBuffer</span>();
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> { <span style=color:#a6e22e>data</span>: <span style=color:#66d9ef>Array.from</span>(<span style=color:#66d9ef>new</span> <span style=color:#a6e22e>Uint8Array</span>(<span style=color:#a6e22e>buffer</span>)) };
</span></span><span style=display:flex><span>}, { <span style=color:#a6e22e>url</span>, <span style=color:#a6e22e>range</span> });
</span></span></code></pre></div><p>The magic of this code lies in the fact that <code>fetch(url, ...)</code> is executed <strong>inside the browser</strong>. This means the request:</p><ul><li>Automatically carries all the current page&rsquo;s cookies and session information.</li><li>Originates from the same IP address and browser fingerprint.</li><li>Has completely passed any Cloudflare or other JavaScript-based human verification checks the site might have deployed.</li></ul><p>It was a perfect, legitimate request originating from a real user session. The server could not distinguish it from the player&rsquo;s own requests.</p><p>The response from <code>fetch</code> is an <code>ArrayBuffer</code>, an object that cannot be directly passed from the browser back to Node.js. Therefore, I cleverly converted it into a plain JavaScript array of numbers using <code>Array.from(new Uint8Array(buffer))</code>, a data structure that can be serialized as JSON and passed across process boundaries.</p><p>Back in my Node.js environment, I received this array of numbers, converted it back into a Node.js <code>Buffer</code> object with <code>Buffer.from(chunkData.data)</code>, and then steadily wrote it to my file stream.</p><p>I repeated this process in a <code>for</code> loop, sequentially requesting the entire video file chunk by chunk, according to the size I defined, until every last byte was downloaded.</p><p>This solution proved to be unassailable. It completely sidestepped the race condition because it was no longer passively listening. It also entirely avoided interfering with the player&rsquo;s state, because it let the browser&rsquo;s business be the browser&rsquo;s, and the download&rsquo;s be mine. I was simply borrowing the browser&rsquo;s &ldquo;identity&rdquo; to make my own requests.</p><h2 id=polishing-the-product>Polishing the Product<a hidden class=anchor aria-hidden=true href=#polishing-the-product>#</a></h2><p>Having solved the core download challenge, I didn&rsquo;t stop there. A great engineer doesn&rsquo;t just solve problems; they deliver a great tool. I began to add a series of professional features to the script.</p><h3 id=command-line-argument-support>Command-Line Argument Support<a hidden class=anchor aria-hidden=true href=#command-line-argument-support>#</a></h3><p>I removed the hardcoded <code>TARGET_URL</code> and switched to using Node.js&rsquo;s <code>process.argv</code> to parse command-line arguments. Now, a user could simply append any video URL they wanted to download after <code>npm start</code>, vastly improving the tool&rsquo;s flexibility. I also added parameter checking; if no URL was provided, the program would print a usage hint and exit gracefully.</p><h3 id=rich-progress-display>Rich Progress Display<a hidden class=anchor aria-hidden=true href=#rich-progress-display>#</a></h3><p>A long download process without any feedback is a terrible user experience. I created a separate <code>DownloadTracker</code> helper class dedicated to handling download statistics. It records a timestamp at the start of the download and updates the number of downloaded bytes each time a data chunk is received.
To avoid spamming the console with frequent updates, I used a simple throttling technique: the progress information would update at most once per second. With each update, it would calculate:</p><ul><li><strong>Average Download Speed (Mbps)</strong>: <code>(Total Bytes Downloaded * 8) / Seconds Elapsed / 1024 / 1024</code></li><li><strong>Time Remaining (seconds)</strong>: <code>(Total Size - Bytes Downloaded) / Average Speed</code></li><li><strong>Estimated Time of Arrival (ETA)</strong>: <code>Current Time + Time Remaining</code>
I then formatted this information into a single, clear, continuously refreshing status line, such as:
<code>Progress: 25.13% | 216.05MB / 860.00MB | Speed: 123.45 Mbps | ETA: 19:30:45 (55s remaining)</code>
This provided excellent real-time feedback to the user.</li></ul><h3 id=download-retry-mechanism>Download Retry Mechanism<a hidden class=anchor aria-hidden=true href=#download-retry-mechanism>#</a></h3><p>Networks are unstable. During the download of a large file, the failure of a single chunk due to a temporary network glitch should not terminate the entire task. I added a <code>while</code> loop-based retry mechanism to the <code>downloadChunkWithRetries</code> logic.
If a <code>fetch</code> failed, it wouldn&rsquo;t give up immediately. It would enter a <code>catch</code> block where I would increment a retry counter and calculate a wait time using an &ldquo;exponential backoff&rdquo; strategy (e.g., wait 2s on the 1st retry, 4s on the 2nd, 8s on the 3rd&mldr;). After a short wait, it would attempt to download the <strong>exact same</strong> chunk again. Only after reaching the maximum number of retries (e.g., 5) would the program finally give up and abort the entire download. This mechanism dramatically enhanced the program&rsquo;s robustness in unstable network environments.</p><h3 id=intelligent-file-naming>Intelligent File Naming<a hidden class=anchor aria-hidden=true href=#intelligent-file-naming>#</a></h3><p>Finally, I implemented a feature you might have seen in another one of my projects: extracting a meaningful title from the page to use as the filename. I wrote an <code>extractVideoTitle</code> method that would search for specific elements on the page in order of priority (first <code>p[lang="ja"]</code>, then <code>h2.mt-16</code>). If neither was found, it would fall back to using a part of the URL. I also cleaned the extracted title, removing any characters that are illegal in filenames. In the end, the downloaded file was named <code>[Video Title]-[Resolution].mp4</code>, making it instantly identifiable.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://tategotoazarasi.github.io/en/tags/typescript/>Typescript</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/nodejs/>Nodejs</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/playwright/>Playwright</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/web-scraping/>Web-Scraping</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/automation/>Automation</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/video-downloader/>Video-Downloader</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/anti-scraping/>Anti-Scraping</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/race-condition/>Race-Condition</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/network-interception/>Network-Interception</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/browser-automation/>Browser-Automation</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/typescript-video-downloader/>Typescript-Video-Downloader</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/nodejs-playwright-tutorial/>Nodejs-Playwright-Tutorial</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/bypass-anti-scraping/>Bypass-Anti-Scraping</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/handle-dynamic-content/>Handle-Dynamic-Content</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/playwright-network-interception/>Playwright-Network-Interception</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/browser-automation-scripting/>Browser-Automation-Scripting</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/download-streaming-video/>Download-Streaming-Video</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/race-condition-in-web-scraping/>Race-Condition-in-Web-Scraping</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/page-evaluate-fetch/>Page-Evaluate-Fetch</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/robust-downloader-nodejs/>Robust-Downloader-Nodejs</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/typescript-project-from-scratch/>Typescript-Project-From-Scratch</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/debugging-playwright/>Debugging-Playwright</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/advanced-web-scraping-techniques/>Advanced-Web-Scraping-Techniques</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/playwright-vs-puppeteer/>Playwright-vs-Puppeteer</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/content-delivery-network-cdn-scraping/>Content-Delivery-Network-Cdn-Scraping</a></li></ul><nav class=paginav><a class=next href=https://tategotoazarasi.github.io/en/posts/uol-2025-wk3/><span class=title>Next »</span><br><span>Uol 2025 Wk3 && LeetCode Biweekly Contest 168 Solutions</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://tategotoazarasi.github.io/en/>Tategoto Azarasi</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>