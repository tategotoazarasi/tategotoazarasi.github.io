<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Analysis of HPC Matrix Multiplication Performance Benchmarking | Tategoto Azarasi</title>
<meta name=keywords content="hpc,matrix-multiplication,cuda,nvidia-v100,performance-engineering,cpp,python,mpi,nvshmem,avx-512,tensor-cores,numpy,cupy,jax,scientific-computing"><meta name=description content="This post analyzes matrix multiplication performance on Intel Xeon CPUs and NVIDIA V100 GPUs, comparing results across C++, OpenMP, CUDA, MPI, NVSHMEM, and Python frameworks like NumPy and CuPy."><meta name=author content="Tategoto Azarasi"><link rel=canonical href=https://blog.tategotoazarasi.me/en/posts/matrix-multiplication-benchmark-hpc-cuda-python/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=https://blog.tategotoazarasi.me/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://blog.tategotoazarasi.me/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://blog.tategotoazarasi.me/favicon-32x32.png><link rel=apple-touch-icon href=https://blog.tategotoazarasi.me/apple-touch-icon.png><link rel=mask-icon href=https://blog.tategotoazarasi.me/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://blog.tategotoazarasi.me/en/posts/matrix-multiplication-benchmark-hpc-cuda-python/><link rel=alternate hreflang=zh href=https://blog.tategotoazarasi.me/zh/posts/matrix-multiplication-benchmark-hpc-cuda-python/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link crossorigin=anonymous href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ rel=stylesheet><script crossorigin=anonymous defer integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js></script><script crossorigin=anonymous defer integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR onload=renderMathInElement(document.body) src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js></script>>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:url" content="https://blog.tategotoazarasi.me/en/posts/matrix-multiplication-benchmark-hpc-cuda-python/"><meta property="og:site_name" content="Tategoto Azarasi"><meta property="og:title" content="Analysis of HPC Matrix Multiplication Performance Benchmarking"><meta property="og:description" content="This post analyzes matrix multiplication performance on Intel Xeon CPUs and NVIDIA V100 GPUs, comparing results across C++, OpenMP, CUDA, MPI, NVSHMEM, and Python frameworks like NumPy and CuPy."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-11T02:15:47+00:00"><meta property="article:modified_time" content="2026-01-11T02:15:47+00:00"><meta property="article:tag" content="Hpc"><meta property="article:tag" content="Matrix-Multiplication"><meta property="article:tag" content="Cuda"><meta property="article:tag" content="Nvidia-V100"><meta property="article:tag" content="Performance-Engineering"><meta property="article:tag" content="Cpp"><meta name=twitter:card content="summary"><meta name=twitter:title content="Analysis of HPC Matrix Multiplication Performance Benchmarking"><meta name=twitter:description content="This post analyzes matrix multiplication performance on Intel Xeon CPUs and NVIDIA V100 GPUs, comparing results across C++, OpenMP, CUDA, MPI, NVSHMEM, and Python frameworks like NumPy and CuPy."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blog.tategotoazarasi.me/en/posts/"},{"@type":"ListItem","position":2,"name":"Analysis of HPC Matrix Multiplication Performance Benchmarking","item":"https://blog.tategotoazarasi.me/en/posts/matrix-multiplication-benchmark-hpc-cuda-python/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Analysis of HPC Matrix Multiplication Performance Benchmarking","name":"Analysis of HPC Matrix Multiplication Performance Benchmarking","description":"This post analyzes matrix multiplication performance on Intel Xeon CPUs and NVIDIA V100 GPUs, comparing results across C++, OpenMP, CUDA, MPI, NVSHMEM, and Python frameworks like NumPy and CuPy.","keywords":["hpc","matrix-multiplication","cuda","nvidia-v100","performance-engineering","cpp","python","mpi","nvshmem","avx-512","tensor-cores","numpy","cupy","jax","scientific-computing"],"articleBody":"This article presents a systematic test and analysis of matrix multiplication performance across various hardware and software implementations. Based on a high-performance computing node equipped with multi-core CPUs and multiple GPUs, I will start from the most basic algorithmic implementation and progressively examine performance differences across different layers: compiler optimization, specialized computing libraries, GPU parallel computing, and multi-device distributed computing. By comparing detailed performance data, we reveal how hardware characteristics and software implementations collectively influence final computational efficiency.\nBarkla2 HPC The hardware configuration of the computing node used in this test is as follows. The Central Processing Units (CPUs) consist of two Intel Xeon Gold 5218 processors, totaling 32 physical cores, with a base frequency of 2.30 GHz and Turbo Boost support. These CPUs support the AVX-512 vector instruction set and FMA (Fused Multiply-Add) operations, which are crucial for accelerating floating-point matrix operations. The memory system includes a three-level cache: each core has independent L1 and L2 caches (32KB and 1MB respectively), while all cores share a 23MB L3 cache. The system utilizes a NUMA (Non-Uniform Memory Access) architecture with two NUMA nodes; remote memory access latency is significantly higher than local memory access.\nThe Graphics Processing Units (GPUs) include four NVIDIA Tesla V100-PCIE-16GB cards. Each GPU features 5120 CUDA cores and 80 Streaming Multiprocessors (SMs). Notably, it includes 640 Tensor Cores, which provide extremely high throughput for half-precision (FP16) matrix operations. Each GPU is equipped with 16GB of HBM2 memory, offering approximately 900GB/s of bandwidth.\nThe interconnect topology between devices significantly impacts multi-GPU performance. Between GPU0 and GPU1, as well as GPU2 and GPU3, high-speed NVLink interconnects (labeled as NODE) are used, each associated with one of the two NUMA nodes. In contrast, GPU pairs located on different nodes (e.g., GPU0 and GPU2) are connected via slower PCIe and UPI paths (labeled as SYS). This architecture implies that in distributed computing, algorithms should minimize cross-node communication to optimize performance.\nNext is the benchmark implementation using C++ and CUDA. We start with the most basic implementation, gradually adding optimizations—first on the CPU, then transitioning to the GPU. This section demonstrates how hardware features translate into actual performance gains.\nThe methodology used for testing is critical. The benchmark pre-allocates a random matrix pool. Each timed iteration randomly selects a pair of matrices from the pool for calculation. This design helps prevent data from staying excessively in high-speed cache due to repeated use, thereby measuring computational throughput that is more representative of real-world scenarios.\nNaive Algorithm We first analyze the most basic matrix multiplication implementation: the triple-nested loop. This is the most direct translation of the algorithm.\nThe core code example is as follows:\nvoid cpu_naive_sgemm(int N, const float* A, const float* B, float* C) { for (int i = 0; i \u003c N; ++i) { for (int k = 0; k \u003c N; ++k) { float temp = A[i * N + k]; for (int j = 0; j \u003c N; ++j) { C[i * N + j] += temp * B[k * N + j]; } } } } The code adopts an (i, k, j) loop order. In the innermost j loop, the program accesses one row of matrix C and one row of matrix B sequentially. This memory access pattern is efficient as it aligns with how CPU caches prefetch contiguous data blocks (cache lines) from memory. Storing A[i * N + k] in a temporary variable temp helps the compiler keep it in a register, further increasing access speed.\nHowever, this implementation has fundamental limitations. First, it is entirely sequential, utilizing only a single CPU core. Second, it does not explicitly leverage vector instruction units like AVX-512. While advanced compilers may attempt auto-vectorization under aggressive optimization, the results are usually inferior to manual optimization.\nBenchmark results reflect these constraints. When the matrix size , the average execution time is approximately 214 microseconds, with performance around 20 GFLOPS. When increases to 1024, the execution time jumps to 185 milliseconds, and performance drops to about 11.6 GFLOPS. This performance degradation is a typical manifestation of the memory hierarchy: smaller matrices can mostly reside in the cache, while larger matrices (e.g., a 1024x1024 matrix occupying about 12MB) exceed cache capacity, forcing CPU cores to wait frequently for data from main memory. Consequently, testing skipped larger sizes of and 4096.\nThis naive implementation establishes a performance baseline, demonstrating the limits of a single-core CPU when facing memory access bottlenecks.\nOpenMP The primary bottleneck of the naive implementation is the use of only one CPU core. To utilize the other cores on the machine, we use OpenMP for parallelization. OpenMP uses compiler directives to guide the parallel execution of loops.\nThe code modification is minimal, primarily adding a single line before the outer loop:\nvoid cpu_omp_sgemm(int N, const float* A, const float* B, float* C) { #pragma omp parallel for for (int i = 0; i \u003c N; ++i) { // ... inner loops remain unchanged } } The #pragma omp parallel for directive creates a pool of threads (set to 32, the number of cores, via the OMP_NUM_THREADS environment variable) and distributes iterations of the following i loop among them. Since each thread is responsible for calculating different rows of the output matrix C, no data races occur.\nThe build script uses find_package(OpenMP REQUIRED) to configure the compilation environment, adding necessary flags (such as -fopenmp for GCC).\nPerformance improvement is significant. At , the average execution time decreased from 214 microseconds to 15.8 microseconds, a speedup of about 13.5x. At , time decreased from 185 milliseconds to 5.6 milliseconds, a speedup of roughly 33x, close to the ideal scaling for 32 cores. This indicates that for large-scale matrix multiplication, the computation can be effectively parallelized across multiple cores with relatively low thread management overhead.\nA key code fix is worth noting: an earlier version attempted to use the collapse(2) clause to parallelize the two outer loops, but this caused multiple threads to write to the same row of matrix C simultaneously, triggering data races. The fixed version parallelizes only the outermost i loop, ensuring each thread writes to independent rows, thus guaranteeing correctness.\nThe OpenMP version significantly boosts performance with minimal code changes, fully utilizing multi-core CPU power. However, it still relies on the compiler for vectorization of the inner loops.\nOpenMP+SIMD While OpenMP achieves multi-core parallelism, each core still primarily executes scalar operations. To further enhance performance, we must leverage SIMD (Single Instruction, Multiple Data) capabilities. The Xeon Gold 5218 CPU supports the AVX-512 instruction set, which can process 16 single-precision floats simultaneously. The AVX512_OMP benchmark uses AVX-512 intrinsics to explicitly control vectorization.\nThe code structure is similar to the OpenMP version, with the outer loop parallelized by OpenMP. The main difference is the innermost j loop, rewritten using AVX-512 instructions:\nvoid cpu_avx512_sgemm(int N, const float* A, const float* B, float* C) { #pragma omp parallel for for (int i = 0; i \u003c N; ++i) { for (int k = 0; k \u003c N; ++k) { __m512 a_val = _mm512_set1_ps(A[i * N + k]); for (int j = 0; j \u003c N; j += 16) { __m512 b_val = _mm512_loadu_ps(\u0026B[k * N + j]); __m512 c_val = _mm512_loadu_ps(\u0026C[i * N + j]); c_val = _mm512_fmadd_ps(a_val, b_val, c_val); _mm512_storeu_ps(\u0026C[i * N + j], c_val); } } } } The logic is as follows:\n__m512 is a 512-bit vector data type containing 16 floats. _mm512_set1_ps broadcasts a scalar value to all 16 positions in a vector. The inner loop increment is 16, as each AVX-512 instruction handles 16 floats. _mm512_loadu_ps loads 16 contiguous floats from memory into a vector register (u allows unaligned addresses). _mm512_fmadd_ps performs the fused multiply-add operation , the core of matrix multiplication. _mm512_storeu_ps stores the result vector back to memory. Performance results show that at , the OpenMP version took about 5.6 ms, and the AVX-512 version also took roughly 5.6 ms—virtually no difference. This indicates that when using -Ofast -march=native flags, the modern GCC compiler is already capable of auto-vectorizing the inner loop of the OpenMP version, generating code as efficient as manual AVX-512 intrinsics.\nThis outcome demonstrates that for well-structured loops, compilers can perform effective auto-vectorization. Manual intrinsics did not yield significant extra performance here; their primary value was verifying that compiler optimization had already reached the algorithmic limit on this CPU.\nOpenBLAS In actual high-performance computing, one rarely implements standard matrix multiplication from scratch but rather uses highly optimized libraries. BLAS (Basic Linear Algebra Subprograms) defines the standard interface for such computations, and OpenBLAS is a widely used open-source implementation.\nThe OpenBLAS benchmark replaces our previous triple loop with a single function call:\ncblas_sgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, N, N, N, 1.0f, A, N, B, N, 0.0f, C, N); Behind this single line are kernels meticulously optimized for various CPU architectures, often written in assembly and employing advanced optimization techniques:\nCache Blocking: Breaking matrices into blocks that fit into CPU cache levels (L1, L2, L3) to reduce data movement between memory and cache. Register Blocking: Further subdivision to maximize data usage within CPU registers, the fastest storage units. Prefetching: Using specific instructions to load data from memory to cache in advance, hiding memory latency. Loop Unrolling: Expanding inner loops to reduce control overhead and provide more instruction-level parallelism for the CPU’s out-of-order execution engine. Performance results clearly demonstrate the value of these optimizations. At , our best manual version (AVX-512+OpenMP) took about 5.6 ms, while OpenBLAS took only 1.46 ms—roughly 4x faster. At , OpenBLAS took about 113 ms, achieving a throughput of approximately 1.5 TFLOPS.\nThis shows that for practical applications, using an optimized library like OpenBLAS is usually the correct choice. It represents the performance limit for matrix multiplication on this specific CPU architecture.\nNaive CUDA After fully optimizing the multi-core CPU, we turned to the GPU for even higher performance. The first GPU benchmark is a “naive” CUDA kernel implementation. Like before, it is a direct translation of the algorithm but designed for the massive parallelism of the GPU. CUDA is NVIDIA’s parallel computing platform; we write “kernels” that are executed simultaneously by many threads.\nThe naive CUDA kernel code is as follows:\n__global__ void cuda_naive_kernel(int N, const float* A, const float* B, float* C) { int row = blockIdx.y * blockDim.y + threadIdx.y; int col = blockIdx.x * blockDim.x + threadIdx.x; if (row \u003c N \u0026\u0026 col \u003c N) { float sum = 0.0f; for (int k = 0; k \u003c N; ++k) { sum += A[row * N + k] * B[k * N + col]; } C[row * N + col] = sum; } } How it works:\nThe __global__ keyword identifies a kernel callable from the CPU. Each thread calculates a unique row and column index based on its block and thread IDs, making each thread responsible for one element in the output matrix C. Threads independently compute the dot product by looping through k, reading a row of A and a column of B. While simple and highly parallel, this method has significant memory efficiency issues. Consider threads in a block with the same row but different col values. For matrix B, as k changes, threads access contiguous memory addresses (B[k][col_0], B[k][col_1]). This “coalesced” access is efficient; the GPU can serve multiple thread requests with a single memory transaction.\nHowever, access to matrix A is inefficient. Threads with the same col but different row (threads in the same column) access addresses separated by elements (A[row_0][k], A[row_1][k]). This results in non-coalesced access, requiring the GPU to launch separate memory transactions for each thread in a warp. Furthermore, there is no data reuse; every thread repeatedly reads from global memory in every iteration without utilizing faster on-chip shared memory.\nDespite these efficiency issues, the GPU’s massive parallel capacity still delivers a major performance boost. At , the naive CUDA kernel execution time was approximately 992 microseconds. This is roughly 5.6x faster than our best multi-core CPU code (5.6 ms) and 1.5x faster than the highly optimized OpenBLAS (1.46 ms). It achieved a throughput of about 2.1 TFLOPS, surpassing all CPU results and proving that even an unoptimized GPU kernel can outperform CPUs in compute-intensive tasks.\ncuBLAS Similar to OpenBLAS on the CPU, the most effective way to perform standard linear algebra on NVIDIA GPUs is using cuBLAS. In the benchmark, we replaced the custom kernel with a library call.\nFor single-precision (FP32) matrix multiplication:\ncublasSgemm(cublas_handle, CUBLAS_OP_N, CUBLAS_OP_N, N, N, N, \u0026a, B, N, A, N, \u0026b, C, N); cuBLAS employs deeply optimized techniques. Its core is a tiling algorithm where threads in a block work together to load tiles of A and B from slow global memory into fast on-chip shared memory. All threads in the block then use this shared data to perform sub-matrix multiplications before loading the next tile. This design maximizes data reuse and significantly reduces global memory traffic. Additionally, cuBLAS kernels are written in low-level assembly (SASS) to precisely match the Volta architecture’s pipelines, instruction latencies, and register file size.\nPerformance results: At , cuBLAS FP32 execution time was about 215 microseconds—4.6x faster than the naive CUDA kernel and 6.8x faster than OpenBLAS on CPU. Performance reached about 10 TFLOPS, nearing the V100 GPU’s theoretical FP32 peak of 15.7 TFLOPS. At , it maintained high efficiency at roughly 13 TFLOPS.\nA key feature of the V100 is its Tensor Cores, which require half-precision (FP16) calculation. In the cuBLAS_FP16 benchmark, we made two changes: converting input/output matrices to __half (FP16) and enabling Tensor Core math mode:\ncublasSetMathMode(cublas_handle, CUBLAS_TENSOR_OP_MATH); Then, cublasHgemm was called.\nEnabling FP16 and Tensor Cores yielded massive gains. At , time dropped to 50 microseconds (4.3x faster than cuBLAS FP32), with performance exceeding 42 TFLOPS. At , performance reached 87 TFLOPS—a nearly 70x improvement over highly optimized OpenBLAS on a 32-core CPU. Tensor Cores provide order-of-magnitude performance leaps for dense matrix operations common in deep learning and scientific computing.\nMPI + cuBLAS To scale matrix multiplication across four GPUs, we use the industry-standard MPI (Message Passing Interface). In this benchmark, each GPU is treated as an independent compute unit managed by a separate CPU process—a “one process per GPU” model. This avoids the complexity of managing multiple GPUs within a single process.\nThe key initialization step is binding each MPI process to a specific GPU to prevent performance degradation from multiple processes fighting for the same card. We achieve this by calculating a local_rank:\nMPI_Init(\u0026argc, \u0026argv); MPI_Comm_rank(MPI_COMM_WORLD, \u0026mpi_rank); MPI_Comm_size(MPI_COMM_WORLD, \u0026mpi_size); // Create a communicator for processes on the same physical node MPI_Comm node_comm; MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, mpi_rank, MPI_INFO_NULL, \u0026node_comm); MPI_Comm_rank(node_comm, \u0026local_rank); // Bind current process to a specific GPU int num_devices; cudaGetDeviceCount(\u0026num_devices); CUDA_CHECK(cudaSetDevice(local_rank % num_devices)); The logic: MPI_Comm_split_type groups processes sharing memory (i.e., on the same server). Each process gets a local_rank (0, 1, 2, or 3), and cudaSetDevice(local_rank) binds it to the corresponding GPU.\nIn the MatrixPool struct, memory allocation uses standard CUDA calls because each GPU’s memory is independent in the MPI model:\nvoid* alloc_mem(size_t bytes) { void* ptr; CUDA_CHECK(cudaMalloc(\u0026ptr, bytes)); return ptr; } The core of the benchmark loop measures the time taken for four GPUs to calculate in parallel. In the MPI version, synchronization is handled by the CPU. We use MPI_Barrier to ensure all processes are ready before timing and again after calculation:\n// 1. Sync all processes (CPU waits here) MPI_Barrier(MPI_COMM_WORLD); double start = MPI_Wtime(); // 2. Asynchronously launch kernels on GPU if (use_fp16) { cublasHgemm(cublasH, CUBLAS_OP_N, CUBLAS_OP_N, ...); } else { cublasSgemm(cublasH, CUBLAS_OP_N, CUBLAS_OP_N, ...); } // 3. Wait for local GPU to finish CUDA_CHECK(cudaDeviceSynchronize()); // 4. Sync again to get the time of the slowest process MPI_Barrier(MPI_COMM_WORLD); double end = MPI_Wtime(); cublasSgemm or cublasHgemm are non-blocking calls. cudaDeviceSynchronize() makes the CPU wait for its local GPU, and the final MPI_Barrier ensures we measure the time taken by the last process to finish. This is necessary for distributed algorithms because the system’s speed is determined by the slowest component.\nData shows the MPI implementation reached 81.68 TFLOPS (FP16) at . This indicates that for massive matrix multiplication, the synchronization overhead of MPI_Barrier is negligible compared to the computation time, making this scheme highly efficient for compute-intensive tasks.\nNVSHMEM NVSHMEM provides a different parallel programming paradigm. Unlike MPI where independent processes send messages, NVSHMEM establishes a global address space. This allows a kernel running on GPU0 to directly access memory allocated on GPU1 via high-speed NVLink without CPU involvement.\nWe still use MPI to launch the environment but must initialize the NVSHMEM runtime to create a unified view of all GPU memories:\n// Initialize NVSHMEM using MPI to exchange setup info MPI_Comm comm_world = MPI_COMM_WORLD; nvshmemx_init_attr_t attr; attr.mpi_comm = \u0026comm_world; nvshmemx_init_attr(NVSHMEMX_INIT_WITH_MPI_COMM, \u0026attr); The most important difference is memory allocation. Instead of cudaMalloc, we use nvshmem_malloc to allocate from a “symmetric heap”:\nvoid* alloc_mem(size_t bytes) { void* ptr; // NVSHMEM allocation: this memory is now accessible by other GPUs ptr = nvshmem_malloc(bytes); return ptr; } A pointer ptr returned by nvshmem_malloc on GPU0 maps to local physical memory, but other GPUs also know how to access it. While no explicit data transfers (like nvshmem_put or nvshmem_get) were used in this specific benchmark timing loop, nvshmem_malloc ensures the memory layout is optimized for such operations.\nSynchronization moves from the CPU to the GPU (or NVSHMEM runtime) using nvshmem_barrier_all:\n// 1. Global barrier across all GPUs nvshmem_barrier_all(); double start = MPI_Wtime(); // 2. Compute if (use_fp16) { cublasHgemm(...); } // 3. Wait for local GPU CUDA_CHECK(cudaDeviceSynchronize()); // 4. Global barrier again nvshmem_barrier_all(); double end = MPI_Wtime(); Results show NVSHMEM FP16 reached 79.58 TFLOPS at , nearly identical to MPI’s 81.68 TFLOPS. This suggests that for purely compute-intensive tasks where no data is exchanged during calculation, the difference between MPI and NVSHMEM is minimal, as the bottleneck remains the computation itself.\nNext, we look at Python-based benchmarks. Though Python is often seen as slow, it achieves excellent performance in scientific computing by calling high-efficiency back-end libraries. For many researchers, Python offers faster development and cleaner code. We will analyze CPU-based NumPy, followed by GPU-based CuPy and JAX.\nNumPy NumPy is an essential library for scientific computing, providing multi-dimensional arrays and mathematical functions. Its performance relies on delegating compute-intensive operations (like matrix multiplication) to underlying optimized libraries (like OpenBLAS or Intel MKL), the same libraries used in our C++ benchmarks.\nThe Python code is extremely concise:\nimport numpy as np result = np.matmul(a, b) Behind the scenes, NumPy calls cblas_sgemm. Thus, benchmarking NumPy essentially benchmarks the linked BLAS library. The test script ensures full CPU core usage by setting environment variables like OPENBLAS_NUM_THREADS.\nThe script uses the pytest-benchmark framework and a DataPoolCache to pre-generate matrices. By preparing data (including GPU transfers) before timing begins, the run_one_round function measures only core computation time, ensuring a fair comparison with C++.\nAt , NumPy’s average execution time was 1.74 ms; at , it was 89.3 ms. This is very close to the C++ OpenBLAS results (1.46 ms and 98.8 ms respectively). Minor differences likely stem from environment configurations, but overall, it shows NumPy’s performance depends entirely on its BLAS library. Python overhead is negligible for long computations, justifying the “two-language” strategy: Python for high-level logic and C/C++ libraries for performance-critical math.\nCuPy CuPy is essentially “NumPy on the GPU.” It provides an interface nearly identical to NumPy but stores arrays (cupy.ndarray) in GPU memory and executes operations via CUDA. For matrix multiplication, CuPy automatically calls cuBLAS.\nThe workflow is straightforward:\nCreate NumPy arrays on the CPU. Transfer them to GPU using cupy.asarray(). Use cupy.matmul() for calculation. Example:\nimport cupy as cp # a_cpu and b_cpu are NumPy arrays a_gpu = cp.asarray(a_cpu) b_gpu = cp.asarray(b_cpu) # Timed part result_gpu = cp.matmul(a_gpu, b_gpu) # Block until GPU finishes cp.cuda.Stream.null.synchronize() The benchmark measures only cp.matmul, which wraps cublasSgemm or cublasHgemm.\nPerformance results for cupy_fp32 were similar to C++ cuBLAS:\nAt : Python/CuPy took 294 ; C++/cuBLAS took 215 . At : Python/CuPy took 10.49 ms; C++/cuBLAS took 10.56 ms. C++ is slightly faster for small matrices due to lower call overhead, but they are equal for large matrices. For FP16:\nAt : Python/CuPy took 135 ; C++/cuBLAS took 50 . At : Python/CuPy took 1.71 ms; C++/cuBLAS took 1.56 ms. CuPy reached approximately 80 TFLOPS at , close to the C++ version’s 87 TFLOPS, demonstrating its ability to effectively utilize Tensor Cores.\nJAX JAX, developed by Google, offers a NumPy-like API with a powerful function transformation system, most notably Just-In-Time (JIT) compilation. JAX doesn’t execute code immediately; it tracks operations to build a computation graph. The @jax.jit decorator sends this graph to the XLA (Accelerated Linear Algebra) compiler, which optimizes operations (like kernel fusion and memory layout) and generates efficient machine code (CUDA kernels).\nThe benchmark uses this correctly:\nimport jax # Compile once during initialization self._matmul_jit = jax.jit(self.jnp.matmul) # Call in the timing loop result = self._matmul_jit(a_jax, b_jax) result.block_until_ready() # Wait for completion JAX also supports multi-device computing through a PositionalSharding object. In the test, JAX automatically distributes work across two GPUs. While the sharding sharding.reshape((-1, 1)) distributes data along one dimension, jnp.matmul handles the parallel computation. This is significantly simpler than manual MPI setup.\nPerformance for JAX on 2 GPUs:\njax_fp32 at : 9.76 ms (~14 TFLOPS). jax_fp16 at : 4.19 ms (~32.7 TFLOPS). Comparing these:\nSingle-GPU CuPy FP32 at took 10.49 ms (13 TFLOPS). JAX on two GPUs was only slightly faster (9.76 ms), showing that throughput didn’t double—likely due to multi-GPU overhead or insufficient problem size for perfect scaling. For FP16, single-GPU CuPy (1.71 ms, 80 TFLOPS) was much faster than JAX on two GPUs (4.19 ms). This indicates that for this scale, JAX’s sharding and communication overhead outweighed the benefits of the second GPU. This highlights that “two GPUs are better than one” is not always true; performance depends on the interplay of algorithm, framework, scale, and topology. Summary The performance results are summarized in a comprehensive chart, aggregating data from naive C++ loops to multi-GPU distributed computing using TFLOPS as a unified metric.\nThe chart uses log-log coordinates, with matrix size (N) on the horizontal axis and performance (TFLOPS) on the vertical axis. This clearly shows performance changes across several orders of magnitude. FP16 results are dashed lines, and error bars indicate 95% confidence intervals. Reference lines show theoretical peaks for V100 (FP32/FP16) and a 4-GPU system.\nThe hierarchy is clear: at the bottom is C++ Naive (1xCPU) at under 0.1 TFLOPS. Above it are multi-core CPU implementations (OpenMP, AVX512, NumPy) clustered between 1-2 TFLOPS. C++ OpenBLAS sits slightly higher.\nGPU implementations occupy the upper regions. Even the C++ CUDA Naive kernel exceeds all CPU results, peaking over 2 TFLOPS. cuBLAS FP32 (C++ and CuPy) and 4-GPU MPI reach 10-15 TFLOPS, near the single-V100 FP32 peak. Python JAX FP32 (2xGPU) also resides here.\nThe top lines are FP16 implementations (cuBLAS, CuPy, MPI/NVSHMEM). These are significantly higher, with 4-GPU distributed computing reaching ~80 TFLOPS at , near the single-V100 FP16 peak. Python JAX FP16 (2xGPU) is notably lower than other FP16 implementations.\nThis chart synthesizes the performance gaps between simple implementations and hardware-optimized ones, reflecting the impact of compute platforms and precision choices on efficiency.\nWould you like me to generate a detailed summary table comparing the peak TFLOPS achieved by each implementation for the largest matrix size tested?\n","wordCount":"3873","inLanguage":"en","datePublished":"2026-01-11T02:15:47Z","dateModified":"2026-01-11T02:15:47Z","author":{"@type":"Person","name":"Tategoto Azarasi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.tategotoazarasi.me/en/posts/matrix-multiplication-benchmark-hpc-cuda-python/"},"publisher":{"@type":"Organization","name":"Tategoto Azarasi","logo":{"@type":"ImageObject","url":"https://blog.tategotoazarasi.me/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://blog.tategotoazarasi.me/en/ accesskey=h title="Tategoto Azarasi (Alt + H)">Tategoto Azarasi</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://blog.tategotoazarasi.me/zh/ title=中文 aria-label=中文>Zh</a></li></ul></div></div><ul id=menu><li><a href=https://blog.tategotoazarasi.me/en/ title=Home><span>Home</span></a></li><li><a href=https://blog.tategotoazarasi.me/en/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://blog.tategotoazarasi.me/en/>Home</a>&nbsp;»&nbsp;<a href=https://blog.tategotoazarasi.me/en/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Analysis of HPC Matrix Multiplication Performance Benchmarking</h1><div class=post-meta><span title='2026-01-11 02:15:47 +0000 UTC'>January 11, 2026</span>&nbsp;·&nbsp;19 min&nbsp;·&nbsp;3873 words&nbsp;·&nbsp;Tategoto Azarasi&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://blog.tategotoazarasi.me/zh/posts/matrix-multiplication-benchmark-hpc-cuda-python/>Zh</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ol><li><a href=#barkla2-hpc>Barkla2 HPC</a></li><li><a href=#naive-algorithm>Naive Algorithm</a></li><li><a href=#openmp>OpenMP</a></li><li><a href=#openmpsimd>OpenMP+SIMD</a></li><li><a href=#openblas>OpenBLAS</a></li><li><a href=#naive-cuda>Naive CUDA</a></li><li><a href=#cublas>cuBLAS</a></li><li><a href=#mpi--cublas>MPI + cuBLAS</a></li><li><a href=#nvshmem>NVSHMEM</a></li><li><a href=#numpy>NumPy</a></li><li><a href=#cupy>CuPy</a></li><li><a href=#jax>JAX</a></li><li><a href=#summary>Summary</a></li></ol></nav></div></details></div><div class=post-content><p>This article presents a systematic test and analysis of matrix multiplication performance across various hardware and software implementations. Based on a high-performance computing node equipped with multi-core CPUs and multiple GPUs, I will start from the most basic algorithmic implementation and progressively examine performance differences across different layers: compiler optimization, specialized computing libraries, GPU parallel computing, and multi-device distributed computing. By comparing detailed performance data, we reveal how hardware characteristics and software implementations collectively influence final computational efficiency.</p><h2 id=barkla2-hpc>Barkla2 HPC<a hidden class=anchor aria-hidden=true href=#barkla2-hpc>#</a></h2><p>The hardware configuration of the computing node used in this test is as follows. The Central Processing Units (CPUs) consist of two Intel Xeon Gold 5218 processors, totaling 32 physical cores, with a base frequency of 2.30 GHz and Turbo Boost support. These CPUs support the AVX-512 vector instruction set and FMA (Fused Multiply-Add) operations, which are crucial for accelerating floating-point matrix operations. The memory system includes a three-level cache: each core has independent L1 and L2 caches (32KB and 1MB respectively), while all cores share a 23MB L3 cache. The system utilizes a NUMA (Non-Uniform Memory Access) architecture with two NUMA nodes; remote memory access latency is significantly higher than local memory access.</p><p>The Graphics Processing Units (GPUs) include four NVIDIA Tesla V100-PCIE-16GB cards. Each GPU features 5120 CUDA cores and 80 Streaming Multiprocessors (SMs). Notably, it includes 640 Tensor Cores, which provide extremely high throughput for half-precision (FP16) matrix operations. Each GPU is equipped with 16GB of HBM2 memory, offering approximately 900GB/s of bandwidth.</p><p>The interconnect topology between devices significantly impacts multi-GPU performance. Between GPU0 and GPU1, as well as GPU2 and GPU3, high-speed NVLink interconnects (labeled as <code>NODE</code>) are used, each associated with one of the two NUMA nodes. In contrast, GPU pairs located on different nodes (e.g., GPU0 and GPU2) are connected via slower PCIe and UPI paths (labeled as <code>SYS</code>). This architecture implies that in distributed computing, algorithms should minimize cross-node communication to optimize performance.</p><p>Next is the benchmark implementation using C++ and CUDA. We start with the most basic implementation, gradually adding optimizations—first on the CPU, then transitioning to the GPU. This section demonstrates how hardware features translate into actual performance gains.</p><p>The methodology used for testing is critical. The benchmark pre-allocates a random matrix pool. Each timed iteration randomly selects a pair of matrices from the pool for calculation. This design helps prevent data from staying excessively in high-speed cache due to repeated use, thereby measuring computational throughput that is more representative of real-world scenarios.</p><h2 id=naive-algorithm>Naive Algorithm<a hidden class=anchor aria-hidden=true href=#naive-algorithm>#</a></h2><p>We first analyze the most basic matrix multiplication implementation: the triple-nested loop. This is the most direct translation of the algorithm.</p><p>The core code example is as follows:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#66d9ef>void</span> <span style=color:#a6e22e>cpu_naive_sgemm</span>(<span style=color:#66d9ef>int</span> N, <span style=color:#66d9ef>const</span> <span style=color:#66d9ef>float</span><span style=color:#f92672>*</span> A, <span style=color:#66d9ef>const</span> <span style=color:#66d9ef>float</span><span style=color:#f92672>*</span> B, <span style=color:#66d9ef>float</span><span style=color:#f92672>*</span> C) {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>int</span> i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; i <span style=color:#f92672>&lt;</span> N; <span style=color:#f92672>++</span>i) {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>int</span> k <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; k <span style=color:#f92672>&lt;</span> N; <span style=color:#f92672>++</span>k) {
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>float</span> temp <span style=color:#f92672>=</span> A[i <span style=color:#f92672>*</span> N <span style=color:#f92672>+</span> k];
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>int</span> j <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; j <span style=color:#f92672>&lt;</span> N; <span style=color:#f92672>++</span>j) {
</span></span><span style=display:flex><span>                C[i <span style=color:#f92672>*</span> N <span style=color:#f92672>+</span> j] <span style=color:#f92672>+=</span> temp <span style=color:#f92672>*</span> B[k <span style=color:#f92672>*</span> N <span style=color:#f92672>+</span> j];
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>The code adopts an <code>(i, k, j)</code> loop order. In the innermost <code>j</code> loop, the program accesses one row of matrix C and one row of matrix B sequentially. This memory access pattern is efficient as it aligns with how CPU caches prefetch contiguous data blocks (cache lines) from memory. Storing <code>A[i * N + k]</code> in a temporary variable <code>temp</code> helps the compiler keep it in a register, further increasing access speed.</p><p>However, this implementation has fundamental limitations. First, it is entirely sequential, utilizing only a single CPU core. Second, it does not explicitly leverage vector instruction units like AVX-512. While advanced compilers may attempt auto-vectorization under aggressive optimization, the results are usually inferior to manual optimization.</p><p>Benchmark results reflect these constraints. When the matrix size , the average execution time is approximately 214 microseconds, with performance around 20 GFLOPS. When increases to 1024, the execution time jumps to 185 milliseconds, and performance drops to about 11.6 GFLOPS. This performance degradation is a typical manifestation of the memory hierarchy: smaller matrices can mostly reside in the cache, while larger matrices (e.g., a 1024x1024 matrix occupying about 12MB) exceed cache capacity, forcing CPU cores to wait frequently for data from main memory. Consequently, testing skipped larger sizes of and 4096.</p><p>This naive implementation establishes a performance baseline, demonstrating the limits of a single-core CPU when facing memory access bottlenecks.</p><h2 id=openmp>OpenMP<a hidden class=anchor aria-hidden=true href=#openmp>#</a></h2><p>The primary bottleneck of the naive implementation is the use of only one CPU core. To utilize the other cores on the machine, we use OpenMP for parallelization. OpenMP uses compiler directives to guide the parallel execution of loops.</p><p>The code modification is minimal, primarily adding a single line before the outer loop:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#66d9ef>void</span> <span style=color:#a6e22e>cpu_omp_sgemm</span>(<span style=color:#66d9ef>int</span> N, <span style=color:#66d9ef>const</span> <span style=color:#66d9ef>float</span><span style=color:#f92672>*</span> A, <span style=color:#66d9ef>const</span> <span style=color:#66d9ef>float</span><span style=color:#f92672>*</span> B, <span style=color:#66d9ef>float</span><span style=color:#f92672>*</span> C) {
</span></span><span style=display:flex><span>    <span style=color:#75715e>#pragma omp parallel for
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>int</span> i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; i <span style=color:#f92672>&lt;</span> N; <span style=color:#f92672>++</span>i) {
</span></span><span style=display:flex><span>        <span style=color:#75715e>// ... inner loops remain unchanged
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>The <code>#pragma omp parallel for</code> directive creates a pool of threads (set to 32, the number of cores, via the <code>OMP_NUM_THREADS</code> environment variable) and distributes iterations of the following <code>i</code> loop among them. Since each thread is responsible for calculating different rows of the output matrix C, no data races occur.</p><p>The build script uses <code>find_package(OpenMP REQUIRED)</code> to configure the compilation environment, adding necessary flags (such as <code>-fopenmp</code> for GCC).</p><p>Performance improvement is significant. At , the average execution time decreased from 214 microseconds to 15.8 microseconds, a speedup of about 13.5x. At , time decreased from 185 milliseconds to 5.6 milliseconds, a speedup of roughly 33x, close to the ideal scaling for 32 cores. This indicates that for large-scale matrix multiplication, the computation can be effectively parallelized across multiple cores with relatively low thread management overhead.</p><p>A key code fix is worth noting: an earlier version attempted to use the <code>collapse(2)</code> clause to parallelize the two outer loops, but this caused multiple threads to write to the same row of matrix C simultaneously, triggering data races. The fixed version parallelizes only the outermost <code>i</code> loop, ensuring each thread writes to independent rows, thus guaranteeing correctness.</p><p>The OpenMP version significantly boosts performance with minimal code changes, fully utilizing multi-core CPU power. However, it still relies on the compiler for vectorization of the inner loops.</p><h2 id=openmpsimd>OpenMP+SIMD<a hidden class=anchor aria-hidden=true href=#openmpsimd>#</a></h2><p>While OpenMP achieves multi-core parallelism, each core still primarily executes scalar operations. To further enhance performance, we must leverage SIMD (Single Instruction, Multiple Data) capabilities. The Xeon Gold 5218 CPU supports the AVX-512 instruction set, which can process 16 single-precision floats simultaneously. The <code>AVX512_OMP</code> benchmark uses AVX-512 intrinsics to explicitly control vectorization.</p><p>The code structure is similar to the OpenMP version, with the outer loop parallelized by OpenMP. The main difference is the innermost <code>j</code> loop, rewritten using AVX-512 instructions:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#66d9ef>void</span> <span style=color:#a6e22e>cpu_avx512_sgemm</span>(<span style=color:#66d9ef>int</span> N, <span style=color:#66d9ef>const</span> <span style=color:#66d9ef>float</span><span style=color:#f92672>*</span> A, <span style=color:#66d9ef>const</span> <span style=color:#66d9ef>float</span><span style=color:#f92672>*</span> B, <span style=color:#66d9ef>float</span><span style=color:#f92672>*</span> C) {
</span></span><span style=display:flex><span>    <span style=color:#75715e>#pragma omp parallel for
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>int</span> i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; i <span style=color:#f92672>&lt;</span> N; <span style=color:#f92672>++</span>i) {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>int</span> k <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; k <span style=color:#f92672>&lt;</span> N; <span style=color:#f92672>++</span>k) {
</span></span><span style=display:flex><span>            __m512 a_val <span style=color:#f92672>=</span> _mm512_set1_ps(A[i <span style=color:#f92672>*</span> N <span style=color:#f92672>+</span> k]);
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>int</span> j <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; j <span style=color:#f92672>&lt;</span> N; j <span style=color:#f92672>+=</span> <span style=color:#ae81ff>16</span>) {
</span></span><span style=display:flex><span>                __m512 b_val <span style=color:#f92672>=</span> _mm512_loadu_ps(<span style=color:#f92672>&amp;</span>B[k <span style=color:#f92672>*</span> N <span style=color:#f92672>+</span> j]);
</span></span><span style=display:flex><span>                __m512 c_val <span style=color:#f92672>=</span> _mm512_loadu_ps(<span style=color:#f92672>&amp;</span>C[i <span style=color:#f92672>*</span> N <span style=color:#f92672>+</span> j]);
</span></span><span style=display:flex><span>                c_val <span style=color:#f92672>=</span> _mm512_fmadd_ps(a_val, b_val, c_val);
</span></span><span style=display:flex><span>                _mm512_storeu_ps(<span style=color:#f92672>&amp;</span>C[i <span style=color:#f92672>*</span> N <span style=color:#f92672>+</span> j], c_val);
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>The logic is as follows:</p><ul><li><code>__m512</code> is a 512-bit vector data type containing 16 floats.</li><li><code>_mm512_set1_ps</code> broadcasts a scalar value to all 16 positions in a vector.</li><li>The inner loop increment is 16, as each AVX-512 instruction handles 16 floats.</li><li><code>_mm512_loadu_ps</code> loads 16 contiguous floats from memory into a vector register (<code>u</code> allows unaligned addresses).</li><li><code>_mm512_fmadd_ps</code> performs the fused multiply-add operation , the core of matrix multiplication.</li><li><code>_mm512_storeu_ps</code> stores the result vector back to memory.</li></ul><p>Performance results show that at , the OpenMP version took about 5.6 ms, and the AVX-512 version also took roughly 5.6 ms—virtually no difference. This indicates that when using <code>-Ofast -march=native</code> flags, the modern GCC compiler is already capable of auto-vectorizing the inner loop of the OpenMP version, generating code as efficient as manual AVX-512 intrinsics.</p><p>This outcome demonstrates that for well-structured loops, compilers can perform effective auto-vectorization. Manual intrinsics did not yield significant extra performance here; their primary value was verifying that compiler optimization had already reached the algorithmic limit on this CPU.</p><h2 id=openblas>OpenBLAS<a hidden class=anchor aria-hidden=true href=#openblas>#</a></h2><p>In actual high-performance computing, one rarely implements standard matrix multiplication from scratch but rather uses highly optimized libraries. BLAS (Basic Linear Algebra Subprograms) defines the standard interface for such computations, and OpenBLAS is a widely used open-source implementation.</p><p>The OpenBLAS benchmark replaces our previous triple loop with a single function call:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span>cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, N, N, N,
</span></span><span style=display:flex><span>            <span style=color:#ae81ff>1.0f</span>, A, N, B, N, <span style=color:#ae81ff>0.0f</span>, C, N);
</span></span></code></pre></div><p>Behind this single line are kernels meticulously optimized for various CPU architectures, often written in assembly and employing advanced optimization techniques:</p><ul><li><strong>Cache Blocking:</strong> Breaking matrices into blocks that fit into CPU cache levels (L1, L2, L3) to reduce data movement between memory and cache.</li><li><strong>Register Blocking:</strong> Further subdivision to maximize data usage within CPU registers, the fastest storage units.</li><li><strong>Prefetching:</strong> Using specific instructions to load data from memory to cache in advance, hiding memory latency.</li><li><strong>Loop Unrolling:</strong> Expanding inner loops to reduce control overhead and provide more instruction-level parallelism for the CPU’s out-of-order execution engine.</li></ul><p>Performance results clearly demonstrate the value of these optimizations. At , our best manual version (AVX-512+OpenMP) took about 5.6 ms, while OpenBLAS took only 1.46 ms—roughly 4x faster. At , OpenBLAS took about 113 ms, achieving a throughput of approximately 1.5 TFLOPS.</p><p>This shows that for practical applications, using an optimized library like OpenBLAS is usually the correct choice. It represents the performance limit for matrix multiplication on this specific CPU architecture.</p><h2 id=naive-cuda>Naive CUDA<a hidden class=anchor aria-hidden=true href=#naive-cuda>#</a></h2><p>After fully optimizing the multi-core CPU, we turned to the GPU for even higher performance. The first GPU benchmark is a &ldquo;naive&rdquo; CUDA kernel implementation. Like before, it is a direct translation of the algorithm but designed for the massive parallelism of the GPU. CUDA is NVIDIA&rsquo;s parallel computing platform; we write &ldquo;kernels&rdquo; that are executed simultaneously by many threads.</p><p>The naive CUDA kernel code is as follows:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span>__global__ <span style=color:#66d9ef>void</span> <span style=color:#a6e22e>cuda_naive_kernel</span>(<span style=color:#66d9ef>int</span> N, <span style=color:#66d9ef>const</span> <span style=color:#66d9ef>float</span><span style=color:#f92672>*</span> A, <span style=color:#66d9ef>const</span> <span style=color:#66d9ef>float</span><span style=color:#f92672>*</span> B, <span style=color:#66d9ef>float</span><span style=color:#f92672>*</span> C) {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>int</span> row <span style=color:#f92672>=</span> blockIdx.y <span style=color:#f92672>*</span> blockDim.y <span style=color:#f92672>+</span> threadIdx.y;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>int</span> col <span style=color:#f92672>=</span> blockIdx.x <span style=color:#f92672>*</span> blockDim.x <span style=color:#f92672>+</span> threadIdx.x;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (row <span style=color:#f92672>&lt;</span> N <span style=color:#f92672>&amp;&amp;</span> col <span style=color:#f92672>&lt;</span> N) {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>float</span> sum <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0f</span>;
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>int</span> k <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; k <span style=color:#f92672>&lt;</span> N; <span style=color:#f92672>++</span>k) {
</span></span><span style=display:flex><span>            sum <span style=color:#f92672>+=</span> A[row <span style=color:#f92672>*</span> N <span style=color:#f92672>+</span> k] <span style=color:#f92672>*</span> B[k <span style=color:#f92672>*</span> N <span style=color:#f92672>+</span> col];
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>        C[row <span style=color:#f92672>*</span> N <span style=color:#f92672>+</span> col] <span style=color:#f92672>=</span> sum;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>How it works:</p><ul><li>The <code>__global__</code> keyword identifies a kernel callable from the CPU.</li><li>Each thread calculates a unique row and column index based on its block and thread IDs, making each thread responsible for one element in the output matrix C.</li><li>Threads independently compute the dot product by looping through <code>k</code>, reading a row of A and a column of B.</li></ul><p>While simple and highly parallel, this method has significant memory efficiency issues. Consider threads in a block with the same <code>row</code> but different <code>col</code> values. For matrix B, as <code>k</code> changes, threads access contiguous memory addresses (<code>B[k][col_0]</code>, <code>B[k][col_1]</code>). This &ldquo;coalesced&rdquo; access is efficient; the GPU can serve multiple thread requests with a single memory transaction.</p><p>However, access to matrix A is inefficient. Threads with the same <code>col</code> but different <code>row</code> (threads in the same column) access addresses separated by elements (<code>A[row_0][k]</code>, <code>A[row_1][k]</code>). This results in non-coalesced access, requiring the GPU to launch separate memory transactions for each thread in a warp. Furthermore, there is no data reuse; every thread repeatedly reads from global memory in every iteration without utilizing faster on-chip shared memory.</p><p>Despite these efficiency issues, the GPU&rsquo;s massive parallel capacity still delivers a major performance boost. At , the naive CUDA kernel execution time was approximately 992 microseconds. This is roughly 5.6x faster than our best multi-core CPU code (5.6 ms) and 1.5x faster than the highly optimized OpenBLAS (1.46 ms). It achieved a throughput of about 2.1 TFLOPS, surpassing all CPU results and proving that even an unoptimized GPU kernel can outperform CPUs in compute-intensive tasks.</p><h2 id=cublas>cuBLAS<a hidden class=anchor aria-hidden=true href=#cublas>#</a></h2><p>Similar to OpenBLAS on the CPU, the most effective way to perform standard linear algebra on NVIDIA GPUs is using cuBLAS. In the benchmark, we replaced the custom kernel with a library call.</p><p>For single-precision (FP32) matrix multiplication:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span>cublasSgemm(cublas_handle, CUBLAS_OP_N, CUBLAS_OP_N, N, N, N,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&amp;</span>a, B, N, A, N, <span style=color:#f92672>&amp;</span>b, C, N);
</span></span></code></pre></div><p>cuBLAS employs deeply optimized techniques. Its core is a tiling algorithm where threads in a block work together to load tiles of A and B from slow global memory into fast on-chip shared memory. All threads in the block then use this shared data to perform sub-matrix multiplications before loading the next tile. This design maximizes data reuse and significantly reduces global memory traffic. Additionally, cuBLAS kernels are written in low-level assembly (SASS) to precisely match the Volta architecture’s pipelines, instruction latencies, and register file size.</p><p>Performance results: At , cuBLAS FP32 execution time was about 215 microseconds—4.6x faster than the naive CUDA kernel and 6.8x faster than OpenBLAS on CPU. Performance reached about 10 TFLOPS, nearing the V100 GPU&rsquo;s theoretical FP32 peak of 15.7 TFLOPS. At , it maintained high efficiency at roughly 13 TFLOPS.</p><p>A key feature of the V100 is its Tensor Cores, which require half-precision (FP16) calculation. In the <code>cuBLAS_FP16</code> benchmark, we made two changes: converting input/output matrices to <code>__half</code> (FP16) and enabling Tensor Core math mode:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span>cublasSetMathMode(cublas_handle, CUBLAS_TENSOR_OP_MATH);
</span></span></code></pre></div><p>Then, <code>cublasHgemm</code> was called.</p><p>Enabling FP16 and Tensor Cores yielded massive gains. At , time dropped to 50 microseconds (4.3x faster than cuBLAS FP32), with performance exceeding 42 TFLOPS. At , performance reached 87 TFLOPS—a nearly 70x improvement over highly optimized OpenBLAS on a 32-core CPU. Tensor Cores provide order-of-magnitude performance leaps for dense matrix operations common in deep learning and scientific computing.</p><h2 id=mpi--cublas>MPI + cuBLAS<a hidden class=anchor aria-hidden=true href=#mpi--cublas>#</a></h2><p>To scale matrix multiplication across four GPUs, we use the industry-standard MPI (Message Passing Interface). In this benchmark, each GPU is treated as an independent compute unit managed by a separate CPU process—a &ldquo;one process per GPU&rdquo; model. This avoids the complexity of managing multiple GPUs within a single process.</p><p>The key initialization step is binding each MPI process to a specific GPU to prevent performance degradation from multiple processes fighting for the same card. We achieve this by calculating a <code>local_rank</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span>MPI_Init(<span style=color:#f92672>&amp;</span>argc, <span style=color:#f92672>&amp;</span>argv);
</span></span><span style=display:flex><span>MPI_Comm_rank(MPI_COMM_WORLD, <span style=color:#f92672>&amp;</span>mpi_rank);
</span></span><span style=display:flex><span>MPI_Comm_size(MPI_COMM_WORLD, <span style=color:#f92672>&amp;</span>mpi_size);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// Create a communicator for processes on the same physical node
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>MPI_Comm node_comm;
</span></span><span style=display:flex><span>MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, mpi_rank, MPI_INFO_NULL, <span style=color:#f92672>&amp;</span>node_comm);
</span></span><span style=display:flex><span>MPI_Comm_rank(node_comm, <span style=color:#f92672>&amp;</span>local_rank);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// Bind current process to a specific GPU
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>int</span> num_devices;
</span></span><span style=display:flex><span>cudaGetDeviceCount(<span style=color:#f92672>&amp;</span>num_devices);
</span></span><span style=display:flex><span>CUDA_CHECK(cudaSetDevice(local_rank <span style=color:#f92672>%</span> num_devices));
</span></span></code></pre></div><p>The logic: <code>MPI_Comm_split_type</code> groups processes sharing memory (i.e., on the same server). Each process gets a <code>local_rank</code> (0, 1, 2, or 3), and <code>cudaSetDevice(local_rank)</code> binds it to the corresponding GPU.</p><p>In the <code>MatrixPool</code> struct, memory allocation uses standard CUDA calls because each GPU’s memory is independent in the MPI model:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#66d9ef>void</span><span style=color:#f92672>*</span> <span style=color:#a6e22e>alloc_mem</span>(size_t bytes) {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>void</span><span style=color:#f92672>*</span> ptr;
</span></span><span style=display:flex><span>    CUDA_CHECK(cudaMalloc(<span style=color:#f92672>&amp;</span>ptr, bytes));
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> ptr;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>The core of the benchmark loop measures the time taken for four GPUs to calculate in parallel. In the MPI version, synchronization is handled by the CPU. We use <code>MPI_Barrier</code> to ensure all processes are ready before timing and again after calculation:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#75715e>// 1. Sync all processes (CPU waits here)
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>MPI_Barrier(MPI_COMM_WORLD);
</span></span><span style=display:flex><span><span style=color:#66d9ef>double</span> start <span style=color:#f92672>=</span> MPI_Wtime();
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// 2. Asynchronously launch kernels on GPU
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>if</span> (use_fp16) {
</span></span><span style=display:flex><span>    cublasHgemm(cublasH, CUBLAS_OP_N, CUBLAS_OP_N, ...);
</span></span><span style=display:flex><span>} <span style=color:#66d9ef>else</span> {
</span></span><span style=display:flex><span>    cublasSgemm(cublasH, CUBLAS_OP_N, CUBLAS_OP_N, ...);
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// 3. Wait for local GPU to finish
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>CUDA_CHECK(cudaDeviceSynchronize());
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// 4. Sync again to get the time of the slowest process
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>MPI_Barrier(MPI_COMM_WORLD);
</span></span><span style=display:flex><span><span style=color:#66d9ef>double</span> end <span style=color:#f92672>=</span> MPI_Wtime();
</span></span></code></pre></div><p><code>cublasSgemm</code> or <code>cublasHgemm</code> are non-blocking calls. <code>cudaDeviceSynchronize()</code> makes the CPU wait for its local GPU, and the final <code>MPI_Barrier</code> ensures we measure the time taken by the last process to finish. This is necessary for distributed algorithms because the system&rsquo;s speed is determined by the slowest component.</p><p>Data shows the MPI implementation reached 81.68 TFLOPS (FP16) at . This indicates that for massive matrix multiplication, the synchronization overhead of <code>MPI_Barrier</code> is negligible compared to the computation time, making this scheme highly efficient for compute-intensive tasks.</p><h2 id=nvshmem>NVSHMEM<a hidden class=anchor aria-hidden=true href=#nvshmem>#</a></h2><p>NVSHMEM provides a different parallel programming paradigm. Unlike MPI where independent processes send messages, NVSHMEM establishes a global address space. This allows a kernel running on GPU0 to directly access memory allocated on GPU1 via high-speed NVLink without CPU involvement.</p><p>We still use MPI to launch the environment but must initialize the NVSHMEM runtime to create a unified view of all GPU memories:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#75715e>// Initialize NVSHMEM using MPI to exchange setup info
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>MPI_Comm comm_world <span style=color:#f92672>=</span> MPI_COMM_WORLD;
</span></span><span style=display:flex><span>nvshmemx_init_attr_t attr;
</span></span><span style=display:flex><span>attr.mpi_comm <span style=color:#f92672>=</span> <span style=color:#f92672>&amp;</span>comm_world;
</span></span><span style=display:flex><span>nvshmemx_init_attr(NVSHMEMX_INIT_WITH_MPI_COMM, <span style=color:#f92672>&amp;</span>attr);
</span></span></code></pre></div><p>The most important difference is memory allocation. Instead of <code>cudaMalloc</code>, we use <code>nvshmem_malloc</code> to allocate from a &ldquo;symmetric heap&rdquo;:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#66d9ef>void</span><span style=color:#f92672>*</span> <span style=color:#a6e22e>alloc_mem</span>(size_t bytes) {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>void</span><span style=color:#f92672>*</span> ptr;
</span></span><span style=display:flex><span>    <span style=color:#75715e>// NVSHMEM allocation: this memory is now accessible by other GPUs
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    ptr <span style=color:#f92672>=</span> nvshmem_malloc(bytes);
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> ptr;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>A pointer <code>ptr</code> returned by <code>nvshmem_malloc</code> on GPU0 maps to local physical memory, but other GPUs also know how to access it. While no explicit data transfers (like <code>nvshmem_put</code> or <code>nvshmem_get</code>) were used in this specific benchmark timing loop, <code>nvshmem_malloc</code> ensures the memory layout is optimized for such operations.</p><p>Synchronization moves from the CPU to the GPU (or NVSHMEM runtime) using <code>nvshmem_barrier_all</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#75715e>// 1. Global barrier across all GPUs
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>nvshmem_barrier_all();
</span></span><span style=display:flex><span><span style=color:#66d9ef>double</span> start <span style=color:#f92672>=</span> MPI_Wtime();
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// 2. Compute
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>if</span> (use_fp16) {
</span></span><span style=display:flex><span>    cublasHgemm(...);
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// 3. Wait for local GPU
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>CUDA_CHECK(cudaDeviceSynchronize());
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// 4. Global barrier again
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>nvshmem_barrier_all();
</span></span><span style=display:flex><span><span style=color:#66d9ef>double</span> end <span style=color:#f92672>=</span> MPI_Wtime();
</span></span></code></pre></div><p>Results show NVSHMEM FP16 reached 79.58 TFLOPS at , nearly identical to MPI’s 81.68 TFLOPS. This suggests that for purely compute-intensive tasks where no data is exchanged during calculation, the difference between MPI and NVSHMEM is minimal, as the bottleneck remains the computation itself.</p><p>Next, we look at Python-based benchmarks. Though Python is often seen as slow, it achieves excellent performance in scientific computing by calling high-efficiency back-end libraries. For many researchers, Python offers faster development and cleaner code. We will analyze CPU-based NumPy, followed by GPU-based CuPy and JAX.</p><h2 id=numpy>NumPy<a hidden class=anchor aria-hidden=true href=#numpy>#</a></h2><p>NumPy is an essential library for scientific computing, providing multi-dimensional arrays and mathematical functions. Its performance relies on delegating compute-intensive operations (like matrix multiplication) to underlying optimized libraries (like OpenBLAS or Intel MKL), the same libraries used in our C++ benchmarks.</p><p>The Python code is extremely concise:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>result <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>matmul(a, b)
</span></span></code></pre></div><p>Behind the scenes, NumPy calls <code>cblas_sgemm</code>. Thus, benchmarking NumPy essentially benchmarks the linked BLAS library. The test script ensures full CPU core usage by setting environment variables like <code>OPENBLAS_NUM_THREADS</code>.</p><p>The script uses the <code>pytest-benchmark</code> framework and a <code>DataPoolCache</code> to pre-generate matrices. By preparing data (including GPU transfers) before timing begins, the <code>run_one_round</code> function measures only core computation time, ensuring a fair comparison with C++.</p><p>At , NumPy&rsquo;s average execution time was 1.74 ms; at , it was 89.3 ms. This is very close to the C++ OpenBLAS results (1.46 ms and 98.8 ms respectively). Minor differences likely stem from environment configurations, but overall, it shows NumPy&rsquo;s performance depends entirely on its BLAS library. Python overhead is negligible for long computations, justifying the &ldquo;two-language&rdquo; strategy: Python for high-level logic and C/C++ libraries for performance-critical math.</p><h2 id=cupy>CuPy<a hidden class=anchor aria-hidden=true href=#cupy>#</a></h2><p>CuPy is essentially &ldquo;NumPy on the GPU.&rdquo; It provides an interface nearly identical to NumPy but stores arrays (<code>cupy.ndarray</code>) in GPU memory and executes operations via CUDA. For matrix multiplication, CuPy automatically calls cuBLAS.</p><p>The workflow is straightforward:</p><ol><li>Create NumPy arrays on the CPU.</li><li>Transfer them to GPU using <code>cupy.asarray()</code>.</li><li>Use <code>cupy.matmul()</code> for calculation.</li></ol><p>Example:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> cupy <span style=color:#66d9ef>as</span> cp
</span></span><span style=display:flex><span><span style=color:#75715e># a_cpu and b_cpu are NumPy arrays</span>
</span></span><span style=display:flex><span>a_gpu <span style=color:#f92672>=</span> cp<span style=color:#f92672>.</span>asarray(a_cpu)
</span></span><span style=display:flex><span>b_gpu <span style=color:#f92672>=</span> cp<span style=color:#f92672>.</span>asarray(b_cpu)
</span></span><span style=display:flex><span><span style=color:#75715e># Timed part</span>
</span></span><span style=display:flex><span>result_gpu <span style=color:#f92672>=</span> cp<span style=color:#f92672>.</span>matmul(a_gpu, b_gpu)
</span></span><span style=display:flex><span><span style=color:#75715e># Block until GPU finishes</span>
</span></span><span style=display:flex><span>cp<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>Stream<span style=color:#f92672>.</span>null<span style=color:#f92672>.</span>synchronize()
</span></span></code></pre></div><p>The benchmark measures only <code>cp.matmul</code>, which wraps <code>cublasSgemm</code> or <code>cublasHgemm</code>.</p><p>Performance results for <code>cupy_fp32</code> were similar to C++ cuBLAS:</p><ul><li>At : Python/CuPy took 294 ; C++/cuBLAS took 215 .</li><li>At : Python/CuPy took 10.49 ms; C++/cuBLAS took 10.56 ms.</li></ul><p>C++ is slightly faster for small matrices due to lower call overhead, but they are equal for large matrices. For FP16:</p><ul><li>At : Python/CuPy took 135 ; C++/cuBLAS took 50 .</li><li>At : Python/CuPy took 1.71 ms; C++/cuBLAS took 1.56 ms.</li></ul><p>CuPy reached approximately 80 TFLOPS at , close to the C++ version&rsquo;s 87 TFLOPS, demonstrating its ability to effectively utilize Tensor Cores.</p><h2 id=jax>JAX<a hidden class=anchor aria-hidden=true href=#jax>#</a></h2><p>JAX, developed by Google, offers a NumPy-like API with a powerful function transformation system, most notably Just-In-Time (JIT) compilation. JAX doesn&rsquo;t execute code immediately; it tracks operations to build a computation graph. The <code>@jax.jit</code> decorator sends this graph to the XLA (Accelerated Linear Algebra) compiler, which optimizes operations (like kernel fusion and memory layout) and generates efficient machine code (CUDA kernels).</p><p>The benchmark uses this correctly:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> jax
</span></span><span style=display:flex><span><span style=color:#75715e># Compile once during initialization</span>
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>_matmul_jit <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>jit(self<span style=color:#f92672>.</span>jnp<span style=color:#f92672>.</span>matmul)
</span></span><span style=display:flex><span><span style=color:#75715e># Call in the timing loop</span>
</span></span><span style=display:flex><span>result <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>_matmul_jit(a_jax, b_jax)
</span></span><span style=display:flex><span>result<span style=color:#f92672>.</span>block_until_ready() <span style=color:#75715e># Wait for completion</span>
</span></span></code></pre></div><p>JAX also supports multi-device computing through a <code>PositionalSharding</code> object. In the test, JAX automatically distributes work across two GPUs. While the sharding <code>sharding.reshape((-1, 1))</code> distributes data along one dimension, <code>jnp.matmul</code> handles the parallel computation. This is significantly simpler than manual MPI setup.</p><p>Performance for JAX on 2 GPUs:</p><ul><li><code>jax_fp32</code> at : 9.76 ms (~14 TFLOPS).</li><li><code>jax_fp16</code> at : 4.19 ms (~32.7 TFLOPS).</li></ul><p>Comparing these:</p><ul><li>Single-GPU CuPy FP32 at took 10.49 ms (13 TFLOPS). JAX on two GPUs was only slightly faster (9.76 ms), showing that throughput didn&rsquo;t double—likely due to multi-GPU overhead or insufficient problem size for perfect scaling.</li><li>For FP16, single-GPU CuPy (1.71 ms, 80 TFLOPS) was much faster than JAX on two GPUs (4.19 ms). This indicates that for this scale, JAX&rsquo;s sharding and communication overhead outweighed the benefits of the second GPU. This highlights that &ldquo;two GPUs are better than one&rdquo; is not always true; performance depends on the interplay of algorithm, framework, scale, and topology.</li></ul><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><p><img loading=lazy src=/images/matmul_performance_comparison_final.png></p><p>The performance results are summarized in a comprehensive chart, aggregating data from naive C++ loops to multi-GPU distributed computing using TFLOPS as a unified metric.</p><p>The chart uses log-log coordinates, with matrix size (N) on the horizontal axis and performance (TFLOPS) on the vertical axis. This clearly shows performance changes across several orders of magnitude. FP16 results are dashed lines, and error bars indicate 95% confidence intervals. Reference lines show theoretical peaks for V100 (FP32/FP16) and a 4-GPU system.</p><p>The hierarchy is clear: at the bottom is <code>C++ Naive (1xCPU)</code> at under 0.1 TFLOPS. Above it are multi-core CPU implementations (<code>OpenMP</code>, <code>AVX512</code>, <code>NumPy</code>) clustered between 1-2 TFLOPS. <code>C++ OpenBLAS</code> sits slightly higher.</p><p>GPU implementations occupy the upper regions. Even the <code>C++ CUDA Naive</code> kernel exceeds all CPU results, peaking over 2 TFLOPS. <code>cuBLAS FP32</code> (C++ and CuPy) and 4-GPU MPI reach 10-15 TFLOPS, near the single-V100 FP32 peak. <code>Python JAX FP32 (2xGPU)</code> also resides here.</p><p>The top lines are FP16 implementations (<code>cuBLAS</code>, <code>CuPy</code>, <code>MPI/NVSHMEM</code>). These are significantly higher, with 4-GPU distributed computing reaching ~80 TFLOPS at , near the single-V100 FP16 peak. <code>Python JAX FP16 (2xGPU)</code> is notably lower than other FP16 implementations.</p><p>This chart synthesizes the performance gaps between simple implementations and hardware-optimized ones, reflecting the impact of compute platforms and precision choices on efficiency.</p><p>Would you like me to generate a detailed summary table comparing the peak TFLOPS achieved by each implementation for the largest matrix size tested?</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://blog.tategotoazarasi.me/en/tags/hpc/>Hpc</a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/matrix-multiplication/>Matrix-Multiplication</a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/cuda/>Cuda</a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/nvidia-v100/>Nvidia-V100</a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/performance-engineering/>Performance-Engineering</a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/cpp/>Cpp</a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/python/>Python</a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/mpi/>Mpi</a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/nvshmem/>Nvshmem</a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/avx-512/>Avx-512</a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/tensor-cores/>Tensor-Cores</a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/numpy/>Numpy</a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/cupy/>Cupy</a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/jax/>Jax</a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/scientific-computing/>Scientific-Computing</a></li></ul><nav class=paginav><a class=next href=https://blog.tategotoazarasi.me/en/posts/reproducing-retinasim-on-hpc-cluster/><span class=title>Next »</span><br><span>Reproducing RetinaSim on an HPC Cluster</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://blog.tategotoazarasi.me/en/>Tategoto Azarasi</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>