<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Matrix Multiplication Performance Benchmark: from Triple Loops to 100+ GFLOPS on AMD Ryzen AI + Radeon | Tategoto Azarasi</title>
<meta name=keywords content="matrix-multiplication,performance,benchmark,optimization,comparison,cpu,gpu,amd,ryzen-ai,radeon,gfx1150,avx,avx2,avx-512,fma,openmp,blas,eigen,opencv,opencl,clblast,hip,rocm,hipblas,vulkan,compute-shader,linear-algebra,high-performance-computing,hpc,parallel-computing,vectorization,simd,gpgpu,google-benchmark,c-plus-plus"><meta name=description content="An in-depth benchmark comparing the performance of 11 matrix multiplication implementations (Naive, CPU multi-core/SIMD/BLAS, GPU via OpenCL/HIP/Vulkan) on AMD Ryzen AI + Radeon, revealing vast performance gaps and optimization insights."><meta name=author content="Tategoto Azarasi"><link rel=canonical href=https://tategotoazarasi.github.io/en/posts/matrix-multiplication-performance-benchmark-from-triple-loops-to-100-plus-gflops-on-amd-ryzen-ai-radeon/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=https://tategotoazarasi.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tategotoazarasi.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tategotoazarasi.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://tategotoazarasi.github.io/apple-touch-icon.png><link rel=mask-icon href=https://tategotoazarasi.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://tategotoazarasi.github.io/en/posts/matrix-multiplication-performance-benchmark-from-triple-loops-to-100-plus-gflops-on-amd-ryzen-ai-radeon/><link rel=alternate hreflang=zh href=https://tategotoazarasi.github.io/zh/posts/matrix-multiplication-performance-benchmark-from-triple-loops-to-100-plus-gflops-on-amd-ryzen-ai-radeon/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link crossorigin=anonymous href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ rel=stylesheet><script crossorigin=anonymous defer integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js></script><script crossorigin=anonymous defer integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR onload=renderMathInElement(document.body) src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js></script>>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:url" content="https://tategotoazarasi.github.io/en/posts/matrix-multiplication-performance-benchmark-from-triple-loops-to-100-plus-gflops-on-amd-ryzen-ai-radeon/"><meta property="og:site_name" content="Tategoto Azarasi"><meta property="og:title" content="Matrix Multiplication Performance Benchmark: from Triple Loops to 100+ GFLOPS on AMD Ryzen AI + Radeon"><meta property="og:description" content="An in-depth benchmark comparing the performance of 11 matrix multiplication implementations (Naive, CPU multi-core/SIMD/BLAS, GPU via OpenCL/HIP/Vulkan) on AMD Ryzen AI + Radeon, revealing vast performance gaps and optimization insights."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-04-19T20:33:11+08:00"><meta property="article:modified_time" content="2025-04-19T20:33:11+08:00"><meta property="article:tag" content="Matrix-Multiplication"><meta property="article:tag" content="Performance"><meta property="article:tag" content="Benchmark"><meta property="article:tag" content="Optimization"><meta property="article:tag" content="Comparison"><meta property="article:tag" content="Cpu"><meta name=twitter:card content="summary"><meta name=twitter:title content="Matrix Multiplication Performance Benchmark: from Triple Loops to 100+ GFLOPS on AMD Ryzen AI + Radeon"><meta name=twitter:description content="An in-depth benchmark comparing the performance of 11 matrix multiplication implementations (Naive, CPU multi-core/SIMD/BLAS, GPU via OpenCL/HIP/Vulkan) on AMD Ryzen AI + Radeon, revealing vast performance gaps and optimization insights."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tategotoazarasi.github.io/en/posts/"},{"@type":"ListItem","position":2,"name":"Matrix Multiplication Performance Benchmark: from Triple Loops to 100+ GFLOPS on AMD Ryzen AI + Radeon","item":"https://tategotoazarasi.github.io/en/posts/matrix-multiplication-performance-benchmark-from-triple-loops-to-100-plus-gflops-on-amd-ryzen-ai-radeon/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Matrix Multiplication Performance Benchmark: from Triple Loops to 100+ GFLOPS on AMD Ryzen AI + Radeon","name":"Matrix Multiplication Performance Benchmark: from Triple Loops to 100\u002b GFLOPS on AMD Ryzen AI \u002b Radeon","description":"An in-depth benchmark comparing the performance of 11 matrix multiplication implementations (Naive, CPU multi-core/SIMD/BLAS, GPU via OpenCL/HIP/Vulkan) on AMD Ryzen AI + Radeon, revealing vast performance gaps and optimization insights.","keywords":["matrix-multiplication","performance","benchmark","optimization","comparison","cpu","gpu","amd","ryzen-ai","radeon","gfx1150","avx","avx2","avx-512","fma","openmp","blas","eigen","opencv","opencl","clblast","hip","rocm","hipblas","vulkan","compute-shader","linear-algebra","high-performance-computing","hpc","parallel-computing","vectorization","simd","gpgpu","google-benchmark","c-plus-plus"],"articleBody":"Today, let’s chat about a commonplace yet timeless topic – matrix multiplication. “Matrix multiplication? Learned that in university linear algebra, isn’t it just three for loops?” you might say. Indeed, the most basic implementation is exactly that, simple and direct. But in the world of high-performance computing, where every cycle counts, there’s a whole universe hidden behind those three nested loops. Different implementation methods can lead to performance differences that are worlds apart, sometimes by factors of hundreds or even thousands!\nSounds a bit exciting, doesn’t it? Like comparing the speed of an F1 race car to a mobility scooter. Why such a massive gap? Modern CPU and GPU architectures, compiler optimizations, parallel computing techniques, specialized math libraries… these are all critical factors influencing performance.\nTo get a firsthand feel for these differences, I recently conducted a matrix multiplication (square matrices, C = A * B) “performance showdown” on my new gear – a Lenovo ThinkBook 16 G7+ laptop equipped with an AMD Ryzen AI 9 365 processor (featuring integrated Radeon 880M graphics). We invited several “contenders” to the ring, covering a wide range of approaches: from the most naive implementation to methods leveraging CPU multi-cores, SIMD instruction sets, calling professional math libraries, and even harnessing GPU acceleration (using OpenCL, Vulkan Compute, and ROCm/HIP).\nThis blog post will walk you through the entire benchmarking process: from introducing the “race track” environment, dissecting the technical characteristics of each “contender,” to analyzing the final results and summarizing the takeaways. We’re not aiming for a stern academic paper, but rather a relaxed, natural discussion about the technical intricacies and the allure of performance optimization. Hopefully, this will provide some inspiration and satisfy your curiosity about high-performance computing.\nReady? Buckle up, let’s get started!\nHardware and Software Environment As the saying goes, “To do a good job, one must first sharpen one’s tools.” Before diving into the performance tests, let’s lay out the “tools of the trade,” meaning the hardware and software environment used for this benchmark. Understanding this background information will help us better interpret the subsequent performance data.\nMy core hardware configuration includes an AMD Ryzen AI 9 365 processor, belonging to family 26, model 36. This is a fairly new CPU, boasting 10 physical cores and supporting 20 threads, with a base frequency of 2.0 GHz. It features crucial AVX, AVX2, FMA, and, importantly, AVX-512 instruction set support (including various flavors like AVX512F, DQ, CD, BW, VL). While it also integrates an NPU (Neural Processing Unit), our tests primarily focus on its general-purpose CPU and GPU compute capabilities. For memory, the system is equipped with 27.2 GiB (approximately 32GB as reported by the system) of DDR5 RAM; memory size and speed are critical for the performance of large-scale matrix operations. The integrated graphics card is the AMD Radeon Graphics (Radeon 880M). According to information from rocminfo and vulkaninfo, its GPU model identifier is gfx1150 (sometimes shown as 11.5.0), featuring 12 Compute Units (CUs), each containing 2 SIMD units. It can reach a maximum clock frequency of 2900MHz and supports both FP16 and FP64 ( double-precision) computations. This integrated GPU supports Vulkan, OpenCL, and AMD’s ROCm/HIP platform, offering multiple avenues for GPU acceleration in our tests. It’s worth noting specifically that during the benchmark execution, I set the HSA_OVERRIDE_GFX_VERSION=11.5.1 environment variable. This might slightly influence the target code generation or runtime behavior for HIP or hipBLAS, a practice often employed because official rocblas support for gfx1150 wasn’t fully implemented at the time.\nOn the software side, I’m running Arch Linux, a rolling-release distribution, which keeps my software packages relatively up-to-date. The specific kernel version is 6.14.2-2-cachyos (64-bit); CachyOS is an Arch derivative often incorporating performance-enhancing patches. The desktop environment is KDE Plasma 6.3.4, operating on the Wayland display server protocol. For compilation, I primarily use GCC (g++), whose version varies with Arch Linux updates but certainly supports C++17/20 standards along with OpenMP and AVX/AVX-512 instructions. HIP code compilation relies on hipcc from the ROCm toolchain, which is based on Clang. Project building is managed by CMake (version 3.20 or higher).\nThe core libraries and drivers are key components for this benchmark. The ROCm platform needs to support the gfx1150 or gfx1151 GPU model; rocminfo output in the test logs indicates Runtime Version 1.1 and Extension Version 1.6. The OpenCL environment is slightly complex, with two platforms present: the AMD APP SDK (providing OpenCL 2.1, driver version 3635.0) and Mesa rusticl (providing OpenCL 3.0). However, based on the test log stating OpenCL Info: Selected AMD Platform. Using first GPU device. Device Name: gfx1151, we specifically selected the GPU device under the official AMD driver platform for testing, identified as gfx1151. For Vulkan, the instance version is 1.4.309, using the RADV driver (from Mesa 25.0.4), which identifies the device as AMD Radeon Graphics (RADV GFX1150). We utilized the glslc tool to compile GLSL compute shaders into SPIR-V format. The system also has a BLAS (Basic Linear Algebra Subprograms) implementation installed, likely OpenBLAS, a common high-performance choice on Linux distributions, successfully located by CMake’s find_package(BLAS). Additionally, the open-source OpenCL BLAS library, CLBlast, is installed and discoverable by CMake. Furthermore, we tested the popular C++ template library Eigen3 (version 3.3+, provided as header files) and the computer vision library OpenCV (version 4.x, with its core module correctly found by CMake).\nFinally, the entire benchmarking framework is Google Benchmark (v1.9.2). This is an industry-standard C++ benchmarking library offering convenient test fixture management, precise timing, automatic iteration count adjustment, and standardized result output, ensuring the rigor and reliability of our tests.\nTo squeeze out as much performance as possible, we employed some rather aggressive compilation options. For C++ code, we used the GCC (g++) compiler with the -Ofast optimization level, combined with the -march=native flag, allowing the compiler to generate the most optimized machine code based on the specific features of my native CPU (including its AVX-512 capabilities). We also explicitly added -mavx2 -mfma -mavx512f -mavx512dq flags to ensure these SIMD instructions could be utilized. For HIP code, we similarly used the -Ofast optimization option with hipcc (based on Clang). Moreover, CMAKE_HIP_ARCHITECTURES was set to gfx1150 via CMake (based on rocminfo findings) to guide the compiler in generating code for the target GPU architecture. OpenCL Kernel optimization differs; it’s specified not during host code compilation but at runtime via options passed to the clBuildProgram function. A commonly used optimization flag is -cl-fast-relaxed-math, which permits the OpenCL compiler to perform mathematical optimizations that might slightly affect floating-point precision but can significantly improve execution speed. Lastly, for Vulkan compute shaders, we also included the -O option when compiling them into SPIR-V format using the glslc tool, enabling compile-time optimization.\nWith this background set, let’s introduce the contenders and see what tricks they have up their sleeves.\nThe Contenders: Matrix Multiplication Implementations Detailed Next, we’ll introduce each matrix multiplication implementation method that participated in this performance showdown.\nNaive Implementation This contender is the one we’re most familiar with and the starting point for all optimizations. It strictly follows the definition of matrix multiplication, C[i][j] = Σ(A[i][k] * B[k][j]), using three nested loops:\n// Pseudo-code example for i = 0 to N-1: for j = 0 to N-1: sum = 0; for k = 0 to N-1: sum += A[i][k] * B[k][j]; // or A[i*N + k] * B[k*N + j] for row-major 1D array C[i][j] = sum; // or C[i*N + j] = sum The advantage of this naive implementation lies in its extreme simplicity and logical clarity, making it easy to understand. However, its disadvantage is extremely poor performance. This stems mainly from several factors. First, it’s Cache Unfriendly. During computation, access to the B matrix occurs column-wise (in the innermost k-loop, j is constant, k increments, accessing B[k*N + j]), but data is stored row-wise (Row-Major) in memory. This mismatch between access pattern and storage layout leads to frequent CPU cache line misses, requiring constant reloading from main memory and drastically reducing memory access efficiency. Accesses to matrix A (row-wise) and writes to matrix C (element-wise) are comparatively better for caching, but the B matrix access pattern becomes the performance killer. Second, this implementation is entirely serial, failing to utilize the valuable multi-core parallel processing capabilities of modern CPUs. Finally, it also makes no use of the CPU’s SIMD (Single Instruction, Multiple Data) units for vectorized computation; each operation handles only a single element’s multiplication and addition, resulting in low efficiency.\nThis one primarily serves as a performance baseline to see how much improvement other methods can offer.\nOpenMP (CPU Multi-core Parallelism) OpenMP is a parallel programming model based on shared memory, primarily using compiler directives (Pragmas) to guide the compiler in automatically generating parallel code. For loop-intensive tasks like matrix multiplication, it can easily distribute the outer loop (typically the i loop) across different CPU cores for execution.\nImplementation-wise, it merely involves adding a #pragma omp parallel for directive before the outer loop of the Naive version:\n#pragma omp parallel for default(none) shared(A, B, C, N) schedule(static) for (size_t i = 0; i \u003c N; ++i) { // Inner j and k loops remain unchanged for (size_t j = 0; j \u003c N; ++j) { ValueType sum = 0.0; for (size_t k = 0; k \u003c N; ++k) { sum += A[i * N + k] * B[k * N + j]; } C[i * N + j] = sum; } } Let’s break down the key parts of this OpenMP directive. parallel for is the core instruction, telling the compiler to parallelize the subsequent for loop. default(none) is a recommended good practice, forcing the programmer to explicitly declare the scope of each variable within the loop—either shared (shared) or thread-private (private)—to prevent potential errors. shared(A, B, C, N) declares that the matrices A, B, C, and the size N are shared among all concurrently executing threads; A and B are read-only during computation, while C is written to, but since OpenMP typically distributes work row-wise, different threads usually write to different rows of C, generally avoiding write conflicts. Finally, schedule(static) defines the work distribution strategy. It statically pre-divides the loop’s entire iteration space (here, the N iterations of i) into roughly equal chunks and assigns these chunks to the available threads. For well-load-balanced loops like matrix multiplication, static scheduling typically incurs low runtime overhead.\nThe primary advantage of using OpenMP is its implementation simplicity; often, just adding a single compiler directive ( Pragma) before a critical loop conveniently utilizes the CPU’s multi-core resources. Compared to the fully serial Naive implementation, performance usually sees a significant boost, ideally approaching a speedup factor close to the number of CPU cores, although the actual improvement is constrained by factors like memory bandwidth and cache efficiency. However, it also has drawbacks. First, it doesn’t resolve the cache unfriendliness issue present in the Naive version, particularly the column-wise access pattern for matrix B, which limits further performance gains. Second, its performance ceiling is inherently limited by the number of physical CPU cores and the system’s memory bandwidth. Furthermore, for very small matrix sizes (N), the overhead introduced by parallel computing (such as thread creation, management, and synchronization) might even outweigh the time saved by parallel execution, leading to performance degradation instead of improvement.\nCPU SIMD (AVX2/AVX-512 + FMA) SIMD (Single Instruction, Multiple Data) is a crucial feature of modern CPUs. It allows a single instruction to perform the same operation on multiple data elements simultaneously. For instance, AVX2 can process 4 double values at once ( using 256-bit registers), while AVX-512 can handle 8 double values (using 512-bit registers). FMA (Fused Multiply-Add) instructions further enhance efficiency, and potentially precision, by combining a multiplication and an addition into a single instruction.\nTo leverage SIMD, we typically need to use compiler-specific intrinsic functions. This makes the code considerably more complex than the Naive or OpenMP versions.\nAVX2 + FMA (256-bit) To utilize AVX2 and FMA instructions, we included the immintrin.h header file, which provides access to the necessary intrinsic functions. A key optimization strategy here involves changing the loop nesting order to i-k-j. The advantage of this order is that it allows for efficient vectorization within the innermost j loop. Specifically, for fixed i and k, we can first take the scalar value A[i][k] and broadcast it into all 4 double-precision elements of a 256-bit vector a_vec using the _mm256_set1_pd() intrinsic. Next, we load 4 consecutive double values from the k-th row of matrix B (starting at address \u0026B[k*N + j]) into a vector b_vec. Since matrix B is stored row-major, this consecutive load is generally cache-friendly. We opted for _mm256_loadu_pd(), which allows loading from unaligned memory addresses, offering more flexibility. Concurrently, we load the corresponding 4 partial sums from the i-th row of matrix C (address \u0026C[i*N + j]) into c_vec, also using _mm256_loadu_pd(). The core computational step involves executing the FMA (Fused Multiply-Add) operation, c_vec = a_vec * b_vec + c_vec, using the _mm256_fmadd_pd() intrinsic. This single instruction performs 4 pairs of multiplications and additions simultaneously. Finally, the updated result vector c_vec is written back to the corresponding location in matrix C using _mm256_storeu_pd(). Naturally, the implementation of the innermost j loop needs to iterate with a step size of 4 (the AVX2_DOUBLE_COUNT) and also requires special handling for any remaining elements at the end of the row (less than 4), which typically fall back to standard scalar computation.\n// Pseudo-code example (AVX2 + FMA) constexpr size_t AVX2_DOUBLE_COUNT = 4; for (size_t i = 0; i \u003c N; ++i) { for (size_t k = 0; k \u003c N; ++k) { __m256d a_vec = _mm256_set1_pd(A[i*N + k]); // Broadcast A[i][k] for (size_t j = 0; j \u003c N_aligned; j += AVX2_DOUBLE_COUNT) { // Aligned part __m256d b_vec = _mm256_loadu_pd(\u0026B[k*N + j]); // Load 4 doubles from B row k __m256d c_vec = _mm256_loadu_pd(\u0026C[i*N + j]); // Load 4 doubles from C row i c_vec = _mm256_fmadd_pd(a_vec, b_vec, c_vec); // Fused Multiply-Add _mm256_storeu_pd(\u0026C[i*N + j], c_vec); // Store back to C } // Handle remaining elements j = N_aligned to N-1 using scalar operations } } AVX-512 + FMA (512-bit) The implementation principle for AVX-512 + FMA is identical to the AVX2 version. The main difference lies in using 512-bit wide registers and their corresponding intrinsic functions, such as the __m512d type, _mm512_set1_pd, _mm512_loadu_pd, _mm512_fmadd_pd, and _mm512_storeu_pd. Because the registers are wider, the vector computation step size increases to 8 (AVX512_DOUBLE_COUNT), meaning a single instruction can now process 8 double-precision values. Successfully compiling and running AVX-512 code requires the CPU itself to support the instruction set (our Ryzen AI 9 365 processor meets this condition) and necessitates enabling these instructions via appropriate compiler options (like -mavx512f) during compilation.\nThis SIMD-based optimization approach offers significant advantages. Primarily, it can drastically improve the computational performance of a single CPU core. Additionally, employing the i-k-j loop order enhances the memory access pattern for matrix B, making it more cache-friendly. The core benefit comes from fully utilizing the powerful vector processing units within the CPU. However, this method also comes with notable disadvantages. Writing and maintaining code using SIMD intrinsics is considerably complex, and the resulting code suffers from poor portability as it directly depends on the specific instruction sets supported by the target CPU. Developers must also manually handle potential memory alignment issues (although loadu/storeu provide unaligned access, aligned loads/stores are generally faster) and manage the boundary conditions at the end of loops. Furthermore, historically, executing AVX-512 instructions could sometimes trigger the CPU to reduce its operating frequency to manage power consumption and heat generation; while this issue has been largely mitigated in modern CPUs, it remains a potential consideration.\nSIMD + OpenMP (AVX2/AVX-512 + FMA + OpenMP) Since OpenMP can parallelize the outer loop and SIMD can accelerate the inner computations, combining them seems like a powerful synergy. Indeed, it is.\nThe implementation simply involves adding the OpenMP parallel directive before the outer i loop of the SIMD (either AVX2 or AVX-512) version using the i-k-j loop order:\n#pragma omp parallel for default(none) shared(A, B, C, N, N_aligned) schedule(static) for (size_t i = 0; i \u003c N; ++i) { // Inner k and j (SIMD) loops remain unchanged for (size_t k = 0; k \u003c N; ++k) { // ... SIMD intrinsics code as before ... } } The primary advantage of combining SIMD instructions (be it AVX2 or AVX-512) with OpenMP multithreading is its ability to leverage both the CPU’s multi-core parallel processing power and its instruction-level parallelism (vectorization) simultaneously. This two-pronged approach often allows reaching, or at least closely approaching, the theoretical peak performance of the CPU for the given task. However, this method also has clear disadvantages. Firstly, it further compounds the code complexity, incorporating intricacies from both SIMD intrinsics programming and OpenMP parallel management. Secondly, as computation speed is pushed to its limits, the application’s performance bottleneck is very likely to shift from the computation itself to being limited by memory bandwidth – meaning the CPU cores can process data faster than the memory subsystem can supply it. Lastly, achieving optimal performance usually requires careful tuning of OpenMP-related parameters, such as selecting the most effective thread scheduling strategy (e.g., static, dynamic, guided via the schedule clause) and potentially employing advanced thread management techniques like thread affinity or load balancing adjustments.\nBLAS (Basic Linear Algebra Subprograms) BLAS isn’t a specific library but rather a standardized API specification defining interfaces for basic vector and matrix operations. Many organizations and companies provide implementations of BLAS. These libraries typically contain highly optimized C, Fortran, or even assembly code tailored for specific hardware (CPU architecture, cache sizes, SIMD instructions). They often internally implement sophisticated techniques like blocking (or tiling) to maximize cache utilization and automatically employ both SIMD instructions and multithreading.\nWe only need to call the standard C interface cblas_dgemm (’d’ for double precision, ‘gemm’ for general matrix-matrix multiplication):\n// Pseudo-code example cblas_dgemm( CblasRowMajor, // Tell BLAS our data is stored row by row CblasNoTrans, CblasNoTrans, // Neither A nor B needs transposing N, N, N, // M, N, K (for N x N matrices) 1.0, // alpha (for C = alpha*A*B + beta*C) A.data(), N, // Pointer to A data and its leading dimension (cols for RowMajor) B.data(), N, // Pointer to B data and its leading dimension 0.0, // beta (set to 0 to overwrite C, i.e., C = A*B) C.data(), N // Pointer to C data and its leading dimension ); Using a BLAS library for matrix multiplication offers several advantages. The most prominent is extreme ease of use; developers typically only need to call a single highly optimized library function (like cblas_dgemm) to perform the complex computation, significantly simplifying the programming effort. Secondly, because these libraries incorporate extensive hardware-specific optimizations, their performance is usually excellent, often approaching the theoretical peak computational throughput of the hardware. Furthermore, as BLAS is a standard interface, it provides good portability – code can generally run unmodified on any target platform that has a compliant BLAS library implementation. Calling a library function also results in very concise application code. Of course, using BLAS also has disadvantages. First, the application needs to be linked against the corresponding BLAS library file during the build process. Second, and most critically, the final performance achieved heavily depends on the quality of the specific BLAS implementation being used. Different BLAS libraries (like OpenBLAS, Intel MKL, ATLAS, etc.) can exhibit significant performance variations even on the same hardware.\nEigen \u0026 OpenCV Besides low-level interfaces like BLAS, many high-level C++ libraries also provide matrix operations. We tested two popular examples: Eigen and OpenCV.\nEigen Let’s take a look at the Eigen library. Its key characteristic is being a C++ template library renowned for its elegant API and powerful “Expression Templates” technology. This technique allows Eigen to analyze and optimize complex chains of linear algebra expressions at compile time, avoiding the creation of unnecessary intermediate temporary objects and often automatically generating SIMD instructions for the underlying computations. In terms of usage, Eigen code is also very concise. We can first use Eigen::Map to “map” our raw data stored in std::vector onto Eigen’s internal matrix object – this mapping itself incurs zero memory copy overhead. Then, we can directly use the overloaded * operator to perform the matrix multiplication, like so:\n// Pseudo-code example (Map existing data) Eigen::Map\u003cconst EigenMatrixType\u003e A_map(A.data(), N, N); Eigen::Map\u003cconst EigenMatrixType\u003e B_map(B.data(), N, N); EigenMatrixType C_eigen(N, N); // Eigen's result matrix matrix_multiply_eigen(A_map, B_map, C_eigen); // C_eigen.noalias() = A_map * B_map; It’s worth noting the use of the noalias() method in the code. This explicitly informs Eigen that the output matrix C does not overlap in memory with the input matrices A or B (no aliasing), enabling Eigen to employ more efficient and aggressive internal implementations for optimization.\nOverall, Eigen’s advantages include its very modern API, ease of use, and high code readability. Its ability to perform compile-time optimizations via C++ template metaprogramming is also a significant strength. However, it also has disadvantages. In terms of performance, it might not match specialized, deeply hand-optimized BLAS libraries (the final performance largely depends on the compiler’s optimization capabilities and the complexity of the specific expression). Additionally, due to its heavy reliance on templates, compile times can be relatively longer.\nOpenCV Next up is OpenCV. Its primary characteristic is being a comprehensive library mainly focused on computer vision tasks. However, its core module (core) also provides very powerful matrix operations centered around the cv::Mat class. cv::Mat can manage its own memory or conveniently “wrap” existing external data, avoiding unnecessary copies. An important advantage is that when performing computationally intensive operations like matrix multiplication, OpenCV typically attempts to leverage available underlying optimization mechanisms to accelerate the process. This might include Intel IPP (Integrated Performance Primitives), OpenMP multithreading, or potentially even calling a system-installed BLAS library. When using it, we can wrap the data from our std::vector into cv::Mat objects without copying, specifying the rows, columns, data type (CV_64F for double), and the data pointer. Then, we call the cv::gemm function provided by OpenCV to perform the matrix multiplication. This function’s interface is very similar to the gemm function in BLAS:\n// Pseudo-code example cv::Mat A_cv(N, N, CV_64F, A.data()); // Wrap existing data cv::Mat B_cv(N, N, CV_64F, B.data()); cv::Mat C_cv(N, N, CV_64F); // OpenCV result matrix matrix_multiply_opencv(A_cv, B_cv, C_cv); // cv::gemm(A_cv, B_cv, 1.0, cv::Mat(), 0.0, C_cv); OpenCV’s advantages lie in its extremely rich feature set, extending far beyond just matrix multiplication to cover a vast range of image processing and computer vision functionalities. If your project is already using OpenCV, employing it for matrix operations allows for seamless integration with other library features. Furthermore, it may leverage various backend optimization libraries to enhance performance. However, its disadvantages are also notable, primarily the fact that it introduces a relatively large and complex library dependency. If your task solely involves pure linear algebra computations, incorporating the entire OpenCV library might not be the most lightweight choice.\nOpenCL Now we turn to OpenCL (Open Computing Language), an open standard framework designed for cross-platform, heterogeneous parallel computing, allowing programs to utilize various compute devices including CPUs, GPUs, DSPs, and even FPGAs.\nThe typical workflow for computing with OpenCL is rather involved, encompassing multiple steps. First, one needs to query available OpenCL platforms (like the AMD APP SDK) and select a compute device from one of them (such as the gfx1151 GPU used in our tests). Next, a Context must be created; this acts as a container for managing the selected device(s) and associated resources like memory objects and command queues. Following that, a Command Queue is created for the chosen device, which serves as the conduit for submitting tasks (like memory transfers and kernel executions) to the device. The core data (matrices A, B, C) needs to reside in Memory Buffers on the device, created as cl_mem objects; this necessitates copying the input data A and B from host (CPU) memory to their respective device buffers. The computational task itself is defined in an OpenCL Kernel, typically written in a separate .cl file (like our matrix_mult.cl); this source code must be loaded, compiled (at which point optimization options like -cl-fast-relaxed-math can be passed), and built into an OpenCL Program object (cl_program). From this program object, the specific kernel function object (cl_kernel) to be executed is obtained. Before executing the kernel, its arguments must be set using clSetKernelArg, passing the device buffer objects (the cl_mem handles for A, B, C) and the matrix size N, among other potential parameters. Kernel execution is initiated by enqueuing the task onto the command queue using clEnqueueNDRangeKernel. This requires specifying the total number of global work-items (usually N* N, with each work-item calculating one element of C) and optionally, the local work-item size (the Workgroup size, e.g., 16x16, which impacts resource usage and performance). After the kernel finishes execution on the device, the results stored in the C buffer on the device must be copied back to host memory using clEnqueueReadBuffer. Finally, and crucially, all created OpenCL objects (kernel, program, buffers, queue, context) must be explicitly released to prevent resource leaks.\nRegarding the OpenCL Kernel code (matrix_mult.cl), it’s written in the OpenCL C language, a dialect based on the C99 standard with extensions for parallel computing. In our matrix multiplication kernel, each work-item (think of it as a lightweight thread) uses the built-in functions get_global_id(0) and get_global_id(1) to determine its unique coordinates (column col, row row) within the global N x N computation grid. Then, each work-item independently executes the inner loop over k to compute the dot product and store the result for C[row][col]. Since we’re using double as our data type, the kernel code needs the #pragma OPENCL EXTENSION cl_khr_fp64 : enable directive to explicitly enable support for double-precision floating-point numbers.\nThe main advantages of OpenCL are its theoretical cross-platform and cross-vendor compatibility and its ability to fully leverage the massive parallel compute power of GPUs and other accelerators. However, its disadvantages are also significant: the programming model is relatively complex, requiring developers to manually manage platforms, devices, contexts, memory, synchronization, and more, which often leads to verbose code. Furthermore, data must be explicitly transferred between the host and the device, introducing latency and bandwidth overhead that can negatively impact performance (become counterproductive), especially for computationally small tasks. Additionally, the actual performance of an OpenCL application can be sensitive to the quality of the specific vendor’s driver implementation.\nCLBlast CLBlast can be thought of as the BLAS implementation for the OpenCL ecosystem. Its design goal is to provide an API compatible with the traditional BLAS interface, but its internal computational logic is implemented using the OpenCL standard, enabling it to run on any GPU (or other accelerator) that supports OpenCL.\nIn terms of usage, invoking CLBlast is significantly simpler than manually writing and managing OpenCL kernels. First, you still need an initialized OpenCL environment, including a context and a command queue; we can directly reuse the global context g_clContext prepared for the pure OpenCL implementation. Next, OpenCL memory buffers need to be created for the input and output matrices, and the host data must be copied to the input buffers, just as in standard OpenCL. Once these prerequisites are met, the core step involves calling the CLBlast function clblast::Gemm(...) ( using the C++ template interface here, where ValueType automatically determines the precision). When calling this function, you need to pass arguments describing the matrix layout (row-major or column-major), whether the input matrices should be transposed, the matrix dimensions (M, N, K), the scalar values alpha and beta, pointers to the device-side OpenCL buffer objects, the leading dimension of each matrix (which is typically the number of columns for row-major storage), and the OpenCL command queue to use for execution. The CLBlast library then takes care of internally invoking its pre-compiled and optimized OpenCL kernels to perform the actual computation. After the computation is complete, the developer still needs to copy the results from the device-side C buffer back to host memory, similar to standard OpenCL practice.\nThe primary advantages of CLBlast are that it offers a standard BLAS interface, greatly simplifying the programming effort required for GPU-accelerated matrix operations using OpenCL. Furthermore, because the kernel functions within the CLBlast library are typically meticulously optimized by its developers (likely employing advanced techniques like sophisticated tiling, shared memory optimization, etc.), its performance is often superior to relatively simple OpenCL kernels written by application developers. However, it also has disadvantages. First, it relies on the target system having a correctly installed and configured OpenCL runtime environment as well as the CLBlast library itself. Second, like all GPU acceleration schemes based on a discrete memory model, it still involves the overhead of data transfer between the host and the device, which can become a performance bottleneck for small problems or in bandwidth-limited scenarios.\nVulkan Compute Next is Vulkan Compute. Vulkan itself was primarily designed as a next-generation, high-performance graphics rendering API, but it also incorporates powerful general-purpose computing (GPGPU) capabilities implemented via Compute Shaders.\nThe workflow for performing computations using Vulkan is arguably even more verbose and lower-level than OpenCL. Broadly, it involves the following sequence of steps: First is the initialization of a Vulkan Instance, followed by selecting a suitable Physical Device (usually the GPU), and then creating a Logical Device based on it, along with obtaining a Compute Queue for submitting computational tasks. The computation logic itself needs to be written in a compute shader (like our matrix_mult.comp), typically using the GLSL language. This shader must then be compiled into Vulkan’s standard intermediate representation, SPIR-V format (using a tool like glslc -O), and this SPIR-V code is loaded to create a Shader Module (VkShaderModule). For data storage, you must explicitly allocate Memory ( VkDeviceMemory) on the device and create Vulkan Buffers (VkBuffer) to hold the input matrices A, B, and the output matrix C. This process involves complex decisions regarding memory type selection, allocation, and binding buffers to the allocated memory. Copying data from the host (CPU) to these device buffers usually requires an intermediate, host-visible Staging Buffer. To allow the shader to access these buffer resources, Descriptors must be set up. This includes defining a Descriptor Set Layout (VkDescriptorSetLayout) to declare the resources the shader needs (e.g., three storage buffers), creating a Descriptor Pool (VkDescriptorPool) from which to allocate descriptor sets, allocating a specific Descriptor Set (VkDescriptorSet), and finally “connecting” or updating this descriptor set with the information about our created buffers. With the shader module and descriptors ready, the next step is to create the Compute Pipeline. This requires first creating a Pipeline Layout (VkPipelineLayout), which associates the descriptor set layouts used by the shader, and then creating the actual compute pipeline object (VkPipeline) based on this layout and the shader module. The actual commands are submitted via a Command Buffer. One must be allocated from a Command Pool (VkCommandPool). Then, you begin recording commands into it: first, you bind the compute pipeline and the descriptor set containing the resource information, and then you invoke vkCmdDispatch to launch the computation. vkCmdDispatch requires specifying the number of workgroups to launch, which usually needs to be calculated based on the matrix size N and the number of threads per workgroup defined in the shader (the local_size). Once command recording is complete, the command buffer is submitted to the previously obtained compute queue for execution. Since submission is asynchronous, Vulkan synchronization primitives like Fences or Semaphores must be used to wait for the GPU computation to finish. After completion, the results in the device’s C buffer need to be copied back to host memory, again likely using a staging buffer. The final step involves meticulously destroying all created Vulkan objects ( pipeline, layout, descriptors, pool, buffers, memory, device, instance, etc.) in the reverse order of creation to release resources properly.\nOur compute shader (matrix_mult.comp) is written in GLSL (OpenGL Shading Language). The layout (local_size_x = 16, local_size_y = 16) directive at the top defines that each workgroup consists of 16x16=256 work-items (threads). The layout(set = 0, binding = ...) specifications define how the shader accesses the buffers A, B, and C via binding points (0, 1, 2) within descriptor set 0. Inside the main function, the built-in variable gl_GlobalInvocationID.xy provides the global coordinates of the current work-item within the overall compute grid ( where id.x corresponds to the column and id.y to the row). The core computation logic, involving the loop over k to calculate the dot product for C[id.y * N + id.x], is very similar to the OpenCL kernel.\nThe advantages of using Vulkan Compute lie in it being a modern graphics API designed to reduce driver overhead on the CPU. If an application already requires graphics rendering, using Vulkan Compute allows for better integration with the rendering pipeline, potentially sharing resources and context. Vulkan also offers very fine-grained control over the hardware, enabling deep performance optimization. However, its disadvantages are quite prominent: the API is extremely verbose, and the initialization and setup processes are highly complex, leading to massive code overhead and comparatively lower development productivity. Vulkan’s primary design focus remains graphics rendering; although its compute capabilities are powerful, the ecosystem for general-purpose computing, including high-level library support and overall ease of use, might be considered somewhat less mature compared to OpenCL or NVIDIA’s CUDA / AMD’s HIP. And, just like OpenCL, the overhead of data transfer between host and device persists and needs careful management.\nHIP (Heterogeneous-Compute Interface for Portability) Now let’s discuss HIP (Heterogeneous-Compute Interface for Portability). HIP is an integral part of AMD’s ROCm (Radeon Open Compute) platform, designed to provide a C++ GPU programming model very similar to NVIDIA’s CUDA. One of its primary goals is to simplify the process of porting existing CUDA code to run on AMD GPUs.\nThe host-side (Host Code) workflow for GPU computing using HIP is considerably more concise compared to OpenCL and Vulkan, closely resembling the CUDA style. First, you need to allocate device memory for the input matrices A, B, and the output matrix C on the target GPU device using the hipMalloc() function. Then, data is transferred from host memory to device memory using hipMemcpy() (specifying hipMemcpyHostToDevice as the direction) for matrices A and B. The core computational task is initiated by launching the kernel function (matrix_multiply_hip_kernel) using a syntax very similar to CUDA’s \u003c\u003c\u003e\u003e notation, which specifies the kernel’s execution configuration. GridDim defines the number of thread blocks (analogous to OpenCL workgroups) to launch, while BlockDim defines the number of threads within each block (e.g., we might set it to 16x16). The grid dimensions usually need to be calculated based on the total matrix size N and the chosen block dimensions to ensure the entire computation is covered. Since kernel launches are asynchronous, the host code must call hipDeviceSynchronize() to wait for all computations on the GPU to complete. After computation finishes, the results from the C matrix in device memory are transferred back to host memory using hipMemcpy() (this time specifying hipMemcpyDeviceToHost). Finally, it’s crucial to release all device memory allocated earlier using the hipFree() function. Throughout this process, it’s recommended to use our defined HIP_CHECK() macro (which internally calls hipGetErrorString) to check the return value of every HIP API call for timely error detection and handling.\nThe HIP device-side code (in the matrix_mult_hip.hip file) is written using standard C++ syntax along with some HIP-specific extensions. Functions marked with the __global__ keyword are kernel functions that can be launched from the host using the \u003c\u003c\u003c...\u003e\u003e\u003e syntax. Inside the kernel function, built-in variables like blockIdx (index of the current block within the grid), threadIdx (index of the current thread within its block), and blockDim (dimensions of the block) are accessible. By combining these variables, we can calculate the global ID of the current thread ( corresponding to the row and col in the result matrix), similar to how global IDs are obtained in OpenCL/Vulkan ( e.g., via get_global_id or gl_GlobalInvocationID). The core computational logic of our matrix multiplication kernel (the inner loop over k) is essentially the same as the OpenCL and Vulkan kernels we saw earlier.\nOverall, HIP’s main advantages are its provision of a C++ interface, which is generally easier to use and learn compared to OpenCL’s C API or the extremely verbose Vulkan API. Its high degree of syntactic similarity to CUDA significantly facilitates porting existing CUDA codebases to the AMD platform. Being part of the ROCm platform, HIP is tightly integrated with AMD’s GPU drivers and toolchain (like the hipcc compiler), usually resulting in good performance and compatibility. However, HIP also has disadvantages. It primarily targets AMD GPUs (although the HIP Clang project provides some capability to run on certain NVIDIA GPUs, this isn’t its main focus). Using HIP requires installing the relatively large ROCm SDK. And, like all GPU computing solutions based on a discrete memory model, the overhead of data transfer between the host and device remains a performance factor to consider.\nhipBLAS Finally, we arrive at hipBLAS. You can think of it as the BLAS library within the HIP ecosystem, analogous to cuBLAS in the CUDA world or CLBlast in the OpenCL sphere. hipBLAS is the officially provided library from the ROCm platform, offering Basic Linear Algebra Subprograms accelerated using HIP technology for AMD GPUs.\nUsing hipBLAS follows a pattern similar to other GPU BLAS libraries and is simpler than writing raw HIP kernels. First, a functional HIP runtime environment is a prerequisite. Before using hipBLAS functions, you need to create a hipBLAS handle, an object managing the library’s internal state, via hipblasHandle_t handle; hipblasCreate(\u0026handle); for initialization. Memory management proceeds as with HIP kernels: use hipMalloc to allocate memory for A, B, and C on the GPU device, and use hipMemcpy to transfer host data to the device buffers for A and B. The core computation involves calling the hipblasDgemm() function (’d’ for double precision). Its parameter list closely resembles cblas_dgemm, with key differences being the need to pass the previously created hipBLAS handle and the fact that the pointers for A, B, and C must be device memory pointers. You also need to specify the operation for each matrix, e.g., whether it needs transposition (HIPBLAS_OP_N for no transpose). One crucial detail to pay attention to is that hipBLAS, like many traditional BLAS libraries, defaults to expecting data stored in column-major order. However, C++ developers typically work with row-major storage. If our inputs A and B are row-major, and we want to compute the row-major result C = A * B, calling hipblasDgemm directly requires careful handling of the data layout. A common trick is to leverage the mathematical identity CT = BT * AT. This involves telling hipblasDgemm to compute BT * AT (passing HIPBLAS_OP_T for both A and B), swapping the device pointers passed for A and B, swapping their leading dimensions (lda, ldb), and also swapping the matrix dimensions M and N. The resulting buffer computed this way is C transposed (CT stored in column-major order), which happens to have the exact same memory layout as C stored in row-major order. Alternatively, a more direct approach, if supported by your hipBLAS version, would be to check for an API function or setting that directly supports row-major inputs. However, for our matrix_multiply_hipblas implementation, we assume it internally handles the layout correctly (perhaps via the transpose trick or a newer interface) to provide behavior consistent with cblas_dgemm. Since the call executes asynchronously, it’s necessary to call hipDeviceSynchronize() to ensure the hipBLAS operation is completed and synchronized. Afterwards, use hipMemcpy to copy the result from the device C buffer back to the host. Finally, don’t forget to destroy the hipBLAS handle using hipblasDestroy(handle) to release its resources. As always, using the HIPBLAS_CHECK() macro to verify the status of each hipBLAS API call is recommended for robust error handling.\nThe primary advantages of hipBLAS are that it provides a standard BLAS interface, making high-performance linear algebra on AMD GPUs relatively easy to use. The library contains HIP kernels that are highly optimized by AMD specifically for their GPU architectures, thus usually delivering very high performance and effectively leveraging the hardware’s potential. Naturally, there are disadvantages too. Using hipBLAS depends on having the ROCm/HIP development environment and the hipBLAS library correctly installed. Like all GPU acceleration methods, the cost of data transfer between host and device remains. Furthermore, developers must pay close attention to handling the row-major versus column-major data layout issue to ensure correct function calls and parameter settings.\nAlright, all the contenders have been introduced. From simple serial loops to complex GPU programming, we’ve covered a spectrum of mainstream performance optimization ideas and technology stacks. Next up, let’s see how they actually performed in our benchmark tests!\nBenchmarking Methodology To ensure a fair comparison between these different implementations, we utilized the Google Benchmark framework. We also designed a specific test fixture, named MatrixMultFixture, to manage the setup and teardown tasks associated with each individual test run.\nDuring the test setup (SetUp) phase for each test case, the program first determines the current square matrix size N based on parameters passed by the Google Benchmark framework. It then allocates host (CPU) memory, typically using std::vector, for the input matrices A and B, as well as for an output matrix C intended to store results from CPU, SIMD, or some GPU implementations. Subsequently, matrices A and B are filled with random numbers. If the test involves the Eigen or OpenCV libraries, their respective specific result matrices (like C_eigen, C_cv) are also allocated at this stage. It’s important to note that for technologies requiring a persistent global context, such as OpenCL, Vulkan, and HIP, their initialization (e.g., via functions like initOpenCL, initVulkan) and final cleanup ( e.g., cleanupOpenCL) are performed once at the beginning and end of the entire benchmark program’s execution (within the main function), not within the per-test SetUp and TearDown. This avoids the significant overhead of repeatedly initializing and destroying these heavyweight contexts for every single test iteration.\nNext comes the test execution phase, driven by Google Benchmark’s macros. Each distinct matrix multiplication implementation corresponds to a separate Benchmark test function, for instance, BENCHMARK_F(MatrixMultFixture, BM_Naive) signifies the test for the Naive implementation. Inside each such test function, the core logic resides within a for (auto _ : state) loop controlled by Google Benchmark. Within this loop, we invoke the specific matrix multiplication function currently being tested, such as matrix_multiply_naive(A, B, C, N). The Google Benchmark framework intelligently and automatically adjusts the number of times this loop runs to ensure stable and reliable timing measurements are obtained. For libraries that necessitate data mapping or wrapping (like Eigen and OpenCV), the mapping (Eigen::Map) or wrapper object creation (cv::Mat) typically occurs inside this loop, but since these are usually zero-copy or low-overhead operations, their impact on the performance measurement is minimal. For the GPU-accelerated implementations (including OpenCL, Vulkan, HIP, CLBlast, hipBLAS), calling their respective execution functions usually encapsulates a sequence of operations: potentially creating (or reusing) device-side memory buffers, transferring input data from host to device (Host-to-Device), launching the computation kernel on the GPU, waiting for kernel execution to complete (synchronization), and finally transferring the computed results back from device to host (Device-to-Host).\nThe test cleanup (TearDown) phase is relatively straightforward, mainly involving the release of the host memory resources allocated during the SetUp phase, for example, by calling methods like A.clear(), B.clear(), C.clear(), and so forth.\nRegarding the test scope, we selected a range of N values for benchmarking, specifically 64, 128, 256, 512, and 1024. Choosing these powers of two is a common practice in benchmarking, as it helps in observing performance trends as the problem scale increases, particularly when plotted on logarithmic axes.\nIn terms of performance metrics, Google Benchmark primarily measures and reports real_time, which corresponds to the wall-clock time elapsed. Based on this measured time (typically in nanoseconds, ns) and the current matrix size N, we calculated a more informative core performance metric: GFLOPS (Giga Floating-point Operations Per Second). The formula used was GFLOPS = (2.0 * N^3) / (time_ns / 1e9). This calculation assumes that a standard square matrix multiplication requires 2 * N^3 floating-point operations (roughly N^3 multiplications and N^3 additions). All benchmark results were ultimately saved to a JSON formatted file named benchmark_results.json for convenient post-processing.\nFinally, for results visualization and easier comparison, we used Python along with the powerful data manipulation library pandas and the plotting library matplotlib. A script reads the generated JSON file, parses the data, calculates GFLOPS for each run, and then generates the performance comparison plot. In the plot, the X-axis represents the matrix size N (using a base-2 logarithmic scale to better show power-of-two relationships), and the Y-axis represents the performance in GFLOPS (also using a logarithmic scale to accommodate the vast differences in performance). This graphical representation allows us to see the performance gaps between different implementations and their respective scaling trends with problem size at a glance.\nNow, let’s see the final report card!\nPerformance Data Analysis Please take a look at the performance comparison chart plotted from the benchmark results:\nTo interpret this information-rich chart, let’s first examine the axes. The X-axis represents the matrix size N, spanning from 64 to 1024, and employs a base-2 logarithmic scale. The Y-axis denotes performance in GFLOPS (billions of floating-point operations per second) and also uses a logarithmic scale. The choice of logarithmic scales is crucial here; it helps to clearly display implementations with vastly different performance levels on the same graph and makes it easier to observe the relative performance trends as N changes. The legend on the right side lists all the implementation methods tested, along with their corresponding markers and colors, allowing easy identification of each line.\nLooking at the overall trends, several prominent patterns emerge. First, most implementations exhibit improved performance as the matrix size N increases, reflected by the generally upward slope of the curves. This is expected because for larger N, the total computational workload (which scales as O(N^3)) becomes much larger relative to fixed or slower-growing overheads (like function call costs, GPU data transfer latencies, thread startup times, etc.). This allows the benefits of parallelism and optimization to become more pronounced. Additionally, larger computational tasks are better at amortizing memory access latencies. Second, there’s an extremely wide range of performance across different implementations, differing by orders of magnitude. This is strikingly evident when comparing the lowest performer, the Naive implementation, to the top performer, hipBLAS (at N=1024). The performance gap exceeds 170,000 times! (Specifically, Naive at ~0.0006 GFLOPS vs. hipBLAS at ~102 GFLOPS). This dramatically underscores the importance and potential impact of optimization. Third, we observe that some curves tend to flatten out or even slightly decrease at larger values of N. This typically indicates that the performance of that implementation is hitting a bottleneck under the current conditions. Such bottlenecks could be varied, including saturated memory bandwidth (data can’t be supplied fast enough), insufficient CPU or GPU cache capacity for the working set causing lower cache hit rates, reaching the limit of GPU core utilization, or perhaps certain unoptimized overheads growing linearly or faster with N, starting to negate the computational speedup.\nTo analyze the performance data more deeply, we can group the implementations and compare them within and across groups.\nFirst, let’s look at the CPU Basic Group, comparing the simplest Naive implementation against the version using only OpenMP for parallelism. The Naive implementation (yellow ‘+’ marker) is undeniably the slowest. Its curve hugs the bottom of the chart on the log scale, showing very little growth with N, reaching only about 0.6 GFLOPS at N=1024 ( re-reading based on plot, correcting potential misinterpretation of raw data; GFLOPS derived from JSON). In contrast, the OpenMP version (orange square marker), leveraging the CPU’s 20 threads, shows a marked improvement, achieving around 4 GFLOPS at N=1024, roughly 6-7 times faster than Naive. Nevertheless, compared to more advanced optimization techniques, this is still quite slow. Its relatively flat performance curve suggests that simple multi-core parallelism might quickly become limited by factors like memory bandwidth.\nNext is the CPU SIMD Group, where we examine the impact of using AVX2 and AVX-512 instructions, both alone and combined with OpenMP. The single-threaded AVX2+FMA implementation (dark blue circle) already demonstrates the power of vectorization, delivering respectable performance (~1.7 GFLOPS at N=1024), even slightly outperforming the pure OpenMP version for N \u003c 512. Moving to AVX512+FMA (green triangle) yields further speedup, as the 512-bit vectors can process twice the data per instruction compared to AVX2, reaching about 2.4 GFLOPS at N=1024. The real performance leap occurs when combining SIMD with multi-threading. AVX2+FMA_OMP (red diamond) achieves roughly 9.5 GFLOPS at N=1024, more than 5 times faster than single-threaded AVX2 and over twice as fast as pure OpenMP. The champion within this group, and indeed the top performer among all CPU implementations tested, is AVX512+FMA_OMP (purple inverted triangle). By combining the widest available SIMD vectors with multi-core parallelism, it hits an impressive 15 GFLOPS at N=1024, about a 60% improvement over the AVX2+OMP version. Its line sits at the pinnacle of the CPU-only results.\nNow, let’s consider the CPU Professional Library Group, comparing BLAS, Eigen, and OpenCV. BLAS (purple ‘V’ marker) delivered excellent performance, reaching approximately 53 GFLOPS at N=1024 (correction based on re-reading the plot), nearly matching or slightly exceeding our best manually tuned CPU code (AVX512+FMA_OMP). This strongly indicates that the BLAS library installed on the system (likely OpenBLAS) is extremely well-optimized internally, effectively utilizing both SIMD instructions and multi-threading. Equally impressive was OpenCVLib (light blue circle), whose performance closely tracked BLAS, even slightly surpassing it at N=1024 with about 54 GFLOPS. This suggests that OpenCV’s gemm implementation benefits from powerful backend optimizations, possibly by calling an optimized BLAS library or another performance kernel library like IPP internally. However, EigenLib (pink star) showed surprisingly poor performance in this specific test, lagging behind even the basic OpenMP version and achieving only about 0.7 GFLOPS at N=1024. This contrasts sharply with Eigen’s generally high-performance reputation. Possible reasons for this anomaly could include suboptimal usage of Eigen in the test code (though unlikely if using standard operations), the compiler failing to adequately optimize Eigen’s expression templates for this specific case, or perhaps compatibility issues between the particular Eigen version and the test environment. Therefore, this result for Eigen should be viewed with caution and not generalized; it’s likely specific to the conditions of this benchmark.\nFinally, we examine the GPU Acceleration Group, comprising implementations using OpenCL, Vulkan, HIP, and the corresponding BLAS libraries CLBlast and hipBLAS. A general trend across all GPU methods is that their performance tends to be lower than well-optimized CPU methods (like BLAS or AVX+OMP) at smaller matrix sizes (e.g., N=64), sometimes even slower than Naive+OpenMP. This is primarily due to the overhead associated with GPU computing, namely the time spent transferring data between the CPU and GPU (Host-to-Device and Device-to-Host) and the latency involved in launching the GPU kernel. For small tasks, these fixed overheads constitute a large portion of the total execution time. However, as N increases, the massive parallel processing capability of the GPU dominates, and their performance curves rise rapidly, quickly surpassing all CPU-based implementations.\nAmong the manually written kernels (where we coded the computation logic in OpenCL C, GLSL, or HIP C++), OpenCL (cyan diamond) performed quite well, reaching about 58 GFLOPS at N=1024, with a steep curve indicating good scalability. Vulkan (green up-triangle) also delivered good performance, although slightly lower than OpenCL and the HIP kernel, at around 29 GFLOPS for N=1024. Given Vulkan’s API complexity, this result seems reasonable, possibly leaving room for further driver or shader optimization. The HIP kernel (gray ‘X’ marker) exhibited anomalously low performance at N=64 ( potentially due to measurement error or an initialization glitch), but for N=128 and larger, its performance quickly caught up and closely mirrored that of OpenCL, reaching about 57 GFLOPS at N=1024. This suggests that for this relatively simple kernel, the underlying execution efficiency of HIP and OpenCL on this particular AMD GPU is quite similar.\nPerformance took another significant jump when using the GPU BLAS libraries. CLBlast (brown diamond), being the OpenCL BLAS library, far outperformed our handwritten OpenCL kernel, achieving roughly 95 GFLOPS at N=1024. This highlights the value of specialized library optimizations; CLBlast likely employs more sophisticated techniques internally, such as advanced memory access patterns, data tiling, and efficient use of GPU shared memory (LDS). The undisputed overall winner of this entire benchmark was hipBLAS (red down-triangle). As the native BLAS library for AMD’s ROCm platform, it delivered the most outstanding performance, breaking the 100 GFLOPS barrier at N=1024 and reaching approximately 102 GFLOPS. This typically signifies that hipBLAS is best able to leverage the specific hardware features and instructions of the AMD GPU.\nLet’s briefly summarize the highlights and points of caution from this benchmark. The clear performance leaders at N=1024 were the GPU BLAS libraries, hipBLAS and CLBlast. Within the CPU realm, the system BLAS library, OpenCV, and the manually crafted AVX512+FMA+OMP implementation were the top contenders. The sheer magnitude of performance improvement observed was astounding: from the basic Naive method to the fastest hipBLAS implementation, the speedup at N=1024 exceeded a factor of 170,000! The advantage of using GPUs became evident for matrix sizes of N=256 and larger in our tests, with the gap widening as N increased. This also underscored the importance of using professional libraries like BLAS, CLBlast, hipBLAS, and even OpenCV, which often outperform manual optimization efforts (especially simpler custom GPU kernels) by encapsulating extensive hardware-specific tuning. On the cautionary side, the anomalously poor performance of Eigen in this specific test warrants further investigation and should not be taken as a general statement about Eigen’s capabilities. Similarly, the outlier result for the HIP kernel at N=64 suggests that this particular data point might be invalid and should be treated carefully.\nIn essence, this performance showdown vividly illustrates the vast differences that various technological approaches can make. From elementary CPU loops to intricate GPU programming, every optimization technique has its rationale and optimal use case.\nDeeper Dive: Discussion \u0026 Caveats While this performance benchmark provides us with a wealth of direct data, it also prompts further reflection and requires acknowledging certain limitations and important considerations when interpreting the results.\nFirst and foremost, the results are highly hardware-dependent. All tests were conducted on a specific platform featuring an AMD Ryzen AI 9 processor paired with a Radeon 880M integrated GPU. Running the same benchmarks on different hardware, such as an Intel CPU or an NVIDIA GPU, could yield dramatically different outcomes and performance rankings. For example, Intel CPUs often show exceptional performance when coupled with Intel’s own MKL (Math Kernel Library), while NVIDIA GPUs would necessitate the use of the CUDA programming model and the cuBLAS library to achieve their best results.\nSecond, the choice of compiler and library versions can significantly influence the outcome. The specific version of GCC or Clang used, the selected optimization flags (e.g., -Ofast versus -O3 might trade precision or standard conformance for speed), and the particular version and build configuration of mathematical libraries like BLAS, OpenCV, or Eigen can all impact the final performance numbers. For instance, substituting OpenBLAS with MKL on an Intel CPU could lead to completely different BLAS performance results.\nFurthermore, the data type and matrix characteristics are crucial factors. This benchmark exclusively used double ( 64-bit double-precision floating-point numbers) for square matrices. If we were to switch to float (32-bit single-precision), performance would generally be higher due to halved data volume reducing memory bandwidth pressure, SIMD instructions processing twice as many elements per operation, and some hardware intrinsically favoring single-precision computations. Additionally, our tests focused on dense square matrices. For matrices with special structures like sparsity, symmetry, or bandedness, employing specialized storage formats, algorithms, and dedicated libraries is essential for efficient computation.\nMoreover, GFLOPS isn’t the whole story. While GFLOPS is a vital metric for gauging raw computational throughput, it doesn’t capture the full picture of real-world application performance. Especially in the context of GPU computing, the time spent transferring data between the host (CPU) and the device (GPU) – operations like hipMemcpy or clEnqueueWrite/ReadBuffer – constitutes an integral part of the total task duration. Our benchmark, likely focusing on the time spent within the Google Benchmark loop, might primarily measure the core computation time and potentially underrepresent or exclude the full data transfer overhead. In practical applications, the end-to-end execution time is what truly matters. For small matrix problems, this data transfer overhead can even dominate the overall time.\nWe must also consider the trade-offs between implementation complexity and ease of use. The highest-performing solutions, such as hipBLAS or CLBlast, while relatively simple to use (calling library functions), rely on the user having the specific SDKs (like ROCm) and environments correctly installed and configured. On the other hand, manually writing SIMD intrinsics or GPU kernel code (for OpenCL, Vulkan, or HIP) might offer finer control over performance but demands deep expertise in low-level hardware details and parallel programming, often involving significant development, debugging, and optimization effort. The Naive and OpenMP approaches are the simplest to implement but yield the poorest performance. Therefore, selecting the right implementation method for a real-world project requires careful balancing between performance requirements, development costs, code portability, and long-term maintainability.\nIt’s also worth acknowledging that regarding cache optimization, the CPU SIMD and GPU kernels (OpenCL/Vulkan/HIP) that we manually implemented were relatively basic and did not incorporate sophisticated data blocking (or tiling) strategies. Blocking is an advanced optimization technique that involves partitioning large matrices into smaller sub-matrices (blocks) and performing computations block-wise. Its main goal is to maximize the utilization of CPU or GPU caches by improving data locality and cache hit rates. This technique is one of the core reasons why high-performance BLAS libraries achieve near-peak hardware performance. If we were to implement complex blocking in our manual code, their performance might improve further, but at the cost of a dramatic increase in code complexity.\nFinally, the anomalous results observed for the Eigen library and the HIP kernel at N=64 serve as a reminder that benchmark results should always be interpreted critically. When encountering data that starkly contradicts expectations, one should resist jumping to immediate conclusions and instead try to investigate potential causes – could it be a bug in the code, an issue with compilation flags, measurement inaccuracies, interference from other system processes, or perhaps a compatibility problem specific to the test environment? Only through careful scrutiny and validation can we gain confidence in the benchmark findings.\nThe Finish Line: Conclusion \u0026 Outlook Having journeyed through this comprehensive matrix multiplication performance showdown—spanning CPUs and GPUs, serial and parallel approaches, manual optimizations, and professional libraries—we can draw several clear conclusions.\nFirst and foremost, optimization is absolutely crucial. The chasm in performance between the most basic Naive implementation and highly optimized solutions is immense, vividly demonstrating that for compute-intensive tasks, selecting the right algorithms and implementation techniques is paramount for achieving acceptable, let alone excellent, performance. Second, leveraging hardware features yields significant rewards. Utilizing modern CPU capabilities like multi-core processing (e.g., via OpenMP) and SIMD instruction sets (either through manual intrinsics or library-provided automatic vectorization) provides substantial speedups; combining these two often pushes CPU performance towards its practical limits. Third, the potential for GPU acceleration is enormous. For computational tasks of sufficient scale (in our tests, starting around N=256), the massively parallel architecture of GPUs enables performance levels far exceeding what CPUs can offer. Fourth, it highlights the value of making good use of professional libraries. Specialized math libraries such as BLAS (and its various implementations like OpenBLAS, MKL, AOCL-BLAS), CLBlast, hipBLAS (or cuBLAS for NVIDIA), encapsulate a vast amount of low-level optimization expertise. Employing them is frequently the most effective path to achieving both high performance and good development productivity. Even higher-level libraries like OpenCV may rely on these optimized backends internally. However, we must also recognize that there is no “silver bullet” in performance optimization; no single method reigns supreme in all scenarios. Small-scale problems might favor CPU implementations due to avoided data transfer overheads, while large-scale problems clearly benefit from GPU acceleration. The optimal choice will invariably depend on the specific hardware platform, the required precision ( single vs. double), and the available development resources and constraints. Finally, all these findings point towards the importance of continuous learning and hands-on practice. High-performance computing is a rapidly evolving field with constant advancements in hardware architectures, programming models, and compiler technologies. Maintaining curiosity, persistently learning new techniques, and personally testing and validating assumptions are the keys to truly mastering the art and science of performance optimization.\nHopefully, this exploration into the performance landscape of matrix multiplication has provided everyone with a more tangible understanding of the diverse computing technologies available. From the humble three nested loops to blistering speeds exceeding one hundred GFLOPS, the journey reflects the culmination of ingenuity in computer architecture, parallel computing, and software engineering. Perhaps the next time you’re faced with a task involving large-scale matrix operations, you’ll recall the contenders we discussed today and feel more equipped to choose the most suitable acceleration strategy for your application!\nAppendix: Benchmark Results Data Table omitted as requested.\nImplementation Matrix Size (N) Real Time (ns) Performance (GFLOPS) Naive 64 640,561 0.818 Naive 128 5,250,421 0.799 Naive 256 42,393,811 0.791 Naive 512 569,762,981 0.471 Naive 1024 3,447,583,101 0.623 OpenMP 64 149,270 3.512 OpenMP 128 1,036,590 4.046 OpenMP 256 6,844,282 4.903 OpenMP 512 62,077,042 4.324 OpenMP 1024 578,410,614 3.713 AVX2+FMA 64 311,178 1.685 AVX2+FMA 128 2,505,685 1.674 AVX2+FMA 256 19,324,494 1.736 AVX2+FMA 512 152,734,950 1.758 AVX2+FMA 1024 1,237,421,611 1.735 AVX512+FMA 64 221,951 2.362 AVX512+FMA 128 1,702,158 2.464 AVX512+FMA 256 14,094,445 2.381 AVX512+FMA 512 107,877,880 2.488 AVX512+FMA 1024 921,593,993 2.330 AVX2+FMA_OMP 64 90,276 5.808 AVX2+FMA_OMP 128 664,552 6.311 AVX2+FMA_OMP 256 3,656,076 9.178 AVX2+FMA_OMP 512 27,922,787 9.613 AVX2+FMA_OMP 1024 216,519,971 9.918 AVX512+FMA_OMP 64 86,896 6.033 AVX512+FMA_OMP 128 427,994 9.799 AVX512+FMA_OMP 256 2,648,926 12.667 AVX512+FMA_OMP 512 18,439,355 14.558 AVX512+FMA_OMP 1024 140,055,382 15.333 Eigen 64 904,785 0.579 Eigen 128 12,846,593 0.326 Eigen 256 32,201,997 1.042 Eigen 512 284,153,414 0.945 Eigen 1024 2,316,560,842 0.927 OpenCV 64 33,326 15.732 OpenCV 128 73,443 57.110 OpenCV 256 538,501 62.311 OpenCV 512 4,811,569 55.790 OpenCV 1024 36,290,270 59.175 BLAS 64 10,609 49.420 BLAS 128 73,929 56.734 BLAS 256 535,021 62.716 BLAS 512 5,210,261 51.521 BLAS 1024 36,608,529 58.661 Vulkan 64 258,650 2.027 Vulkan 128 850,222 4.933 Vulkan 256 2,015,570 16.648 Vulkan 512 15,517,304 17.300 Vulkan 1024 69,655,183 30.830 OpenCL 64 69,397 7.555 OpenCL 128 147,861 28.367 OpenCL 256 593,376 56.548 OpenCL 512 5,842,253 45.947 OpenCL 1024 38,429,528 55.881 CLBlast 64 61,002 8.595 CLBlast 128 127,007 33.024 CLBlast 256 426,358 78.700 CLBlast 512 3,740,453 71.765 CLBlast 1024 20,777,060 103.358 HIP 64 856,032,739 0.000612 HIP 128 171,225 24.496 HIP 256 613,603 54.684 HIP 512 5,788,911 46.371 HIP 1024 38,210,712 56.201 hipBLAS 64 2,080,484 0.252 hipBLAS 128 2,146,978 1.954 hipBLAS 256 2,691,232 12.468 hipBLAS 512 5,960,233 45.038 hipBLAS 1024 21,356,498 100.554 ","wordCount":"10476","inLanguage":"en","datePublished":"2025-04-19T20:33:11+08:00","dateModified":"2025-04-19T20:33:11+08:00","author":{"@type":"Person","name":"Tategoto Azarasi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://tategotoazarasi.github.io/en/posts/matrix-multiplication-performance-benchmark-from-triple-loops-to-100-plus-gflops-on-amd-ryzen-ai-radeon/"},"publisher":{"@type":"Organization","name":"Tategoto Azarasi","logo":{"@type":"ImageObject","url":"https://tategotoazarasi.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://tategotoazarasi.github.io/en/ accesskey=h title="Tategoto Azarasi (Alt + H)">Tategoto Azarasi</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://tategotoazarasi.github.io/zh/ title=中文 aria-label=中文>Zh</a></li></ul></div></div><ul id=menu><li><a href=https://tategotoazarasi.github.io/en/ title=Home><span>Home</span></a></li><li><a href=https://tategotoazarasi.github.io/en/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://tategotoazarasi.github.io/en/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://tategotoazarasi.github.io/en/>Home</a>&nbsp;»&nbsp;<a href=https://tategotoazarasi.github.io/en/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Matrix Multiplication Performance Benchmark: from Triple Loops to 100+ GFLOPS on AMD Ryzen AI + Radeon</h1><div class=post-meta><span title='2025-04-19 20:33:11 +0800 +0800'>April 19, 2025</span>&nbsp;·&nbsp;50 min&nbsp;·&nbsp;10476 words&nbsp;·&nbsp;Tategoto Azarasi&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://tategotoazarasi.github.io/zh/posts/matrix-multiplication-performance-benchmark-from-triple-loops-to-100-plus-gflops-on-amd-ryzen-ai-radeon/>Zh</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ol><li><a href=#hardware-and-software-environment>Hardware and Software Environment</a></li><li><a href=#the-contenders-matrix-multiplication-implementations-detailed>The Contenders: Matrix Multiplication Implementations Detailed</a><ol><li><a href=#naive-implementation>Naive Implementation</a></li><li><a href=#openmp-cpu-multi-core-parallelism>OpenMP (CPU Multi-core Parallelism)</a></li><li><a href=#cpu-simd-avx2avx-512--fma>CPU SIMD (AVX2/AVX-512 + FMA)</a><ol><li><a href=#avx2--fma-256-bit>AVX2 + FMA (256-bit)</a></li><li><a href=#avx-512--fma-512-bit>AVX-512 + FMA (512-bit)</a></li></ol></li><li><a href=#simd--openmp-avx2avx-512--fma--openmp>SIMD + OpenMP (AVX2/AVX-512 + FMA + OpenMP)</a></li><li><a href=#blas-basic-linear-algebra-subprograms>BLAS (Basic Linear Algebra Subprograms)</a></li><li><a href=#eigen--opencv>Eigen & OpenCV</a><ol><li><a href=#eigen>Eigen</a></li><li><a href=#opencv>OpenCV</a></li></ol></li><li><a href=#opencl>OpenCL</a></li><li><a href=#clblast>CLBlast</a></li><li><a href=#vulkan-compute>Vulkan Compute</a></li><li><a href=#hip-heterogeneous-compute-interface-for-portability>HIP (Heterogeneous-Compute Interface for Portability)</a></li><li><a href=#hipblas>hipBLAS</a></li></ol></li><li><a href=#benchmarking-methodology>Benchmarking Methodology</a></li><li><a href=#performance-data-analysis>Performance Data Analysis</a></li><li><a href=#deeper-dive-discussion--caveats>Deeper Dive: Discussion & Caveats</a></li><li><a href=#the-finish-line-conclusion--outlook>The Finish Line: Conclusion & Outlook</a></li><li><a href=#appendix-benchmark-results-data>Appendix: Benchmark Results Data</a></li></ol></nav></div></details></div><div class=post-content><p>Today, let&rsquo;s chat about a commonplace yet timeless topic – matrix multiplication. &ldquo;Matrix multiplication? Learned that
in university linear algebra, isn&rsquo;t it just three <code>for</code> loops?&rdquo; you might say. Indeed, the most basic implementation is
exactly that, simple and direct. But in the world of high-performance computing, where every cycle counts, there&rsquo;s a
whole universe hidden behind those three nested loops. Different implementation methods can lead to performance
differences that are worlds apart, sometimes by factors of hundreds or even thousands!</p><p>Sounds a bit exciting, doesn&rsquo;t it? Like comparing the speed of an F1 race car to a mobility scooter. Why such a massive
gap? Modern CPU and GPU architectures, compiler optimizations, parallel computing techniques, specialized math
libraries&mldr; these are all critical factors influencing performance.</p><p>To get a firsthand feel for these differences, I recently conducted a matrix multiplication (square matrices,
<code>C = A * B</code>) &ldquo;performance showdown&rdquo; on my new gear – a Lenovo ThinkBook 16 G7+ laptop equipped with an AMD Ryzen AI 9
365 processor (featuring integrated Radeon 880M graphics). We invited several &ldquo;contenders&rdquo; to the ring, covering a wide
range of approaches: from the most naive implementation to methods leveraging CPU multi-cores, SIMD instruction sets,
calling professional math libraries, and even harnessing GPU acceleration (using OpenCL, Vulkan Compute, and ROCm/HIP).</p><p>This blog post will walk you through the entire benchmarking process: from introducing the &ldquo;race track&rdquo; environment,
dissecting the technical characteristics of each &ldquo;contender,&rdquo; to analyzing the final results and summarizing the
takeaways. We&rsquo;re not aiming for a stern academic paper, but rather a relaxed, natural discussion about the technical
intricacies and the allure of performance optimization. Hopefully, this will provide some inspiration and satisfy your
curiosity about high-performance computing.</p><p>Ready? Buckle up, let&rsquo;s get started!</p><h2 id=hardware-and-software-environment>Hardware and Software Environment<a hidden class=anchor aria-hidden=true href=#hardware-and-software-environment>#</a></h2><p>As the saying goes, &ldquo;To do a good job, one must first sharpen one&rsquo;s tools.&rdquo; Before diving into the performance tests,
let&rsquo;s lay out the &ldquo;tools of the trade,&rdquo; meaning the hardware and software environment used for this benchmark.
Understanding this background information will help us better interpret the subsequent performance data.</p><p>My core hardware configuration includes an AMD Ryzen AI 9 365 processor, belonging to family 26, model 36. This is a
fairly new CPU, boasting 10 physical cores and supporting 20 threads, with a base frequency of 2.0 GHz. It features
crucial AVX, AVX2, FMA, and, importantly, AVX-512 instruction set support (including various flavors like AVX512F, DQ,
CD, BW, VL). While it also integrates an NPU (Neural Processing Unit), our tests primarily focus on its general-purpose
CPU and GPU compute capabilities. For memory, the system is equipped with 27.2 GiB (approximately 32GB as reported by
the system) of DDR5 RAM; memory size and speed are critical for the performance of large-scale matrix operations. The
integrated graphics card is the AMD Radeon Graphics (Radeon 880M). According to information from <code>rocminfo</code> and
<code>vulkaninfo</code>, its GPU model identifier is <code>gfx1150</code> (sometimes shown as <code>11.5.0</code>), featuring 12 Compute Units (CUs),
each containing 2 SIMD units. It can reach a maximum clock frequency of 2900MHz and supports both FP16 and FP64 (
double-precision) computations. This integrated GPU supports Vulkan, OpenCL, and AMD&rsquo;s ROCm/HIP platform, offering
multiple avenues for GPU acceleration in our tests. It&rsquo;s worth noting specifically that during the benchmark execution,
I set the <code>HSA_OVERRIDE_GFX_VERSION=11.5.1</code> environment variable. This might slightly influence the target code
generation or runtime behavior for HIP or hipBLAS, a practice often employed because official <code>rocblas</code> support for
<code>gfx1150</code> wasn&rsquo;t fully implemented at the time.</p><p>On the software side, I&rsquo;m running Arch Linux, a rolling-release distribution, which keeps my software packages
relatively up-to-date. The specific kernel version is <code>6.14.2-2-cachyos</code> (64-bit); CachyOS is an Arch derivative often
incorporating performance-enhancing patches. The desktop environment is KDE Plasma 6.3.4, operating on the Wayland
display server protocol. For compilation, I primarily use GCC (g++), whose version varies with Arch Linux updates but
certainly supports C++17/20 standards along with OpenMP and AVX/AVX-512 instructions. HIP code compilation relies on
<code>hipcc</code> from the ROCm toolchain, which is based on Clang. Project building is managed by CMake (version 3.20 or higher).</p><p>The core libraries and drivers are key components for this benchmark. The ROCm platform needs to support the <code>gfx1150</code>
or <code>gfx1151</code> GPU model; <code>rocminfo</code> output in the test logs indicates Runtime Version 1.1 and Extension Version 1.6. The
OpenCL environment is slightly complex, with two platforms present: the AMD APP SDK (providing OpenCL 2.1, driver
version 3635.0) and Mesa rusticl (providing OpenCL 3.0). However, based on the test log stating
<code>OpenCL Info: Selected AMD Platform. Using first GPU device. Device Name: gfx1151</code>, we specifically selected the GPU
device under the official AMD driver platform for testing, identified as <code>gfx1151</code>. For Vulkan, the instance version is
1.4.309, using the RADV driver (from Mesa 25.0.4), which identifies the device as <code>AMD Radeon Graphics (RADV GFX1150)</code>.
We utilized the <code>glslc</code> tool to compile GLSL compute shaders into SPIR-V format. The system also has a BLAS (Basic
Linear Algebra Subprograms) implementation installed, likely OpenBLAS, a common high-performance choice on Linux
distributions, successfully located by CMake&rsquo;s <code>find_package(BLAS)</code>. Additionally, the open-source OpenCL BLAS library,
CLBlast, is installed and discoverable by CMake. Furthermore, we tested the popular C++ template library Eigen3 (version
3.3+, provided as header files) and the computer vision library OpenCV (version 4.x, with its core module correctly
found by CMake).</p><p>Finally, the entire benchmarking framework is Google Benchmark (v1.9.2). This is an industry-standard C++ benchmarking
library offering convenient test fixture management, precise timing, automatic iteration count adjustment, and
standardized result output, ensuring the rigor and reliability of our tests.</p><p>To squeeze out as much performance as possible, we employed some rather aggressive compilation options. For C++ code, we
used the GCC (g++) compiler with the <code>-Ofast</code> optimization level, combined with the <code>-march=native</code> flag, allowing the
compiler to generate the most optimized machine code based on the specific features of my native CPU (including its
AVX-512 capabilities). We also explicitly added <code>-mavx2 -mfma -mavx512f -mavx512dq</code> flags to ensure these SIMD
instructions could be utilized. For HIP code, we similarly used the <code>-Ofast</code> optimization option with <code>hipcc</code> (based on
Clang). Moreover, <code>CMAKE_HIP_ARCHITECTURES</code> was set to <code>gfx1150</code> via CMake (based on <code>rocminfo</code> findings) to guide the
compiler in generating code for the target GPU architecture. OpenCL Kernel optimization differs; it&rsquo;s specified not
during host code compilation but at runtime via options passed to the <code>clBuildProgram</code> function. A commonly used
optimization flag is <code>-cl-fast-relaxed-math</code>, which permits the OpenCL compiler to perform mathematical optimizations
that might slightly affect floating-point precision but can significantly improve execution speed. Lastly, for Vulkan
compute shaders, we also included the <code>-O</code> option when compiling them into SPIR-V format using the <code>glslc</code> tool,
enabling compile-time optimization.</p><p>With this background set, let&rsquo;s introduce the contenders and see what tricks they have up their sleeves.</p><h2 id=the-contenders-matrix-multiplication-implementations-detailed>The Contenders: Matrix Multiplication Implementations Detailed<a hidden class=anchor aria-hidden=true href=#the-contenders-matrix-multiplication-implementations-detailed>#</a></h2><p>Next, we&rsquo;ll introduce each matrix multiplication implementation method that participated in this performance showdown.</p><h3 id=naive-implementation>Naive Implementation<a hidden class=anchor aria-hidden=true href=#naive-implementation>#</a></h3><p>This contender is the one we&rsquo;re most familiar with and the starting point for all optimizations. It strictly follows the
definition of matrix multiplication, C[i][j] = Σ(A[i][k] * B[k][j]), using three nested loops:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#75715e>// Pseudo-code example
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span> to N<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#f92672>:</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>for</span> j <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span> to N<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#f92672>:</span>
</span></span><span style=display:flex><span>    sum <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> k <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span> to N<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#f92672>:</span>
</span></span><span style=display:flex><span>      sum <span style=color:#f92672>+=</span> A[i][k] <span style=color:#f92672>*</span> B[k][j]; <span style=color:#75715e>// or A[i*N + k] * B[k*N + j] for row-major 1D array
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    C[i][j] <span style=color:#f92672>=</span> sum; <span style=color:#75715e>// or C[i*N + j] = sum
</span></span></span></code></pre></div><p>The advantage of this naive implementation lies in its extreme simplicity and logical clarity, making it easy to
understand. However, its disadvantage is extremely poor performance. This stems mainly from several factors. First, it&rsquo;s
Cache Unfriendly. During computation, access to the B matrix occurs column-wise (in the innermost k-loop, j is constant,
k increments, accessing <code>B[k*N + j]</code>), but data is stored row-wise (Row-Major) in memory. This mismatch between access
pattern and storage layout leads to frequent CPU cache line misses, requiring constant reloading from main memory and
drastically reducing memory access efficiency. Accesses to matrix A (row-wise) and writes to matrix C (element-wise) are
comparatively better for caching, but the B matrix access pattern becomes the performance killer. Second, this
implementation is entirely serial, failing to utilize the valuable multi-core parallel processing capabilities of modern
CPUs. Finally, it also makes no use of the CPU&rsquo;s SIMD (Single Instruction, Multiple Data) units for vectorized
computation; each operation handles only a single element&rsquo;s multiplication and addition, resulting in low efficiency.</p><p>This one primarily serves as a performance baseline to see how much improvement other methods can offer.</p><h3 id=openmp-cpu-multi-core-parallelism>OpenMP (CPU Multi-core Parallelism)<a hidden class=anchor aria-hidden=true href=#openmp-cpu-multi-core-parallelism>#</a></h3><p>OpenMP is a parallel programming model based on shared memory, primarily using compiler directives (Pragmas) to guide
the compiler in automatically generating parallel code. For loop-intensive tasks like matrix multiplication, it can
easily distribute the outer loop (typically the <code>i</code> loop) across different CPU cores for execution.</p><p>Implementation-wise, it merely involves adding a <code>#pragma omp parallel for</code> directive before the outer loop of the Naive
version:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#75715e>#pragma omp parallel for default(none) shared(A, B, C, N) schedule(static)
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>for</span> (size_t i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; i <span style=color:#f92672>&lt;</span> N; <span style=color:#f92672>++</span>i) {
</span></span><span style=display:flex><span>  <span style=color:#75715e>// Inner j and k loops remain unchanged
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>  <span style=color:#66d9ef>for</span> (size_t j <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; j <span style=color:#f92672>&lt;</span> N; <span style=color:#f92672>++</span>j) {
</span></span><span style=display:flex><span>    ValueType sum <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> (size_t k <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; k <span style=color:#f92672>&lt;</span> N; <span style=color:#f92672>++</span>k) {
</span></span><span style=display:flex><span>      sum <span style=color:#f92672>+=</span> A[i <span style=color:#f92672>*</span> N <span style=color:#f92672>+</span> k] <span style=color:#f92672>*</span> B[k <span style=color:#f92672>*</span> N <span style=color:#f92672>+</span> j];
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    C[i <span style=color:#f92672>*</span> N <span style=color:#f92672>+</span> j] <span style=color:#f92672>=</span> sum;
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Let&rsquo;s break down the key parts of this OpenMP directive. <code>parallel for</code> is the core instruction, telling the compiler to
parallelize the subsequent <code>for</code> loop. <code>default(none)</code> is a recommended good practice, forcing the programmer to
explicitly declare the scope of each variable within the loop—either shared (<code>shared</code>) or thread-private (<code>private</code>)—to
prevent potential errors. <code>shared(A, B, C, N)</code> declares that the matrices A, B, C, and the size N are shared among all
concurrently executing threads; A and B are read-only during computation, while C is written to, but since OpenMP
typically distributes work row-wise, different threads usually write to different rows of C, generally avoiding write
conflicts. Finally, <code>schedule(static)</code> defines the work distribution strategy. It statically pre-divides the loop&rsquo;s
entire iteration space (here, the N iterations of <code>i</code>) into roughly equal chunks and assigns these chunks to the
available threads. For well-load-balanced loops like matrix multiplication, static scheduling typically incurs low
runtime overhead.</p><p>The primary advantage of using OpenMP is its implementation simplicity; often, just adding a single compiler directive (
Pragma) before a critical loop conveniently utilizes the CPU&rsquo;s multi-core resources. Compared to the fully serial Naive
implementation, performance usually sees a significant boost, ideally approaching a speedup factor close to the number
of CPU cores, although the actual improvement is constrained by factors like memory bandwidth and cache efficiency.
However, it also has drawbacks. First, it doesn&rsquo;t resolve the cache unfriendliness issue present in the Naive version,
particularly the column-wise access pattern for matrix B, which limits further performance gains. Second, its
performance ceiling is inherently limited by the number of physical CPU cores and the system&rsquo;s memory bandwidth.
Furthermore, for very small matrix sizes (N), the overhead introduced by parallel computing (such as thread creation,
management, and synchronization) might even outweigh the time saved by parallel execution, leading to performance
degradation instead of improvement.</p><h3 id=cpu-simd-avx2avx-512--fma>CPU SIMD (AVX2/AVX-512 + FMA)<a hidden class=anchor aria-hidden=true href=#cpu-simd-avx2avx-512--fma>#</a></h3><p>SIMD (Single Instruction, Multiple Data) is a crucial feature of modern CPUs. It allows a single instruction to perform
the same operation on multiple data elements simultaneously. For instance, AVX2 can process 4 <code>double</code> values at once (
using 256-bit registers), while AVX-512 can handle 8 <code>double</code> values (using 512-bit registers). FMA (Fused Multiply-Add)
instructions further enhance efficiency, and potentially precision, by combining a multiplication and an addition into a
single instruction.</p><p>To leverage SIMD, we typically need to use compiler-specific intrinsic functions. This makes the code considerably more
complex than the Naive or OpenMP versions.</p><h4 id=avx2--fma-256-bit>AVX2 + FMA (256-bit)<a hidden class=anchor aria-hidden=true href=#avx2--fma-256-bit>#</a></h4><p>To utilize AVX2 and FMA instructions, we included the <code>immintrin.h</code> header file, which provides access to the necessary
intrinsic functions. A key optimization strategy here involves changing the loop nesting order to <code>i-k-j</code>. The advantage
of this order is that it allows for efficient vectorization within the innermost <code>j</code> loop. Specifically, for fixed <code>i</code>
and <code>k</code>, we can first take the scalar value <code>A[i][k]</code> and broadcast it into all 4 double-precision elements of a 256-bit
vector <code>a_vec</code> using the <code>_mm256_set1_pd()</code> intrinsic. Next, we load 4 consecutive <code>double</code> values from the k-th row of
matrix B (starting at address <code>&amp;B[k*N + j]</code>) into a vector <code>b_vec</code>. Since matrix B is stored row-major, this consecutive
load is generally cache-friendly. We opted for <code>_mm256_loadu_pd()</code>, which allows loading from unaligned memory
addresses, offering more flexibility. Concurrently, we load the corresponding 4 partial sums from the i-th row of matrix
C (address <code>&amp;C[i*N + j]</code>) into <code>c_vec</code>, also using <code>_mm256_loadu_pd()</code>. The core computational step involves executing
the FMA (Fused Multiply-Add) operation, <code>c_vec = a_vec * b_vec + c_vec</code>, using the <code>_mm256_fmadd_pd()</code> intrinsic. This
single instruction performs 4 pairs of multiplications and additions simultaneously. Finally, the updated result vector
<code>c_vec</code> is written back to the corresponding location in matrix C using <code>_mm256_storeu_pd()</code>. Naturally, the
implementation of the innermost <code>j</code> loop needs to iterate with a step size of 4 (the AVX2_DOUBLE_COUNT) and also
requires special handling for any remaining elements at the end of the row (less than 4), which typically fall back to
standard scalar computation.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#75715e>// Pseudo-code example (AVX2 + FMA)
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>constexpr</span> size_t AVX2_DOUBLE_COUNT <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> (size_t i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; i <span style=color:#f92672>&lt;</span> N; <span style=color:#f92672>++</span>i) {
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>for</span> (size_t k <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; k <span style=color:#f92672>&lt;</span> N; <span style=color:#f92672>++</span>k) {
</span></span><span style=display:flex><span>    __m256d a_vec <span style=color:#f92672>=</span> _mm256_set1_pd(A[i<span style=color:#f92672>*</span>N <span style=color:#f92672>+</span> k]); <span style=color:#75715e>// Broadcast A[i][k]
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>for</span> (size_t j <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; j <span style=color:#f92672>&lt;</span> N_aligned; j <span style=color:#f92672>+=</span> AVX2_DOUBLE_COUNT) { <span style=color:#75715e>// Aligned part
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>      __m256d b_vec <span style=color:#f92672>=</span> _mm256_loadu_pd(<span style=color:#f92672>&amp;</span>B[k<span style=color:#f92672>*</span>N <span style=color:#f92672>+</span> j]);  <span style=color:#75715e>// Load 4 doubles from B row k
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>      __m256d c_vec <span style=color:#f92672>=</span> _mm256_loadu_pd(<span style=color:#f92672>&amp;</span>C[i<span style=color:#f92672>*</span>N <span style=color:#f92672>+</span> j]);  <span style=color:#75715e>// Load 4 doubles from C row i
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>      c_vec <span style=color:#f92672>=</span> _mm256_fmadd_pd(a_vec, b_vec, c_vec); <span style=color:#75715e>// Fused Multiply-Add
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>      _mm256_storeu_pd(<span style=color:#f92672>&amp;</span>C[i<span style=color:#f92672>*</span>N <span style=color:#f92672>+</span> j], c_vec); <span style=color:#75715e>// Store back to C
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    }
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Handle remaining elements j = N_aligned to N-1 using scalar operations
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h4 id=avx-512--fma-512-bit>AVX-512 + FMA (512-bit)<a hidden class=anchor aria-hidden=true href=#avx-512--fma-512-bit>#</a></h4><p>The implementation principle for AVX-512 + FMA is identical to the AVX2 version. The main difference lies in using
512-bit wide registers and their corresponding intrinsic functions, such as the <code>__m512d</code> type, <code>_mm512_set1_pd</code>,
<code>_mm512_loadu_pd</code>, <code>_mm512_fmadd_pd</code>, and <code>_mm512_storeu_pd</code>. Because the registers are wider, the vector computation
step size increases to 8 (AVX512_DOUBLE_COUNT), meaning a single instruction can now process 8 double-precision values.
Successfully compiling and running AVX-512 code requires the CPU itself to support the instruction set (our Ryzen AI 9
365 processor meets this condition) and necessitates enabling these instructions via appropriate compiler options (like
<code>-mavx512f</code>) during compilation.</p><p>This SIMD-based optimization approach offers significant advantages. Primarily, it can drastically improve the
computational performance of a single CPU core. Additionally, employing the <code>i-k-j</code> loop order enhances the memory
access pattern for matrix B, making it more cache-friendly. The core benefit comes from fully utilizing the powerful
vector processing units within the CPU. However, this method also comes with notable disadvantages. Writing and
maintaining code using SIMD intrinsics is considerably complex, and the resulting code suffers from poor portability as
it directly depends on the specific instruction sets supported by the target CPU. Developers must also manually handle
potential memory alignment issues (although <code>loadu/storeu</code> provide unaligned access, aligned loads/stores are generally
faster) and manage the boundary conditions at the end of loops. Furthermore, historically, executing AVX-512
instructions could sometimes trigger the CPU to reduce its operating frequency to manage power consumption and heat
generation; while this issue has been largely mitigated in modern CPUs, it remains a potential consideration.</p><h3 id=simd--openmp-avx2avx-512--fma--openmp>SIMD + OpenMP (AVX2/AVX-512 + FMA + OpenMP)<a hidden class=anchor aria-hidden=true href=#simd--openmp-avx2avx-512--fma--openmp>#</a></h3><p>Since OpenMP can parallelize the outer loop and SIMD can accelerate the inner computations, combining them seems like a
powerful synergy. Indeed, it is.</p><p>The implementation simply involves adding the OpenMP parallel directive before the outer <code>i</code> loop of the SIMD (either
AVX2 or AVX-512) version using the <code>i-k-j</code> loop order:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#75715e>#pragma omp parallel for default(none) shared(A, B, C, N, N_aligned) schedule(static)
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>for</span> (size_t i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; i <span style=color:#f92672>&lt;</span> N; <span style=color:#f92672>++</span>i) {
</span></span><span style=display:flex><span>  <span style=color:#75715e>// Inner k and j (SIMD) loops remain unchanged
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>  <span style=color:#66d9ef>for</span> (size_t k <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; k <span style=color:#f92672>&lt;</span> N; <span style=color:#f92672>++</span>k) {
</span></span><span style=display:flex><span>    <span style=color:#75715e>// ... SIMD intrinsics code as before ...
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>The primary advantage of combining SIMD instructions (be it AVX2 or AVX-512) with OpenMP multithreading is its ability
to leverage both the CPU&rsquo;s multi-core parallel processing power and its instruction-level parallelism (vectorization)
simultaneously. This two-pronged approach often allows reaching, or at least closely approaching, the theoretical peak
performance of the CPU for the given task. However, this method also has clear disadvantages. Firstly, it further
compounds the code complexity, incorporating intricacies from both SIMD intrinsics programming and OpenMP parallel
management. Secondly, as computation speed is pushed to its limits, the application&rsquo;s performance bottleneck is very
likely to shift from the computation itself to being limited by memory bandwidth – meaning the CPU cores can process
data faster than the memory subsystem can supply it. Lastly, achieving optimal performance usually requires careful
tuning of OpenMP-related parameters, such as selecting the most effective thread scheduling strategy (e.g., static,
dynamic, guided via the <code>schedule</code> clause) and potentially employing advanced thread management techniques like thread
affinity or load balancing adjustments.</p><h3 id=blas-basic-linear-algebra-subprograms>BLAS (Basic Linear Algebra Subprograms)<a hidden class=anchor aria-hidden=true href=#blas-basic-linear-algebra-subprograms>#</a></h3><p>BLAS isn&rsquo;t a specific library but rather a standardized API specification defining interfaces for basic vector and
matrix operations. Many organizations and companies provide implementations of BLAS. These libraries typically contain
highly optimized C, Fortran, or even assembly code tailored for specific hardware (CPU architecture, cache sizes, SIMD
instructions). They often internally implement sophisticated techniques like blocking (or tiling) to maximize cache
utilization and automatically employ both SIMD instructions and multithreading.</p><p>We only need to call the standard C interface <code>cblas_dgemm</code> (&rsquo;d&rsquo; for double precision, &lsquo;gemm&rsquo; for general matrix-matrix
multiplication):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=display:flex><span><span style=color:#75715e>// Pseudo-code example
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#a6e22e>cblas_dgemm</span>(
</span></span><span style=display:flex><span>    CblasRowMajor, <span style=color:#75715e>// Tell BLAS our data is stored row by row
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    CblasNoTrans, CblasNoTrans, <span style=color:#75715e>// Neither A nor B needs transposing
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    N, N, N,        <span style=color:#75715e>// M, N, K (for N x N matrices)
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#ae81ff>1.0</span>,            <span style=color:#75715e>// alpha (for C = alpha*A*B + beta*C)
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    A.<span style=color:#a6e22e>data</span>(), N,    <span style=color:#75715e>// Pointer to A data and its leading dimension (cols for RowMajor)
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    B.<span style=color:#a6e22e>data</span>(), N,    <span style=color:#75715e>// Pointer to B data and its leading dimension
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#ae81ff>0.0</span>,            <span style=color:#75715e>// beta (set to 0 to overwrite C, i.e., C = A*B)
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    C.<span style=color:#a6e22e>data</span>(), N     <span style=color:#75715e>// Pointer to C data and its leading dimension
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>);
</span></span></code></pre></div><p>Using a BLAS library for matrix multiplication offers several advantages. The most prominent is extreme ease of use;
developers typically only need to call a single highly optimized library function (like <code>cblas_dgemm</code>) to perform the
complex computation, significantly simplifying the programming effort. Secondly, because these libraries incorporate
extensive hardware-specific optimizations, their performance is usually excellent, often approaching the theoretical
peak computational throughput of the hardware. Furthermore, as BLAS is a standard interface, it provides good
portability – code can generally run unmodified on any target platform that has a compliant BLAS library implementation.
Calling a library function also results in very concise application code. Of course, using BLAS also has disadvantages.
First, the application needs to be linked against the corresponding BLAS library file during the build process. Second,
and most critically, the final performance achieved heavily depends on the quality of the specific BLAS implementation
being used. Different BLAS libraries (like OpenBLAS, Intel MKL, ATLAS, etc.) can exhibit significant performance
variations even on the same hardware.</p><h3 id=eigen--opencv>Eigen & OpenCV<a hidden class=anchor aria-hidden=true href=#eigen--opencv>#</a></h3><p>Besides low-level interfaces like BLAS, many high-level C++ libraries also provide matrix operations. We tested two
popular examples: Eigen and OpenCV.</p><h4 id=eigen>Eigen<a hidden class=anchor aria-hidden=true href=#eigen>#</a></h4><p>Let&rsquo;s take a look at the Eigen library. Its key characteristic is being a C++ template library renowned for its elegant
API and powerful &ldquo;Expression Templates&rdquo; technology. This technique allows Eigen to analyze and optimize complex chains
of linear algebra expressions at compile time, avoiding the creation of unnecessary intermediate temporary objects and
often automatically generating SIMD instructions for the underlying computations. In terms of usage, Eigen code is also
very concise. We can first use <code>Eigen::Map</code> to &ldquo;map&rdquo; our raw data stored in <code>std::vector</code> onto Eigen&rsquo;s internal matrix
object – this mapping itself incurs zero memory copy overhead. Then, we can directly use the overloaded <code>*</code> operator to
perform the matrix multiplication, like so:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#75715e>// Pseudo-code example (Map existing data)
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>Eigen<span style=color:#f92672>::</span>Map<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>const</span> EigenMatrixType<span style=color:#f92672>&gt;</span> A_map(A.data(), N, N);
</span></span><span style=display:flex><span>Eigen<span style=color:#f92672>::</span>Map<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>const</span> EigenMatrixType<span style=color:#f92672>&gt;</span> B_map(B.data(), N, N);
</span></span><span style=display:flex><span>EigenMatrixType <span style=color:#a6e22e>C_eigen</span>(N, N); <span style=color:#75715e>// Eigen&#39;s result matrix
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>matrix_multiply_eigen(A_map, B_map, C_eigen); <span style=color:#75715e>// C_eigen.noalias() = A_map * B_map;
</span></span></span></code></pre></div><p>It&rsquo;s worth noting the use of the <code>noalias()</code> method in the code. This explicitly informs Eigen that the output matrix C
does not overlap in memory with the input matrices A or B (no aliasing), enabling Eigen to employ more efficient and
aggressive internal implementations for optimization.</p><p>Overall, Eigen&rsquo;s advantages include its very modern API, ease of use, and high code readability. Its ability to perform
compile-time optimizations via C++ template metaprogramming is also a significant strength. However, it also has
disadvantages. In terms of performance, it might not match specialized, deeply hand-optimized BLAS libraries (the final
performance largely depends on the compiler&rsquo;s optimization capabilities and the complexity of the specific expression).
Additionally, due to its heavy reliance on templates, compile times can be relatively longer.</p><h4 id=opencv>OpenCV<a hidden class=anchor aria-hidden=true href=#opencv>#</a></h4><p>Next up is OpenCV. Its primary characteristic is being a comprehensive library mainly focused on computer vision tasks.
However, its core module (<code>core</code>) also provides very powerful matrix operations centered around the <code>cv::Mat</code> class.
<code>cv::Mat</code> can manage its own memory or conveniently &ldquo;wrap&rdquo; existing external data, avoiding unnecessary copies. An
important advantage is that when performing computationally intensive operations like matrix multiplication, OpenCV
typically attempts to leverage available underlying optimization mechanisms to accelerate the process. This might
include Intel IPP (Integrated Performance Primitives), OpenMP multithreading, or potentially even calling a
system-installed BLAS library. When using it, we can wrap the data from our <code>std::vector</code> into <code>cv::Mat</code> objects without
copying, specifying the rows, columns, data type (<code>CV_64F</code> for double), and the data pointer. Then, we call the
<code>cv::gemm</code> function provided by OpenCV to perform the matrix multiplication. This function&rsquo;s interface is very similar
to the <code>gemm</code> function in BLAS:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#75715e>// Pseudo-code example
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>cv<span style=color:#f92672>::</span>Mat A_cv(N, N, CV_64F, A.data()); <span style=color:#75715e>// Wrap existing data
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>cv<span style=color:#f92672>::</span>Mat B_cv(N, N, CV_64F, B.data());
</span></span><span style=display:flex><span>cv<span style=color:#f92672>::</span>Mat C_cv(N, N, CV_64F);          <span style=color:#75715e>// OpenCV result matrix
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>matrix_multiply_opencv(A_cv, B_cv, C_cv); <span style=color:#75715e>// cv::gemm(A_cv, B_cv, 1.0, cv::Mat(), 0.0, C_cv);
</span></span></span></code></pre></div><p>OpenCV&rsquo;s advantages lie in its extremely rich feature set, extending far beyond just matrix multiplication to cover a
vast range of image processing and computer vision functionalities. If your project is already using OpenCV, employing
it for matrix operations allows for seamless integration with other library features. Furthermore, it may leverage
various backend optimization libraries to enhance performance. However, its disadvantages are also notable, primarily
the fact that it introduces a relatively large and complex library dependency. If your task solely involves pure linear
algebra computations, incorporating the entire OpenCV library might not be the most lightweight choice.</p><h3 id=opencl>OpenCL<a hidden class=anchor aria-hidden=true href=#opencl>#</a></h3><p>Now we turn to OpenCL (Open Computing Language), an open standard framework designed for cross-platform, heterogeneous
parallel computing, allowing programs to utilize various compute devices including CPUs, GPUs, DSPs, and even FPGAs.</p><p>The typical workflow for computing with OpenCL is rather involved, encompassing multiple steps. First, one needs to
query available OpenCL platforms (like the AMD APP SDK) and select a compute device from one of them (such as the
<code>gfx1151</code> GPU used in our tests). Next, a Context must be created; this acts as a container for managing the selected
device(s) and associated resources like memory objects and command queues. Following that, a Command Queue is created
for the chosen device, which serves as the conduit for submitting tasks (like memory transfers and kernel executions) to
the device. The core data (matrices A, B, C) needs to reside in Memory Buffers on the device, created as <code>cl_mem</code>
objects; this necessitates copying the input data A and B from host (CPU) memory to their respective device buffers. The
computational task itself is defined in an OpenCL Kernel, typically written in a separate <code>.cl</code> file (like our
<code>matrix_mult.cl</code>); this source code must be loaded, compiled (at which point optimization options like
<code>-cl-fast-relaxed-math</code> can be passed), and built into an OpenCL Program object (<code>cl_program</code>). From this program
object, the specific kernel function object (<code>cl_kernel</code>) to be executed is obtained. Before executing the kernel, its
arguments must be set using <code>clSetKernelArg</code>, passing the device buffer objects (the <code>cl_mem</code> handles for A, B, C) and
the matrix size N, among other potential parameters. Kernel execution is initiated by enqueuing the task onto the
command queue using <code>clEnqueueNDRangeKernel</code>. This requires specifying the total number of global work-items (usually N*
N, with each work-item calculating one element of C) and optionally, the local work-item size (the Workgroup size, e.g.,
16x16, which impacts resource usage and performance). After the kernel finishes execution on the device, the results
stored in the C buffer on the device must be copied back to host memory using <code>clEnqueueReadBuffer</code>. Finally, and
crucially, all created OpenCL objects (kernel, program, buffers, queue, context) must be explicitly released to prevent
resource leaks.</p><p>Regarding the OpenCL Kernel code (<code>matrix_mult.cl</code>), it&rsquo;s written in the OpenCL C language, a dialect based on the C99
standard with extensions for parallel computing. In our matrix multiplication kernel, each work-item (think of it as a
lightweight thread) uses the built-in functions <code>get_global_id(0)</code> and <code>get_global_id(1)</code> to determine its unique
coordinates (column <code>col</code>, row <code>row</code>) within the global N x N computation grid. Then, each work-item independently
executes the inner loop over <code>k</code> to compute the dot product and store the result for <code>C[row][col]</code>. Since we&rsquo;re using
<code>double</code> as our data type, the kernel code needs the <code>#pragma OPENCL EXTENSION cl_khr_fp64 : enable</code> directive to
explicitly enable support for double-precision floating-point numbers.</p><p>The main advantages of OpenCL are its theoretical cross-platform and cross-vendor compatibility and its ability to fully
leverage the massive parallel compute power of GPUs and other accelerators. However, its disadvantages are also
significant: the programming model is relatively complex, requiring developers to manually manage platforms, devices,
contexts, memory, synchronization, and more, which often leads to verbose code. Furthermore, data must be explicitly
transferred between the host and the device, introducing latency and bandwidth overhead that can negatively impact
performance (become counterproductive), especially for computationally small tasks. Additionally, the actual performance
of an OpenCL application can be sensitive to the quality of the specific vendor&rsquo;s driver implementation.</p><h3 id=clblast>CLBlast<a hidden class=anchor aria-hidden=true href=#clblast>#</a></h3><p>CLBlast can be thought of as the BLAS implementation for the OpenCL ecosystem. Its design goal is to provide an API
compatible with the traditional BLAS interface, but its internal computational logic is implemented using the OpenCL
standard, enabling it to run on any GPU (or other accelerator) that supports OpenCL.</p><p>In terms of usage, invoking CLBlast is significantly simpler than manually writing and managing OpenCL kernels. First,
you still need an initialized OpenCL environment, including a context and a command queue; we can directly reuse the
global context <code>g_clContext</code> prepared for the pure OpenCL implementation. Next, OpenCL memory buffers need to be created
for the input and output matrices, and the host data must be copied to the input buffers, just as in standard OpenCL.
Once these prerequisites are met, the core step involves calling the CLBlast function <code>clblast::Gemm&lt;ValueType>(...)</code> (
using the C++ template interface here, where <code>ValueType</code> automatically determines the precision). When calling this
function, you need to pass arguments describing the matrix layout (row-major or column-major), whether the input
matrices should be transposed, the matrix dimensions (M, N, K), the scalar values alpha and beta, pointers to the
device-side OpenCL buffer objects, the leading dimension of each matrix (which is typically the number of columns for
row-major storage), and the OpenCL command queue to use for execution. The CLBlast library then takes care of internally
invoking its pre-compiled and optimized OpenCL kernels to perform the actual computation. After the computation is
complete, the developer still needs to copy the results from the device-side C buffer back to host memory, similar to
standard OpenCL practice.</p><p>The primary advantages of CLBlast are that it offers a standard BLAS interface, greatly simplifying the programming
effort required for GPU-accelerated matrix operations using OpenCL. Furthermore, because the kernel functions within the
CLBlast library are typically meticulously optimized by its developers (likely employing advanced techniques like
sophisticated tiling, shared memory optimization, etc.), its performance is often superior to relatively simple OpenCL
kernels written by application developers. However, it also has disadvantages. First, it relies on the target system
having a correctly installed and configured OpenCL runtime environment as well as the CLBlast library itself. Second,
like all GPU acceleration schemes based on a discrete memory model, it still involves the overhead of data transfer
between the host and the device, which can become a performance bottleneck for small problems or in bandwidth-limited
scenarios.</p><h3 id=vulkan-compute>Vulkan Compute<a hidden class=anchor aria-hidden=true href=#vulkan-compute>#</a></h3><p>Next is Vulkan Compute. Vulkan itself was primarily designed as a next-generation, high-performance graphics rendering
API, but it also incorporates powerful general-purpose computing (GPGPU) capabilities implemented via Compute Shaders.</p><p>The workflow for performing computations using Vulkan is arguably even more verbose and lower-level than OpenCL.
Broadly, it involves the following sequence of steps: First is the initialization of a Vulkan Instance, followed by
selecting a suitable Physical Device (usually the GPU), and then creating a Logical Device based on it, along with
obtaining a Compute Queue for submitting computational tasks. The computation logic itself needs to be written in a
compute shader (like our <code>matrix_mult.comp</code>), typically using the GLSL language. This shader must then be compiled into
Vulkan&rsquo;s standard intermediate representation, SPIR-V format (using a tool like <code>glslc -O</code>), and this SPIR-V code is
loaded to create a Shader Module (<code>VkShaderModule</code>). For data storage, you must explicitly allocate Memory (
<code>VkDeviceMemory</code>) on the device and create Vulkan Buffers (<code>VkBuffer</code>) to hold the input matrices A, B, and the output
matrix C. This process involves complex decisions regarding memory type selection, allocation, and binding buffers to
the allocated memory. Copying data from the host (CPU) to these device buffers usually requires an intermediate,
host-visible Staging Buffer. To allow the shader to access these buffer resources, Descriptors must be set up. This
includes defining a Descriptor Set Layout (<code>VkDescriptorSetLayout</code>) to declare the resources the shader needs (e.g.,
three storage buffers), creating a Descriptor Pool (<code>VkDescriptorPool</code>) from which to allocate descriptor sets,
allocating a specific Descriptor Set (<code>VkDescriptorSet</code>), and finally &ldquo;connecting&rdquo; or updating this descriptor set with
the information about our created buffers. With the shader module and descriptors ready, the next step is to create the
Compute Pipeline. This requires first creating a Pipeline Layout (<code>VkPipelineLayout</code>), which associates the descriptor
set layouts used by the shader, and then creating the actual compute pipeline object (<code>VkPipeline</code>) based on this layout
and the shader module. The actual commands are submitted via a Command Buffer. One must be allocated from a Command
Pool (<code>VkCommandPool</code>). Then, you begin recording commands into it: first, you bind the compute pipeline and the
descriptor set containing the resource information, and then you invoke <code>vkCmdDispatch</code> to launch the computation.
<code>vkCmdDispatch</code> requires specifying the number of workgroups to launch, which usually needs to be calculated based on
the matrix size N and the number of threads per workgroup defined in the shader (the <code>local_size</code>). Once command
recording is complete, the command buffer is submitted to the previously obtained compute queue for execution. Since
submission is asynchronous, Vulkan synchronization primitives like Fences or Semaphores must be used to wait for the GPU
computation to finish. After completion, the results in the device&rsquo;s C buffer need to be copied back to host memory,
again likely using a staging buffer. The final step involves meticulously destroying all created Vulkan objects (
pipeline, layout, descriptors, pool, buffers, memory, device, instance, etc.) in the reverse order of creation to
release resources properly.</p><p>Our compute shader (<code>matrix_mult.comp</code>) is written in GLSL (OpenGL Shading Language). The
<code>layout (local_size_x = 16, local_size_y = 16)</code> directive at the top defines that each workgroup consists of 16x16=256
work-items (threads). The <code>layout(set = 0, binding = ...)</code> specifications define how the shader accesses the buffers A,
B, and C via binding points (0, 1, 2) within descriptor set 0. Inside the <code>main</code> function, the built-in variable
<code>gl_GlobalInvocationID.xy</code> provides the global coordinates of the current work-item within the overall compute grid (
where <code>id.x</code> corresponds to the column and <code>id.y</code> to the row). The core computation logic, involving the loop over <code>k</code>
to calculate the dot product for <code>C[id.y * N + id.x]</code>, is very similar to the OpenCL kernel.</p><p>The advantages of using Vulkan Compute lie in it being a modern graphics API designed to reduce driver overhead on the
CPU. If an application already requires graphics rendering, using Vulkan Compute allows for better integration with the
rendering pipeline, potentially sharing resources and context. Vulkan also offers very fine-grained control over the
hardware, enabling deep performance optimization. However, its disadvantages are quite prominent: the API is extremely
verbose, and the initialization and setup processes are highly complex, leading to massive code overhead and
comparatively lower development productivity. Vulkan&rsquo;s primary design focus remains graphics rendering; although its
compute capabilities are powerful, the ecosystem for general-purpose computing, including high-level library support and
overall ease of use, might be considered somewhat less mature compared to OpenCL or NVIDIA&rsquo;s CUDA / AMD&rsquo;s HIP. And, just
like OpenCL, the overhead of data transfer between host and device persists and needs careful management.</p><h3 id=hip-heterogeneous-compute-interface-for-portability>HIP (Heterogeneous-Compute Interface for Portability)<a hidden class=anchor aria-hidden=true href=#hip-heterogeneous-compute-interface-for-portability>#</a></h3><p>Now let&rsquo;s discuss HIP (Heterogeneous-Compute Interface for Portability). HIP is an integral part of AMD&rsquo;s ROCm (Radeon
Open Compute) platform, designed to provide a C++ GPU programming model very similar to NVIDIA&rsquo;s CUDA. One of its
primary goals is to simplify the process of porting existing CUDA code to run on AMD GPUs.</p><p>The host-side (Host Code) workflow for GPU computing using HIP is considerably more concise compared to OpenCL and
Vulkan, closely resembling the CUDA style. First, you need to allocate device memory for the input matrices A, B, and
the output matrix C on the target GPU device using the <code>hipMalloc()</code> function. Then, data is transferred from host
memory to device memory using <code>hipMemcpy()</code> (specifying <code>hipMemcpyHostToDevice</code> as the direction) for matrices A and B.
The core computational task is initiated by launching the kernel function (<code>matrix_multiply_hip_kernel</code>) using a syntax
very similar to CUDA&rsquo;s <code>&lt;&lt;&lt;GridDim, BlockDim>>></code> notation, which specifies the kernel&rsquo;s execution configuration.
<code>GridDim</code> defines the number of thread blocks (analogous to OpenCL workgroups) to launch, while <code>BlockDim</code> defines the
number of threads within each block (e.g., we might set it to 16x16). The grid dimensions usually need to be calculated
based on the total matrix size N and the chosen block dimensions to ensure the entire computation is covered. Since
kernel launches are asynchronous, the host code must call <code>hipDeviceSynchronize()</code> to wait for all computations on the
GPU to complete. After computation finishes, the results from the C matrix in device memory are transferred back to host
memory using <code>hipMemcpy()</code> (this time specifying <code>hipMemcpyDeviceToHost</code>). Finally, it&rsquo;s crucial to release all device
memory allocated earlier using the <code>hipFree()</code> function. Throughout this process, it&rsquo;s recommended to use our defined
<code>HIP_CHECK()</code> macro (which internally calls <code>hipGetErrorString</code>) to check the return value of every HIP API call for
timely error detection and handling.</p><p>The HIP device-side code (in the <code>matrix_mult_hip.hip</code> file) is written using standard C++ syntax along with some
HIP-specific extensions. Functions marked with the <code>__global__</code> keyword are kernel functions that can be launched from
the host using the <code>&lt;&lt;&lt;...>>></code> syntax. Inside the kernel function, built-in variables like <code>blockIdx</code> (index of the
current block within the grid), <code>threadIdx</code> (index of the current thread within its block), and <code>blockDim</code> (dimensions
of the block) are accessible. By combining these variables, we can calculate the global ID of the current thread (
corresponding to the <code>row</code> and <code>col</code> in the result matrix), similar to how global IDs are obtained in OpenCL/Vulkan (
e.g., via <code>get_global_id</code> or <code>gl_GlobalInvocationID</code>). The core computational logic of our matrix multiplication
kernel (the inner loop over <code>k</code>) is essentially the same as the OpenCL and Vulkan kernels we saw earlier.</p><p>Overall, HIP&rsquo;s main advantages are its provision of a C++ interface, which is generally easier to use and learn compared
to OpenCL&rsquo;s C API or the extremely verbose Vulkan API. Its high degree of syntactic similarity to CUDA significantly
facilitates porting existing CUDA codebases to the AMD platform. Being part of the ROCm platform, HIP is tightly
integrated with AMD&rsquo;s GPU drivers and toolchain (like the <code>hipcc</code> compiler), usually resulting in good performance and
compatibility. However, HIP also has disadvantages. It primarily targets AMD GPUs (although the HIP Clang project
provides some capability to run on certain NVIDIA GPUs, this isn&rsquo;t its main focus). Using HIP requires installing the
relatively large ROCm SDK. And, like all GPU computing solutions based on a discrete memory model, the overhead of data
transfer between the host and device remains a performance factor to consider.</p><h3 id=hipblas>hipBLAS<a hidden class=anchor aria-hidden=true href=#hipblas>#</a></h3><p>Finally, we arrive at hipBLAS. You can think of it as the BLAS library within the HIP ecosystem, analogous to cuBLAS in
the CUDA world or CLBlast in the OpenCL sphere. hipBLAS is the officially provided library from the ROCm platform,
offering Basic Linear Algebra Subprograms accelerated using HIP technology for AMD GPUs.</p><p>Using hipBLAS follows a pattern similar to other GPU BLAS libraries and is simpler than writing raw HIP kernels. First,
a functional HIP runtime environment is a prerequisite. Before using hipBLAS functions, you need to create a hipBLAS
handle, an object managing the library&rsquo;s internal state, via <code>hipblasHandle_t handle; hipblasCreate(&amp;handle);</code> for
initialization. Memory management proceeds as with HIP kernels: use <code>hipMalloc</code> to allocate memory for A, B, and C on
the GPU device, and use <code>hipMemcpy</code> to transfer host data to the device buffers for A and B. The core computation
involves calling the <code>hipblasDgemm()</code> function (&rsquo;d&rsquo; for double precision). Its parameter list closely resembles
<code>cblas_dgemm</code>, with key differences being the need to pass the previously created hipBLAS handle and the fact that the
pointers for A, B, and C must be device memory pointers. You also need to specify the operation for each matrix, e.g.,
whether it needs transposition (<code>HIPBLAS_OP_N</code> for no transpose). One crucial detail to pay attention to is that
hipBLAS, like many traditional BLAS libraries, defaults to expecting data stored in column-major order. However, C++
developers typically work with row-major storage. If our inputs A and B are row-major, and we want to compute the
row-major result C = A * B, calling <code>hipblasDgemm</code> directly requires careful handling of the data layout. A common trick
is to leverage the mathematical identity CT = BT * AT. This involves telling
<code>hipblasDgemm</code> to compute BT * AT (passing <code>HIPBLAS_OP_T</code> for both A and B), swapping the device
pointers passed for A and B, swapping their leading dimensions (lda, ldb), and also swapping the matrix dimensions M and
N. The resulting buffer computed this way is C transposed (CT stored in column-major order), which happens to
have the exact same memory layout as C stored in row-major order. Alternatively, a more direct approach, if supported by
your hipBLAS version, would be to check for an API function or setting that directly supports row-major inputs. However,
for our <code>matrix_multiply_hipblas</code> implementation, we assume it internally handles the layout correctly (perhaps via the
transpose trick or a newer interface) to provide behavior consistent with <code>cblas_dgemm</code>. Since the call executes
asynchronously, it&rsquo;s necessary to call <code>hipDeviceSynchronize()</code> to ensure the hipBLAS operation is completed and
synchronized. Afterwards, use <code>hipMemcpy</code> to copy the result from the device C buffer back to the host. Finally, don&rsquo;t
forget to destroy the hipBLAS handle using <code>hipblasDestroy(handle)</code> to release its resources. As always, using the
<code>HIPBLAS_CHECK()</code> macro to verify the status of each hipBLAS API call is recommended for robust error handling.</p><p>The primary advantages of hipBLAS are that it provides a standard BLAS interface, making high-performance linear algebra
on AMD GPUs relatively easy to use. The library contains HIP kernels that are highly optimized by AMD specifically for
their GPU architectures, thus usually delivering very high performance and effectively leveraging the hardware&rsquo;s
potential. Naturally, there are disadvantages too. Using hipBLAS depends on having the ROCm/HIP development environment
and the hipBLAS library correctly installed. Like all GPU acceleration methods, the cost of data transfer between host
and device remains. Furthermore, developers must pay close attention to handling the row-major versus column-major data
layout issue to ensure correct function calls and parameter settings.</p><p>Alright, all the contenders have been introduced. From simple serial loops to complex GPU programming, we&rsquo;ve covered a
spectrum of mainstream performance optimization ideas and technology stacks. Next up, let&rsquo;s see how they actually
performed in our benchmark tests!</p><h2 id=benchmarking-methodology>Benchmarking Methodology<a hidden class=anchor aria-hidden=true href=#benchmarking-methodology>#</a></h2><p>To ensure a fair comparison between these different implementations, we utilized the Google Benchmark framework. We also
designed a specific test fixture, named <code>MatrixMultFixture</code>, to manage the setup and teardown tasks associated with each
individual test run.</p><p>During the test setup (<code>SetUp</code>) phase for each test case, the program first determines the current square matrix size
<code>N</code> based on parameters passed by the Google Benchmark framework. It then allocates host (CPU) memory, typically using
<code>std::vector&lt;ValueType></code>, for the input matrices A and B, as well as for an output matrix C intended to store results
from CPU, SIMD, or some GPU implementations. Subsequently, matrices A and B are filled with random numbers. If the test
involves the Eigen or OpenCV libraries, their respective specific result matrices (like <code>C_eigen</code>, <code>C_cv</code>) are also
allocated at this stage. It&rsquo;s important to note that for technologies requiring a persistent global context, such as
OpenCL, Vulkan, and HIP, their initialization (e.g., via functions like <code>initOpenCL</code>, <code>initVulkan</code>) and final cleanup (
e.g., <code>cleanupOpenCL</code>) are performed once at the beginning and end of the entire benchmark program&rsquo;s execution (within
the <code>main</code> function), not within the per-test <code>SetUp</code> and <code>TearDown</code>. This avoids the significant overhead of repeatedly
initializing and destroying these heavyweight contexts for every single test iteration.</p><p>Next comes the test execution phase, driven by Google Benchmark&rsquo;s macros. Each distinct matrix multiplication
implementation corresponds to a separate Benchmark test function, for instance,
<code>BENCHMARK_F(MatrixMultFixture, BM_Naive)</code> signifies the test for the Naive implementation. Inside each such test
function, the core logic resides within a <code>for (auto _ : state)</code> loop controlled by Google Benchmark. Within this loop,
we invoke the specific matrix multiplication function currently being tested, such as
<code>matrix_multiply_naive(A, B, C, N)</code>. The Google Benchmark framework intelligently and automatically adjusts the number
of times this loop runs to ensure stable and reliable timing measurements are obtained. For libraries that necessitate
data mapping or wrapping (like Eigen and OpenCV), the mapping (<code>Eigen::Map</code>) or wrapper object creation (<code>cv::Mat</code>)
typically occurs inside this loop, but since these are usually zero-copy or low-overhead operations, their impact on the
performance measurement is minimal. For the GPU-accelerated implementations (including OpenCL, Vulkan, HIP, CLBlast,
hipBLAS), calling their respective execution functions usually encapsulates a sequence of operations: potentially
creating (or reusing) device-side memory buffers, transferring input data from host to device (Host-to-Device),
launching the computation kernel on the GPU, waiting for kernel execution to complete (synchronization), and finally
transferring the computed results back from device to host (Device-to-Host).</p><p>The test cleanup (<code>TearDown</code>) phase is relatively straightforward, mainly involving the release of the host memory
resources allocated during the <code>SetUp</code> phase, for example, by calling methods like <code>A.clear()</code>, <code>B.clear()</code>,
<code>C.clear()</code>, and so forth.</p><p>Regarding the test scope, we selected a range of N values for benchmarking, specifically 64, 128, 256, 512, and 1024.
Choosing these powers of two is a common practice in benchmarking, as it helps in observing performance trends as the
problem scale increases, particularly when plotted on logarithmic axes.</p><p>In terms of performance metrics, Google Benchmark primarily measures and reports <code>real_time</code>, which corresponds to the
wall-clock time elapsed. Based on this measured time (typically in nanoseconds, <code>ns</code>) and the current matrix size <code>N</code>,
we calculated a more informative core performance metric: GFLOPS (Giga Floating-point Operations Per Second). The
formula used was <code>GFLOPS = (2.0 * N^3) / (time_ns / 1e9)</code>. This calculation assumes that a standard square matrix
multiplication requires <code>2 * N^3</code> floating-point operations (roughly N^3 multiplications and N^3 additions). All
benchmark results were ultimately saved to a JSON formatted file named <code>benchmark_results.json</code> for convenient
post-processing.</p><p>Finally, for results visualization and easier comparison, we used Python along with the powerful data manipulation
library pandas and the plotting library matplotlib. A script reads the generated JSON file, parses the data, calculates
GFLOPS for each run, and then generates the performance comparison plot. In the plot, the X-axis represents the matrix
size N (using a base-2 logarithmic scale to better show power-of-two relationships), and the Y-axis represents the
performance in GFLOPS (also using a logarithmic scale to accommodate the vast differences in performance). This
graphical representation allows us to see the performance gaps between different implementations and their respective
scaling trends with problem size at a glance.</p><p>Now, let&rsquo;s see the final report card!</p><h2 id=performance-data-analysis>Performance Data Analysis<a hidden class=anchor aria-hidden=true href=#performance-data-analysis>#</a></h2><p>Please take a look at the performance comparison chart plotted from the benchmark results:</p><p><img alt="Matrix Multiplication Performance Comparison Plot" loading=lazy src=/images/qH2uR.png></p><p>To interpret this information-rich chart, let&rsquo;s first examine the axes. The X-axis represents the matrix size N,
spanning from 64 to 1024, and employs a base-2 logarithmic scale. The Y-axis denotes performance in GFLOPS (billions of
floating-point operations per second) and also uses a logarithmic scale. The choice of logarithmic scales is crucial
here; it helps to clearly display implementations with vastly different performance levels on the same graph and makes
it easier to observe the relative performance trends as N changes. The legend on the right side lists all the
implementation methods tested, along with their corresponding markers and colors, allowing easy identification of each
line.</p><p>Looking at the overall trends, several prominent patterns emerge. First, most implementations exhibit improved
performance as the matrix size N increases, reflected by the generally upward slope of the curves. This is expected
because for larger N, the total computational workload (which scales as O(N^3)) becomes much larger relative to fixed or
slower-growing overheads (like function call costs, GPU data transfer latencies, thread startup times, etc.). This
allows the benefits of parallelism and optimization to become more pronounced. Additionally, larger computational tasks
are better at amortizing memory access latencies. Second, there&rsquo;s an extremely wide range of performance across
different implementations, differing by orders of magnitude. This is strikingly evident when comparing the lowest
performer, the Naive implementation, to the top performer, hipBLAS (at N=1024). The performance gap exceeds 170,000
times! (Specifically, Naive at ~0.0006 GFLOPS vs. hipBLAS at ~102 GFLOPS). This dramatically underscores the importance
and potential impact of optimization. Third, we observe that some curves tend to flatten out or even slightly decrease
at larger values of N. This typically indicates that the performance of that implementation is hitting a bottleneck
under the current conditions. Such bottlenecks could be varied, including saturated memory bandwidth (data can&rsquo;t be
supplied fast enough), insufficient CPU or GPU cache capacity for the working set causing lower cache hit rates,
reaching the limit of GPU core utilization, or perhaps certain unoptimized overheads growing linearly or faster with N,
starting to negate the computational speedup.</p><p>To analyze the performance data more deeply, we can group the implementations and compare them within and across groups.</p><p>First, let&rsquo;s look at the CPU Basic Group, comparing the simplest Naive implementation against the version using only
OpenMP for parallelism. The Naive implementation (yellow &lsquo;+&rsquo; marker) is undeniably the slowest. Its curve hugs the
bottom of the chart on the log scale, showing very little growth with N, reaching only about 0.6 GFLOPS at N=1024 (
<em>re-reading based on plot, correcting potential misinterpretation of raw data; GFLOPS derived from JSON</em>). In contrast,
the OpenMP version (orange square marker), leveraging the CPU&rsquo;s 20 threads, shows a marked improvement, achieving around
4 GFLOPS at N=1024, roughly 6-7 times faster than Naive. Nevertheless, compared to more advanced optimization
techniques, this is still quite slow. Its relatively flat performance curve suggests that simple multi-core parallelism
might quickly become limited by factors like memory bandwidth.</p><p>Next is the CPU SIMD Group, where we examine the impact of using AVX2 and AVX-512 instructions, both alone and combined
with OpenMP. The single-threaded AVX2+FMA implementation (dark blue circle) already demonstrates the power of
vectorization, delivering respectable performance (~1.7 GFLOPS at N=1024), even slightly outperforming the pure OpenMP
version for N &lt; 512. Moving to AVX512+FMA (green triangle) yields further speedup, as the 512-bit vectors can process
twice the data per instruction compared to AVX2, reaching about 2.4 GFLOPS at N=1024. The real performance leap occurs
when combining SIMD with multi-threading. AVX2+FMA_OMP (red diamond) achieves roughly 9.5 GFLOPS at N=1024, more than 5
times faster than single-threaded AVX2 and over twice as fast as pure OpenMP. The champion within this group, and indeed
the top performer among all CPU implementations tested, is AVX512+FMA_OMP (purple inverted triangle). By combining the
widest available SIMD vectors with multi-core parallelism, it hits an impressive 15 GFLOPS at N=1024, about a 60%
improvement over the AVX2+OMP version. Its line sits at the pinnacle of the CPU-only results.</p><p>Now, let&rsquo;s consider the CPU Professional Library Group, comparing BLAS, Eigen, and OpenCV. BLAS (purple &lsquo;V&rsquo; marker)
delivered excellent performance, reaching approximately 53 GFLOPS at N=1024 (<em>correction based on re-reading the plot</em>),
nearly matching or slightly exceeding our best manually tuned CPU code (AVX512+FMA_OMP). This strongly indicates that
the BLAS library installed on the system (likely OpenBLAS) is extremely well-optimized internally, effectively utilizing
both SIMD instructions and multi-threading. Equally impressive was OpenCVLib (light blue circle), whose performance
closely tracked BLAS, even slightly surpassing it at N=1024 with about 54 GFLOPS. This suggests that OpenCV&rsquo;s <code>gemm</code>
implementation benefits from powerful backend optimizations, possibly by calling an optimized BLAS library or another
performance kernel library like IPP internally. However, EigenLib (pink star) showed surprisingly poor performance in
this specific test, lagging behind even the basic OpenMP version and achieving only about 0.7 GFLOPS at N=1024. This
contrasts sharply with Eigen&rsquo;s generally high-performance reputation. Possible reasons for this anomaly could include
suboptimal usage of Eigen in the test code (though unlikely if using standard operations), the compiler failing to
adequately optimize Eigen&rsquo;s expression templates for this specific case, or perhaps compatibility issues between the
particular Eigen version and the test environment. Therefore, this result for Eigen should be viewed with caution and
not generalized; it&rsquo;s likely specific to the conditions of this benchmark.</p><p>Finally, we examine the GPU Acceleration Group, comprising implementations using OpenCL, Vulkan, HIP, and the
corresponding BLAS libraries CLBlast and hipBLAS. A general trend across all GPU methods is that their performance tends
to be lower than well-optimized CPU methods (like BLAS or AVX+OMP) at smaller matrix sizes (e.g., N=64), sometimes even
slower than Naive+OpenMP. This is primarily due to the overhead associated with GPU computing, namely the time spent
transferring data between the CPU and GPU (Host-to-Device and Device-to-Host) and the latency involved in launching the
GPU kernel. For small tasks, these fixed overheads constitute a large portion of the total execution time. However, as N
increases, the massive parallel processing capability of the GPU dominates, and their performance curves rise rapidly,
quickly surpassing all CPU-based implementations.</p><p>Among the manually written kernels (where we coded the computation logic in OpenCL C, GLSL, or HIP C++), OpenCL (cyan
diamond) performed quite well, reaching about 58 GFLOPS at N=1024, with a steep curve indicating good scalability.
Vulkan (green up-triangle) also delivered good performance, although slightly lower than OpenCL and the HIP kernel, at
around 29 GFLOPS for N=1024. Given Vulkan&rsquo;s API complexity, this result seems reasonable, possibly leaving room for
further driver or shader optimization. The HIP kernel (gray &lsquo;X&rsquo; marker) exhibited anomalously low performance at N=64 (
potentially due to measurement error or an initialization glitch), but for N=128 and larger, its performance quickly
caught up and closely mirrored that of OpenCL, reaching about 57 GFLOPS at N=1024. This suggests that for this
relatively simple kernel, the underlying execution efficiency of HIP and OpenCL on this particular AMD GPU is quite
similar.</p><p>Performance took another significant jump when using the GPU BLAS libraries. CLBlast (brown diamond), being the OpenCL
BLAS library, far outperformed our handwritten OpenCL kernel, achieving roughly 95 GFLOPS at N=1024. This highlights the
value of specialized library optimizations; CLBlast likely employs more sophisticated techniques internally, such as
advanced memory access patterns, data tiling, and efficient use of GPU shared memory (LDS). The undisputed overall
winner of this entire benchmark was hipBLAS (red down-triangle). As the native BLAS library for AMD&rsquo;s ROCm platform, it
delivered the most outstanding performance, breaking the 100 GFLOPS barrier at N=1024 and reaching approximately 102
GFLOPS. This typically signifies that hipBLAS is best able to leverage the specific hardware features and instructions
of the AMD GPU.</p><p>Let&rsquo;s briefly summarize the highlights and points of caution from this benchmark. The clear performance leaders at
N=1024 were the GPU BLAS libraries, hipBLAS and CLBlast. Within the CPU realm, the system BLAS library, OpenCV, and the
manually crafted AVX512+FMA+OMP implementation were the top contenders. The sheer magnitude of performance improvement
observed was astounding: from the basic Naive method to the fastest hipBLAS implementation, the speedup at N=1024
exceeded a factor of 170,000! The advantage of using GPUs became evident for matrix sizes of N=256 and larger in our
tests, with the gap widening as N increased. This also underscored the importance of using professional libraries like
BLAS, CLBlast, hipBLAS, and even OpenCV, which often outperform manual optimization efforts (especially simpler custom
GPU kernels) by encapsulating extensive hardware-specific tuning. On the cautionary side, the anomalously poor
performance of Eigen in this specific test warrants further investigation and should not be taken as a general statement
about Eigen&rsquo;s capabilities. Similarly, the outlier result for the HIP kernel at N=64 suggests that this particular data
point might be invalid and should be treated carefully.</p><p>In essence, this performance showdown vividly illustrates the vast differences that various technological approaches can
make. From elementary CPU loops to intricate GPU programming, every optimization technique has its rationale and optimal
use case.</p><h2 id=deeper-dive-discussion--caveats>Deeper Dive: Discussion & Caveats<a hidden class=anchor aria-hidden=true href=#deeper-dive-discussion--caveats>#</a></h2><p>While this performance benchmark provides us with a wealth of direct data, it also prompts further reflection and
requires acknowledging certain limitations and important considerations when interpreting the results.</p><p>First and foremost, the results are highly hardware-dependent. All tests were conducted on a specific platform featuring
an AMD Ryzen AI 9 processor paired with a Radeon 880M integrated GPU. Running the same benchmarks on different hardware,
such as an Intel CPU or an NVIDIA GPU, could yield dramatically different outcomes and performance rankings. For
example, Intel CPUs often show exceptional performance when coupled with Intel&rsquo;s own MKL (Math Kernel Library), while
NVIDIA GPUs would necessitate the use of the CUDA programming model and the cuBLAS library to achieve their best
results.</p><p>Second, the choice of compiler and library versions can significantly influence the outcome. The specific version of GCC
or Clang used, the selected optimization flags (e.g., <code>-Ofast</code> versus <code>-O3</code> might trade precision or standard
conformance for speed), and the particular version and build configuration of mathematical libraries like BLAS, OpenCV,
or Eigen can all impact the final performance numbers. For instance, substituting OpenBLAS with MKL on an Intel CPU
could lead to completely different BLAS performance results.</p><p>Furthermore, the data type and matrix characteristics are crucial factors. This benchmark exclusively used <code>double</code> (
64-bit double-precision floating-point numbers) for square matrices. If we were to switch to <code>float</code> (32-bit
single-precision), performance would generally be higher due to halved data volume reducing memory bandwidth pressure,
SIMD instructions processing twice as many elements per operation, and some hardware intrinsically favoring
single-precision computations. Additionally, our tests focused on dense square matrices. For matrices with special
structures like sparsity, symmetry, or bandedness, employing specialized storage formats, algorithms, and dedicated
libraries is essential for efficient computation.</p><p>Moreover, GFLOPS isn&rsquo;t the whole story. While GFLOPS is a vital metric for gauging raw computational throughput, it
doesn&rsquo;t capture the full picture of real-world application performance. Especially in the context of GPU computing, the
time spent transferring data between the host (CPU) and the device (GPU) – operations like <code>hipMemcpy</code> or
<code>clEnqueueWrite/ReadBuffer</code> – constitutes an integral part of the total task duration. Our benchmark, likely focusing on
the time spent within the Google Benchmark loop, might primarily measure the core computation time and potentially
underrepresent or exclude the full data transfer overhead. In practical applications, the end-to-end execution time is
what truly matters. For small matrix problems, this data transfer overhead can even dominate the overall time.</p><p>We must also consider the trade-offs between implementation complexity and ease of use. The highest-performing
solutions, such as hipBLAS or CLBlast, while relatively simple to <em>use</em> (calling library functions), rely on the user
having the specific SDKs (like ROCm) and environments correctly installed and configured. On the other hand, manually
writing SIMD intrinsics or GPU kernel code (for OpenCL, Vulkan, or HIP) might offer finer control over performance but
demands deep expertise in low-level hardware details and parallel programming, often involving significant development,
debugging, and optimization effort. The Naive and OpenMP approaches are the simplest to implement but yield the poorest
performance. Therefore, selecting the right implementation method for a real-world project requires careful balancing
between performance requirements, development costs, code portability, and long-term maintainability.</p><p>It&rsquo;s also worth acknowledging that regarding cache optimization, the CPU SIMD and GPU kernels (OpenCL/Vulkan/HIP) that
we manually implemented were relatively basic and did not incorporate sophisticated data blocking (or tiling)
strategies. Blocking is an advanced optimization technique that involves partitioning large matrices into smaller
sub-matrices (blocks) and performing computations block-wise. Its main goal is to maximize the utilization of CPU or GPU
caches by improving data locality and cache hit rates. This technique is one of the core reasons why high-performance
BLAS libraries achieve near-peak hardware performance. If we were to implement complex blocking in our manual code,
their performance might improve further, but at the cost of a dramatic increase in code complexity.</p><p>Finally, the anomalous results observed for the Eigen library and the HIP kernel at N=64 serve as a reminder that
benchmark results should always be interpreted critically. When encountering data that starkly contradicts expectations,
one should resist jumping to immediate conclusions and instead try to investigate potential causes – could it be a bug
in the code, an issue with compilation flags, measurement inaccuracies, interference from other system processes, or
perhaps a compatibility problem specific to the test environment? Only through careful scrutiny and validation can we
gain confidence in the benchmark findings.</p><h2 id=the-finish-line-conclusion--outlook>The Finish Line: Conclusion & Outlook<a hidden class=anchor aria-hidden=true href=#the-finish-line-conclusion--outlook>#</a></h2><p>Having journeyed through this comprehensive matrix multiplication performance showdown—spanning CPUs and GPUs, serial
and parallel approaches, manual optimizations, and professional libraries—we can draw several clear conclusions.</p><p>First and foremost, optimization is absolutely crucial. The chasm in performance between the most basic Naive
implementation and highly optimized solutions is immense, vividly demonstrating that for compute-intensive tasks,
selecting the right algorithms and implementation techniques is paramount for achieving acceptable, let alone excellent,
performance. Second, leveraging hardware features yields significant rewards. Utilizing modern CPU capabilities like
multi-core processing (e.g., via OpenMP) and SIMD instruction sets (either through manual intrinsics or library-provided
automatic vectorization) provides substantial speedups; combining these two often pushes CPU performance towards its
practical limits. Third, the potential for GPU acceleration is enormous. For computational tasks of sufficient scale (in
our tests, starting around N=256), the massively parallel architecture of GPUs enables performance levels far exceeding
what CPUs can offer. Fourth, it highlights the value of making good use of professional libraries. Specialized math
libraries such as BLAS (and its various implementations like OpenBLAS, MKL, AOCL-BLAS), CLBlast, hipBLAS (or cuBLAS for
NVIDIA), encapsulate a vast amount of low-level optimization expertise. Employing them is frequently the most effective
path to achieving both high performance and good development productivity. Even higher-level libraries like OpenCV may
rely on these optimized backends internally. However, we must also recognize that there is no &ldquo;silver bullet&rdquo; in
performance optimization; no single method reigns supreme in all scenarios. Small-scale problems might favor CPU
implementations due to avoided data transfer overheads, while large-scale problems clearly benefit from GPU
acceleration. The optimal choice will invariably depend on the specific hardware platform, the required precision (
single vs. double), and the available development resources and constraints. Finally, all these findings point towards
the importance of continuous learning and hands-on practice. High-performance computing is a rapidly evolving field with
constant advancements in hardware architectures, programming models, and compiler technologies. Maintaining curiosity,
persistently learning new techniques, and personally testing and validating assumptions are the keys to truly mastering
the art and science of performance optimization.</p><p>Hopefully, this exploration into the performance landscape of matrix multiplication has provided everyone with a more
tangible understanding of the diverse computing technologies available. From the humble three nested loops to blistering
speeds exceeding one hundred GFLOPS, the journey reflects the culmination of ingenuity in computer architecture,
parallel computing, and software engineering. Perhaps the next time you&rsquo;re faced with a task involving large-scale
matrix operations, you&rsquo;ll recall the contenders we discussed today and feel more equipped to choose the most suitable
acceleration strategy for your application!</p><h2 id=appendix-benchmark-results-data>Appendix: Benchmark Results Data<a hidden class=anchor aria-hidden=true href=#appendix-benchmark-results-data>#</a></h2><p>Table omitted as requested.</p><table><thead><tr><th style=text-align:left>Implementation</th><th style=text-align:left>Matrix Size (N)</th><th style=text-align:left>Real Time (ns)</th><th style=text-align:left>Performance (GFLOPS)</th></tr></thead><tbody><tr><td style=text-align:left>Naive</td><td style=text-align:left>64</td><td style=text-align:left>640,561</td><td style=text-align:left>0.818</td></tr><tr><td style=text-align:left>Naive</td><td style=text-align:left>128</td><td style=text-align:left>5,250,421</td><td style=text-align:left>0.799</td></tr><tr><td style=text-align:left>Naive</td><td style=text-align:left>256</td><td style=text-align:left>42,393,811</td><td style=text-align:left>0.791</td></tr><tr><td style=text-align:left>Naive</td><td style=text-align:left>512</td><td style=text-align:left>569,762,981</td><td style=text-align:left>0.471</td></tr><tr><td style=text-align:left>Naive</td><td style=text-align:left>1024</td><td style=text-align:left>3,447,583,101</td><td style=text-align:left>0.623</td></tr><tr><td style=text-align:left>OpenMP</td><td style=text-align:left>64</td><td style=text-align:left>149,270</td><td style=text-align:left>3.512</td></tr><tr><td style=text-align:left>OpenMP</td><td style=text-align:left>128</td><td style=text-align:left>1,036,590</td><td style=text-align:left>4.046</td></tr><tr><td style=text-align:left>OpenMP</td><td style=text-align:left>256</td><td style=text-align:left>6,844,282</td><td style=text-align:left>4.903</td></tr><tr><td style=text-align:left>OpenMP</td><td style=text-align:left>512</td><td style=text-align:left>62,077,042</td><td style=text-align:left>4.324</td></tr><tr><td style=text-align:left>OpenMP</td><td style=text-align:left>1024</td><td style=text-align:left>578,410,614</td><td style=text-align:left>3.713</td></tr><tr><td style=text-align:left>AVX2+FMA</td><td style=text-align:left>64</td><td style=text-align:left>311,178</td><td style=text-align:left>1.685</td></tr><tr><td style=text-align:left>AVX2+FMA</td><td style=text-align:left>128</td><td style=text-align:left>2,505,685</td><td style=text-align:left>1.674</td></tr><tr><td style=text-align:left>AVX2+FMA</td><td style=text-align:left>256</td><td style=text-align:left>19,324,494</td><td style=text-align:left>1.736</td></tr><tr><td style=text-align:left>AVX2+FMA</td><td style=text-align:left>512</td><td style=text-align:left>152,734,950</td><td style=text-align:left>1.758</td></tr><tr><td style=text-align:left>AVX2+FMA</td><td style=text-align:left>1024</td><td style=text-align:left>1,237,421,611</td><td style=text-align:left>1.735</td></tr><tr><td style=text-align:left>AVX512+FMA</td><td style=text-align:left>64</td><td style=text-align:left>221,951</td><td style=text-align:left>2.362</td></tr><tr><td style=text-align:left>AVX512+FMA</td><td style=text-align:left>128</td><td style=text-align:left>1,702,158</td><td style=text-align:left>2.464</td></tr><tr><td style=text-align:left>AVX512+FMA</td><td style=text-align:left>256</td><td style=text-align:left>14,094,445</td><td style=text-align:left>2.381</td></tr><tr><td style=text-align:left>AVX512+FMA</td><td style=text-align:left>512</td><td style=text-align:left>107,877,880</td><td style=text-align:left>2.488</td></tr><tr><td style=text-align:left>AVX512+FMA</td><td style=text-align:left>1024</td><td style=text-align:left>921,593,993</td><td style=text-align:left>2.330</td></tr><tr><td style=text-align:left>AVX2+FMA_OMP</td><td style=text-align:left>64</td><td style=text-align:left>90,276</td><td style=text-align:left>5.808</td></tr><tr><td style=text-align:left>AVX2+FMA_OMP</td><td style=text-align:left>128</td><td style=text-align:left>664,552</td><td style=text-align:left>6.311</td></tr><tr><td style=text-align:left>AVX2+FMA_OMP</td><td style=text-align:left>256</td><td style=text-align:left>3,656,076</td><td style=text-align:left>9.178</td></tr><tr><td style=text-align:left>AVX2+FMA_OMP</td><td style=text-align:left>512</td><td style=text-align:left>27,922,787</td><td style=text-align:left>9.613</td></tr><tr><td style=text-align:left>AVX2+FMA_OMP</td><td style=text-align:left>1024</td><td style=text-align:left>216,519,971</td><td style=text-align:left>9.918</td></tr><tr><td style=text-align:left>AVX512+FMA_OMP</td><td style=text-align:left>64</td><td style=text-align:left>86,896</td><td style=text-align:left>6.033</td></tr><tr><td style=text-align:left>AVX512+FMA_OMP</td><td style=text-align:left>128</td><td style=text-align:left>427,994</td><td style=text-align:left>9.799</td></tr><tr><td style=text-align:left>AVX512+FMA_OMP</td><td style=text-align:left>256</td><td style=text-align:left>2,648,926</td><td style=text-align:left>12.667</td></tr><tr><td style=text-align:left>AVX512+FMA_OMP</td><td style=text-align:left>512</td><td style=text-align:left>18,439,355</td><td style=text-align:left>14.558</td></tr><tr><td style=text-align:left>AVX512+FMA_OMP</td><td style=text-align:left>1024</td><td style=text-align:left>140,055,382</td><td style=text-align:left>15.333</td></tr><tr><td style=text-align:left>Eigen</td><td style=text-align:left>64</td><td style=text-align:left>904,785</td><td style=text-align:left>0.579</td></tr><tr><td style=text-align:left>Eigen</td><td style=text-align:left>128</td><td style=text-align:left>12,846,593</td><td style=text-align:left>0.326</td></tr><tr><td style=text-align:left>Eigen</td><td style=text-align:left>256</td><td style=text-align:left>32,201,997</td><td style=text-align:left>1.042</td></tr><tr><td style=text-align:left>Eigen</td><td style=text-align:left>512</td><td style=text-align:left>284,153,414</td><td style=text-align:left>0.945</td></tr><tr><td style=text-align:left>Eigen</td><td style=text-align:left>1024</td><td style=text-align:left>2,316,560,842</td><td style=text-align:left>0.927</td></tr><tr><td style=text-align:left>OpenCV</td><td style=text-align:left>64</td><td style=text-align:left>33,326</td><td style=text-align:left>15.732</td></tr><tr><td style=text-align:left>OpenCV</td><td style=text-align:left>128</td><td style=text-align:left>73,443</td><td style=text-align:left>57.110</td></tr><tr><td style=text-align:left>OpenCV</td><td style=text-align:left>256</td><td style=text-align:left>538,501</td><td style=text-align:left>62.311</td></tr><tr><td style=text-align:left>OpenCV</td><td style=text-align:left>512</td><td style=text-align:left>4,811,569</td><td style=text-align:left>55.790</td></tr><tr><td style=text-align:left>OpenCV</td><td style=text-align:left>1024</td><td style=text-align:left>36,290,270</td><td style=text-align:left>59.175</td></tr><tr><td style=text-align:left>BLAS</td><td style=text-align:left>64</td><td style=text-align:left>10,609</td><td style=text-align:left>49.420</td></tr><tr><td style=text-align:left>BLAS</td><td style=text-align:left>128</td><td style=text-align:left>73,929</td><td style=text-align:left>56.734</td></tr><tr><td style=text-align:left>BLAS</td><td style=text-align:left>256</td><td style=text-align:left>535,021</td><td style=text-align:left>62.716</td></tr><tr><td style=text-align:left>BLAS</td><td style=text-align:left>512</td><td style=text-align:left>5,210,261</td><td style=text-align:left>51.521</td></tr><tr><td style=text-align:left>BLAS</td><td style=text-align:left>1024</td><td style=text-align:left>36,608,529</td><td style=text-align:left>58.661</td></tr><tr><td style=text-align:left>Vulkan</td><td style=text-align:left>64</td><td style=text-align:left>258,650</td><td style=text-align:left>2.027</td></tr><tr><td style=text-align:left>Vulkan</td><td style=text-align:left>128</td><td style=text-align:left>850,222</td><td style=text-align:left>4.933</td></tr><tr><td style=text-align:left>Vulkan</td><td style=text-align:left>256</td><td style=text-align:left>2,015,570</td><td style=text-align:left>16.648</td></tr><tr><td style=text-align:left>Vulkan</td><td style=text-align:left>512</td><td style=text-align:left>15,517,304</td><td style=text-align:left>17.300</td></tr><tr><td style=text-align:left>Vulkan</td><td style=text-align:left>1024</td><td style=text-align:left>69,655,183</td><td style=text-align:left>30.830</td></tr><tr><td style=text-align:left>OpenCL</td><td style=text-align:left>64</td><td style=text-align:left>69,397</td><td style=text-align:left>7.555</td></tr><tr><td style=text-align:left>OpenCL</td><td style=text-align:left>128</td><td style=text-align:left>147,861</td><td style=text-align:left>28.367</td></tr><tr><td style=text-align:left>OpenCL</td><td style=text-align:left>256</td><td style=text-align:left>593,376</td><td style=text-align:left>56.548</td></tr><tr><td style=text-align:left>OpenCL</td><td style=text-align:left>512</td><td style=text-align:left>5,842,253</td><td style=text-align:left>45.947</td></tr><tr><td style=text-align:left>OpenCL</td><td style=text-align:left>1024</td><td style=text-align:left>38,429,528</td><td style=text-align:left>55.881</td></tr><tr><td style=text-align:left>CLBlast</td><td style=text-align:left>64</td><td style=text-align:left>61,002</td><td style=text-align:left>8.595</td></tr><tr><td style=text-align:left>CLBlast</td><td style=text-align:left>128</td><td style=text-align:left>127,007</td><td style=text-align:left>33.024</td></tr><tr><td style=text-align:left>CLBlast</td><td style=text-align:left>256</td><td style=text-align:left>426,358</td><td style=text-align:left>78.700</td></tr><tr><td style=text-align:left>CLBlast</td><td style=text-align:left>512</td><td style=text-align:left>3,740,453</td><td style=text-align:left>71.765</td></tr><tr><td style=text-align:left>CLBlast</td><td style=text-align:left>1024</td><td style=text-align:left>20,777,060</td><td style=text-align:left>103.358</td></tr><tr><td style=text-align:left>HIP</td><td style=text-align:left>64</td><td style=text-align:left>856,032,739</td><td style=text-align:left>0.000612</td></tr><tr><td style=text-align:left>HIP</td><td style=text-align:left>128</td><td style=text-align:left>171,225</td><td style=text-align:left>24.496</td></tr><tr><td style=text-align:left>HIP</td><td style=text-align:left>256</td><td style=text-align:left>613,603</td><td style=text-align:left>54.684</td></tr><tr><td style=text-align:left>HIP</td><td style=text-align:left>512</td><td style=text-align:left>5,788,911</td><td style=text-align:left>46.371</td></tr><tr><td style=text-align:left>HIP</td><td style=text-align:left>1024</td><td style=text-align:left>38,210,712</td><td style=text-align:left>56.201</td></tr><tr><td style=text-align:left>hipBLAS</td><td style=text-align:left>64</td><td style=text-align:left>2,080,484</td><td style=text-align:left>0.252</td></tr><tr><td style=text-align:left>hipBLAS</td><td style=text-align:left>128</td><td style=text-align:left>2,146,978</td><td style=text-align:left>1.954</td></tr><tr><td style=text-align:left>hipBLAS</td><td style=text-align:left>256</td><td style=text-align:left>2,691,232</td><td style=text-align:left>12.468</td></tr><tr><td style=text-align:left>hipBLAS</td><td style=text-align:left>512</td><td style=text-align:left>5,960,233</td><td style=text-align:left>45.038</td></tr><tr><td style=text-align:left>hipBLAS</td><td style=text-align:left>1024</td><td style=text-align:left>21,356,498</td><td style=text-align:left>100.554</td></tr></tbody></table></div><footer class=post-footer><ul class=post-tags><li><a href=https://tategotoazarasi.github.io/en/tags/matrix-multiplication/>Matrix-Multiplication</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/performance/>Performance</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/benchmark/>Benchmark</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/optimization/>Optimization</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/comparison/>Comparison</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/cpu/>Cpu</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/gpu/>Gpu</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/amd/>Amd</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/ryzen-ai/>Ryzen-Ai</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/radeon/>Radeon</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/gfx1150/>Gfx1150</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/avx/>Avx</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/avx2/>Avx2</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/avx-512/>Avx-512</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/fma/>Fma</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/openmp/>Openmp</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/blas/>Blas</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/eigen/>Eigen</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/opencv/>Opencv</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/opencl/>Opencl</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/clblast/>Clblast</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/hip/>Hip</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/rocm/>Rocm</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/hipblas/>Hipblas</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/vulkan/>Vulkan</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/compute-shader/>Compute-Shader</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/linear-algebra/>Linear-Algebra</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/high-performance-computing/>High-Performance-Computing</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/hpc/>Hpc</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/parallel-computing/>Parallel-Computing</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/vectorization/>Vectorization</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/simd/>Simd</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/gpgpu/>Gpgpu</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/google-benchmark/>Google-Benchmark</a></li><li><a href=https://tategotoazarasi.github.io/en/tags/c-plus-plus/>C-Plus-Plus</a></li></ul><nav class=paginav><a class=prev href=https://tategotoazarasi.github.io/en/posts/automating-online-grading-with-tampermonkey-and-ai/><span class=title>« Prev</span><br><span>Automating Online Grading with Tampermonkey and AI</span>
</a><a class=next href=https://tategotoazarasi.github.io/en/posts/beyond-basic-bridging-robust-eventing-between-cpp-entt-and-rust-wasm-with-boost-signals2/><span class=title>Next »</span><br><span>Beyond Basic Bridging: Robust Eventing Between C++ EnTT and Rust WASM with Boost.Signals2</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://tategotoazarasi.github.io/en/>Tategoto Azarasi</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>