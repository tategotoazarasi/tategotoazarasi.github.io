<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Reproducing RetinaSim on an HPC Cluster | Tategoto Azarasi</title>
<meta name=keywords content="retinasim,hpc,scientific-computing,reproduction,cmake,slurm,spack,dotnet,c-plus-plus,python,debugging,open3d,headless-rendering"><meta name=description content="A detailed account of reproducing the complex multi-language biomedical simulation project RetinaSim on a high-performance computing (HPC) cluster, covering every technical detail from debugging compilation errors to adapting to headless rendering environments."><meta name=author content="Tategoto Azarasi"><link rel=canonical href=https://blog.tategotoazarasi.me/en/posts/reproducing-retinasim-on-hpc-cluster/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=https://blog.tategotoazarasi.me/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://blog.tategotoazarasi.me/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://blog.tategotoazarasi.me/favicon-32x32.png><link rel=apple-touch-icon href=https://blog.tategotoazarasi.me/apple-touch-icon.png><link rel=mask-icon href=https://blog.tategotoazarasi.me/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://blog.tategotoazarasi.me/en/posts/reproducing-retinasim-on-hpc-cluster/><link rel=alternate hreflang=zh href=https://blog.tategotoazarasi.me/zh/posts/reproducing-retinasim-on-hpc-cluster/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link crossorigin=anonymous href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ rel=stylesheet><script crossorigin=anonymous defer integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js></script><script crossorigin=anonymous defer integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR onload=renderMathInElement(document.body) src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js></script>>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:url" content="https://blog.tategotoazarasi.me/en/posts/reproducing-retinasim-on-hpc-cluster/"><meta property="og:site_name" content="Tategoto Azarasi"><meta property="og:title" content="Reproducing RetinaSim on an HPC Cluster"><meta property="og:description" content="A detailed account of reproducing the complex multi-language biomedical simulation project RetinaSim on a high-performance computing (HPC) cluster, covering every technical detail from debugging compilation errors to adapting to headless rendering environments."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-12-28T23:38:53+00:00"><meta property="article:modified_time" content="2025-12-28T23:38:53+00:00"><meta property="article:tag" content="Retinasim"><meta property="article:tag" content="Hpc"><meta property="article:tag" content="Scientific-Computing"><meta property="article:tag" content="Reproduction"><meta property="article:tag" content="Cmake"><meta property="article:tag" content="Slurm"><meta name=twitter:card content="summary"><meta name=twitter:title content="Reproducing RetinaSim on an HPC Cluster"><meta name=twitter:description content="A detailed account of reproducing the complex multi-language biomedical simulation project RetinaSim on a high-performance computing (HPC) cluster, covering every technical detail from debugging compilation errors to adapting to headless rendering environments."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blog.tategotoazarasi.me/en/posts/"},{"@type":"ListItem","position":2,"name":"Reproducing RetinaSim on an HPC Cluster","item":"https://blog.tategotoazarasi.me/en/posts/reproducing-retinasim-on-hpc-cluster/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Reproducing RetinaSim on an HPC Cluster","name":"Reproducing RetinaSim on an HPC Cluster","description":"A detailed account of reproducing the complex multi-language biomedical simulation project RetinaSim on a high-performance computing (HPC) cluster, covering every technical detail from debugging compilation errors to adapting to headless rendering environments.","keywords":["retinasim","hpc","scientific-computing","reproduction","cmake","slurm","spack","dotnet","c-plus-plus","python","debugging","open3d","headless-rendering"],"articleBody":"Recently, I undertook a rather challenging task: to fully reproduce a paper titled “Physics-informed deep generative learning for quantitative assessment of the retina” on a High-Performance Computing (HPC) cluster. The core software repository for this paper is RetinaSim. The goal was not merely to run the code, but to completely replicate its complex software stack and simulation workflow in a strictly managed computational environment, one that likely differed significantly from the original developers’. This blog post will chronicle my entire journey from the initial attempt to the final successful run, focusing on my chain of thought as I diagnosed and resolved a series of tricky issues.\nOur battlefield was a typical HPC cluster, “Barkla2,” running Rocky Linux 9 and using Slurm as its job scheduler. This meant all operations had to be performed via the command line, and any time-consuming computational tasks had to be submitted as batch jobs rather than run directly on the login node. The RetinaSim project itself is a complex hybrid, merging Python scripts for main workflow control with a high-performance fluid dynamics simulator (Reanimate) written in C++ and a vessel generation program (RetinaGen) written in .NET (C#). This heterogeneous technology stack almost guaranteed that we would encounter a variety of unexpected compilation and runtime problems in a new environment.\nMy first step was to obtain the code and draft an initial Slurm script. The code was cloned via git, and its directory structure clearly laid out the various submodules. After a preliminary analysis of files like README and CMakeLists.txt, I understood that compiling Reanimate required CMake and a C++ compiler, while RetinaGen needed the .NET SDK. The Python part depended on a requirements.txt file.\nBased on this information, I wrote the first version of my Slurm script. The script’s goal was to complete all preparatory work sequentially: load the necessary environment modules (like the GCC compiler, CMake, and Python), create a Python virtual environment and install dependencies, compile the C++ and .NET submodules, and finally, attempt to run the main Python script, main.py. This was a standard, seemingly straightforward process. However, reality quickly delivered its first blow.\nFirst Failure: CMakeLists.txt not found Seconds after submitting the first job, it failed. Checking the error log, I saw a familiar and fundamental error message: CMake Error: The source directory \"...\" does not appear to contain CMakeLists.txt. This error means that CMake could not find its core configuration file, CMakeLists.txt, in the directory I had specified.\nMy immediate reaction was to check the paths in my script. To compile the Reanimate submodule, my Slurm script had changed directory to retinasim/Reanimate/Reanimate and then executed the cmake . command. This command tells CMake to use the current directory as the root of the source tree. However, after carefully inspecting the project structure with the ls -R command, I discovered that the CMakeLists.txt file was actually located in the retinasim/Reanimate directory, not in its subdirectory retinasim/Reanimate/Reanimate. This was a classic relative path error, one that is easy to make, especially when dealing with nested subprojects.\nThe diagnosis was straightforward. Since CMakeLists.txt was in the parent directory, the solution was to tell CMake to look there. In Unix-like systems, “..” represents the parent directory. Therefore, I needed to change the compile command from cmake . to cmake ... This small change had significant meaning: it instructed CMake to look one level up from the current directory (Reanimate/Reanimate) for CMakeLists.txt, while still using the current Reanimate/Reanimate directory as the build directory. This way, CMake could find the configuration file and place all generated build files (like Makefiles, object files, and the final executable) in my current location, keeping the source tree clean.\nThis small episode, though simple, served as a reminder that when dealing with unfamiliar and complex projects, especially those involving multiple languages and build systems, the first step is always to carefully and patiently review the directory structure and build scripts. Assuming that configuration files will be in some “obvious” location is a common cause of elementary errors. After fixing this issue, I resubmitted the job with renewed confidence, hoping the compilation process would now proceed smoothly. However, the complexity of the HPC environment was far greater than this, and a deeper problem was waiting for me.\nSecond Failure: Conflict Between NVHPC and GCC After correcting the CMake path issue, the compilation process did indeed begin, but it came to a grinding halt shortly thereafter. This time, the error log was far more cryptic. It was no longer a simple file-not-found error but pointed deep inside the header files of a C++ library, reporting a series of errors like error: extra text after expected end of number. These errors all originated from the file armadillo_bits/include_superlu.hpp, which is part of the Armadillo linear algebra library and is used to integrate the SuperLU sparse matrix solver.\nThe error message itself was very strange. It complained about extraneous text following a number, and all instances pointed to the same preprocessor macro line: #if __has_include(ARMA_INCFILE_WRAP(ARMA_SLU_HEADER_A)) \u0026\u0026 __has_include(ARMA_INCFILE_WRAP(ARMA_SLU_HEADER_B)). __has_include is a feature supported by modern C++ compilers to check for the existence of a header file at compile time. This type of error usually implies that the compiler is having trouble parsing this macro; it might not recognize the syntax, or the expanded content of the macro might not meet its expectations.\nInitially, I suspected that the versions of the Armadillo or SuperLU libraries were incompatible with the code. However, I was using Armadillo and SuperLU loaded via the Spack package manager, as recommended by the HPC administrators. The versions were relatively new and shouldn’t have had such basic syntax issues. I began to carefully review the job’s output log for more clues about the compilation process. Soon, I found the critical piece of information in the output from the CMake configuration stage:\n-- The C compiler identification is NVHPC 25.3.0 -- The CXX compiler identification is NVHPC 25.3.0\nThe mystery was solved. Even though I had loaded the GCC compiler via module load gcc/14.2.0, CMake had automatically selected the compiler from the NVIDIA HPC SDK (NVHPC). This is a common phenomenon on many modern HPC clusters, as the NVHPC compiler is often deeply integrated with the GPU environment, and the system may set it as the default C++ compiler.\nThis created a problem: the armadillo and superlu libraries I had loaded via Spack were almost certainly compiled using the system’s primary compiler, GCC 14.2.0. But now, CMake was instructing the NVHPC compiler to compile the Reanimate code, which depended on these GCC-compiled libraries. Armadillo’s header files, in an effort to be cross-platform compatible, contain a large number of preprocessor macros targeted at different compilers. The line that was failing, __has_include, was likely a modern feature specific to GCC or Clang. The version of NVHPC 25.3.0 I was using might not have supported it well, or it might have produced syntactically incompatible code upon macro expansion. This was a classic case of “environment hell” caused by mixed compilers.\nThe solution had to be to force CMake to use the GCC compiler I had specified, ensuring consistency across the entire build toolchain. To achieve this, I took several steps. First, in the Slurm script, I exported two crucial environment variables: export CC=gcc and export CXX=g++. These variables are a convention in Unix-like environments for specifying the default C and C++ compilers. When CMake starts, it checks these environment variables and gives them priority.\nHowever, simply setting environment variables wasn’t foolproof. CMake has a very important feature: it caches the compiler and environment information it detects during the first configuration into a file named CMakeCache.txt. If I didn’t clear this cache, CMake would stubbornly continue to use the NVHPC compiler it had found in the previous failed attempt, even with the new environment variables set. Therefore, before running the cmake command, I had to clean the build directory and remove all old CMake-generated files. I added the command rm -rf CMakeCache.txt CMakeFiles cmake_install.cmake Makefile to my script to ensure a completely fresh configuration every time.\nTo be absolutely certain, I also added parameters directly to the cmake command itself to specify the compilers: cmake -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++ ... This method has the highest priority and overrides any environment variables or system defaults.\nThis repair process was far more complex than the first. It required an understanding of the HPC’s module system, the Spack package manager, CMake’s inner workings, and the differences between C++ compilers. This problem also highlighted how crucial the principle of “explicit is better than implicit” is when building software in an HPC environment. One cannot rely on automatic detection by tools; one must explicitly tell them which compiler and which libraries to use to achieve predictable and reproducible results in a complex environment. With this deeper understanding, I resubmitted the job. This time, the compilation process successfully generated the object files, but the linking stage threw a new challenge.\nThird Failure: Linker Cannot Find Libraries The compilation process went through smoothly; all .cpp files were successfully compiled into .o object files. However, during the final linking stage, when ld (the linker) tried to link all the object files and external libraries into the final executable Reanimate, it failed. The error message was crystal clear:\n/usr/bin/ld: cannot find -lopenblas /usr/bin/ld: cannot find -lsuperlu\nThe linker was complaining that it couldn’t find the openblas and superlu libraries. This was very puzzling because I had explicitly loaded them at the beginning of the script using module load openblas/0.3.29/gcc-14.2.0 and spack load superlu@5.3.0. Theoretically, the environment should have been configured correctly.\nTo diagnose this issue, I needed to understand how library search paths work during compilation and linking. In Unix-like systems, there are two key environment variables: LD_LIBRARY_PATH and LIBRARY_PATH. LD_LIBRARY_PATH is primarily for runtime, telling the dynamic linker where to find shared libraries (.so files) when a program starts. LIBRARY_PATH, on the other hand, is for compile-time, providing the linker ld with an additional list of paths to search for both static (.a) and shared libraries. The module load and spack load commands typically update LD_LIBRARY_PATH correctly, but whether they update LIBRARY_PATH, or whether cmake automatically uses LIBRARY_PATH, is not always guaranteed.\nThe CMakeLists.txt file used commands like link_libraries(-llapack -lopenblas -lsuperlu). The -l syntax only tells the linker that it needs a library named lib.so or lib.a, but it doesn’t tell it where to find it. The linker searches a series of default paths (like /usr/lib, /usr/local/lib) as well as paths specified by the -L/path/to/lib argument. Clearly, the installation paths for openblas and superlu were not in the default search paths, and CMake had not automatically added them.\nMy solution had to be to find the actual installation paths of these libraries in the Slurm script and explicitly pass them to CMake. This required some scripting skills. For Spack-installed packages, I could use the command spack location -i to get the installation root directory. For example, spack location -i superlu@5.3.0 would return a path like /mnt/data2/users/.../superlu-5.3.0-.... The library files are usually in a lib or lib64 subdirectory. For packages loaded via module, there is often an environment variable like OPENBLAS_ROOT that points to the installation root. If not, I could fall back to parsing the LD_LIBRARY_PATH environment variable to find the path containing “openblas”.\nI added logic to my script to automatically detect these paths and store them in variables like SUPERLU_LIB, ARMADILLO_LIB, and OPENBLAS_LIB. Next, I needed to pass this path information to the linker. The most direct and robust method is to use CMake’s CMAKE_EXE_LINKER_FLAGS variable. I constructed a string like LINKER_FLAGS=\"-L/path/to/superlu/lib -L/path/to/openblas/lib\" and then passed it to CMake via the argument -DCMAKE_EXE_LINKER_FLAGS=\"$LINKER_FLAGS\". This ensures that in the final g++ linking command, these -L flags are correctly added, allowing ld to find the necessary library files. To be safe, I also passed the corresponding header file paths (-I/path/to/include) to CMAKE_CXX_FLAGS.\nThis issue once again confirmed the importance of explicitly specifying paths in an HPC environment. Merely loading a module is not always sufficient for all toolchains (especially complex build systems like CMake) to work seamlessly. A developer needs to understand the entire process from compilation to linking and know how to intervene manually when necessary to “translate” environment information into a language the build tools can understand. This fix gave me a deeper appreciation for the interaction between CMake and environment modules. With both compilation and linking successful, the Reanimate executable was finally generated. Next up was the .NET part.\nFourth Failure: .NET Runtime Not Found With the C++ part successfully compiled and the .NET dotnet build also completed, generating RetinaGen.dll, I was on the verge of success. I eagerly awaited the execution of the Python script. However, when the main program main.py reached the point where it called RetinaGen, the job crashed again. The error log showed:\nYou must install .NET to run this application. App: /.../RetinaGen/bin/Debug/net6.0/RetinaGen .NET location: Not found\nThis was a very perplexing problem. I had already loaded spack load dotnet-core-sdk@6.0.25 at the beginning of the script, and the dotnet build command had executed successfully, proving that the .NET SDK was present. Why, then, could the same program, when called from a Python script via subprocess.Popen, not find the .NET runtime?\nDiagnosing this requires an understanding of how .NET is deployed on Linux and the mechanisms of subprocess environment inheritance. The RetinaGen file generated by dotnet build is actually an “AppHost” executable. It’s a small, native launcher whose primary job is to find the .NET runtime on the system, load it, and then hand over the RetinaGen.dll (the actual assembly) to the runtime for execution. When this AppHost launcher fails to find the .NET runtime, it reports the error above.\nThe cause was likely related to environment propagation. Although I had loaded the Spack environment at the top level of my Slurm script, setting variables like PATH to make the dotnet executable visible, this environment might not have been fully inherited by the subprocess. When the Python interpreter starts as a process, and then it forks a child process to execute RetinaGen, this new child process may not inherit all the environment variables from its parent process (the Slurm job’s shell), especially those dynamically set by Spack to locate the .NET runtime (like DOTNET_ROOT).\nTo solve this, I decided to use a more robust way of calling the .NET program. Instead of running the AppHost (RetinaGen) directly, I could call the dotnet CLI directly and pass the DLL file as an argument: dotnet RetinaGen.dll. The advantage of this approach is that I’m directly using the dotnet executable, which itself knows how to find its associated runtime, thus bypassing the AppHost’s environment search problem. As long as dotnet is in the PATH, this command should work.\nTo implement this change, I couldn’t directly modify the Python source code in the repository, as this would affect its portability and integrity. The best approach was to “patch” it dynamically within the Slurm script. I once again turned to sed, the powerful stream editor. I first located the Python file that calls RetinaGen, which was retinasim/vascular.py. Then, before running main.py, I wrote a sed command to replace the line cmd = [exe_path, fname] in vascular.py with cmd = ['dotnet', exe_path, fname], while also changing the definition of EXE_PATH to point to RetinaGen.dll instead of RetinaGen. For safety, I created a backup file vascular.py.bak before making the modification.\nThis solution demonstrates an advanced technique for adapting to a specific runtime environment without altering the original codebase. In a batch processing environment, the ability to non-interactively and dynamically modify code to resolve environmental issues is an extremely practical skill. It not only solved the immediate problem but also kept the codebase clean, with all modifications documented in the Slurm script, making the entire process fully reproducible. After applying this patch, the .NET part of the call finally succeeded. But just when I thought I was done, one last obstacle related to the graphical interface appeared.\nFifth Failure: Open3D Rendering Crash After resolving all compilation and dependency issues, the program finally began executing its core simulation logic. However, within the generate_lsystem function, it crashed yet again. This time, the error was a Python runtime error related to the Open3D library:\n[Open3D WARNING] GLFW Error: Failed to detect any supported platform [Open3D WARNING] GLFW initialized for headless rendering. [Open3D WARNING] GLFW Error: OSMesa: Library not found [Open3D WARNING] Failed to create window AttributeError: 'NoneType' object has no attribute 'background_color'\nThe first part of the error message consists of warnings from Open3D. It tried to initialize a graphics window (via the GLFW library) but failed because it was running on a “headless” compute node without a physical display. It then attempted to fall back to offscreen rendering using OSMesa but also failed because the corresponding library was not found in the environment. Ultimately, because it could not create a window, the vis.create_window() call likely returned None.\nThe final AttributeError confirmed this diagnosis. The next line of code, opt.background_color = np.asarray(self.bgcolor), was trying to set the background color on a None object, causing the program to crash. Analyzing the generate_lsystem function call in main.py, I found a parameter screen_grab=True. This meant that even though I hadn’t requested an interactive display, the code was still trying to initialize a rendering environment to save an image.\nFor scientific computing tasks running on an HPC, intermediate visualizations are often unnecessary and should even be avoided. The goal is to obtain the final simulation data, not debug images. Therefore, the most direct and pragmatic solution was to disable this screenshot functionality.\nI once again resorted to sed. In the Slurm script, before running main.py, I added a command to patch the main.py file: sed -i 's/screen_grab=True/screen_grab=False/g' main.py. This command finds all instances of screen_grab=True in main.py and replaces them with screen_grab=False. I also added extra modifications to change the default values in argparse, ensuring that all plotting-related behaviors were turned off by default, even without command-line arguments. This fundamentally prevented any code path that would call Open3D’s window creation functions.\nThis fix embodies an important way of thinking in research and engineering practice: prioritizing what matters most. Fixing the complex headless rendering environment on an HPC node (which might require administrator privileges to install system-level dependencies) would have been a time-consuming task that strayed from the main objective. My core goal was to reproduce the simulation. Bypassing this problem with a simple code patch allowed me to focus on the final scientific output rather than struggling in the quagmire of environment configuration.\nAfter applying this final patch, I resubmitted the job. This time, there were no more errors in the log. I saw the program’s output, executing step by step as expected: creating the L-system seed network, writing Amira files, and launching the CCO vessel generation… The program was finally running completely and successfully on the Barkla2 cluster.\nThis end-to-end reproduction process was full of challenges, but every step of debugging and resolution deepened my understanding of HPC environments, multi-language project builds, and software dependency management. From simple path errors to complex compiler and linker issues, and finally to runtime environment differences, this series of obstacles is a microcosm of the universal challenges faced in migrating and reproducing scientific computing software. Through systematic analysis, bold hypotheses, careful validation, and a few scripting tricks, we can ultimately tame this complex “beast,” allowing scientific research to proceed smoothly on powerful computational resources.\n","wordCount":"3234","inLanguage":"en","datePublished":"2025-12-28T23:38:53Z","dateModified":"2025-12-28T23:38:53Z","author":{"@type":"Person","name":"Tategoto Azarasi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.tategotoazarasi.me/en/posts/reproducing-retinasim-on-hpc-cluster/"},"publisher":{"@type":"Organization","name":"Tategoto Azarasi","logo":{"@type":"ImageObject","url":"https://blog.tategotoazarasi.me/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://blog.tategotoazarasi.me/en/ accesskey=h title="Tategoto Azarasi (Alt + H)">Tategoto Azarasi</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://blog.tategotoazarasi.me/zh/ title=中文 aria-label=中文>Zh</a></li></ul></div></div><ul id=menu><li><a href=https://blog.tategotoazarasi.me/en/ title=Home><span>Home</span></a></li><li><a href=https://blog.tategotoazarasi.me/en/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://blog.tategotoazarasi.me/en/>Home</a>&nbsp;»&nbsp;<a href=https://blog.tategotoazarasi.me/en/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Reproducing RetinaSim on an HPC Cluster</h1><div class=post-description>A detailed account of reproducing the complex multi-language biomedical simulation project RetinaSim on a high-performance computing (HPC) cluster, covering every technical detail from debugging compilation errors to adapting to headless rendering environments.</div><div class=post-meta><span title='2025-12-28 23:38:53 +0000 UTC'>December 28, 2025</span>&nbsp;·&nbsp;16 min&nbsp;·&nbsp;3234 words&nbsp;·&nbsp;Tategoto Azarasi&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://blog.tategotoazarasi.me/zh/posts/reproducing-retinasim-on-hpc-cluster/>Zh</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ol><li><a href=#first-failure-cmakeliststxt-not-found>First Failure: CMakeLists.txt not found</a></li><li><a href=#second-failure-conflict-between-nvhpc-and-gcc>Second Failure: Conflict Between NVHPC and GCC</a></li><li><a href=#third-failure-linker-cannot-find-libraries>Third Failure: Linker Cannot Find Libraries</a></li><li><a href=#fourth-failure-net-runtime-not-found>Fourth Failure: .NET Runtime Not Found</a></li><li><a href=#fifth-failure-open3d-rendering-crash>Fifth Failure: Open3D Rendering Crash</a></li></ol></nav></div></details></div><div class=post-content><p>Recently, I undertook a rather challenging task: to fully reproduce a paper titled &ldquo;Physics-informed deep generative learning for quantitative assessment of the retina&rdquo; on a High-Performance Computing (HPC) cluster. The core software repository for this paper is RetinaSim. The goal was not merely to run the code, but to completely replicate its complex software stack and simulation workflow in a strictly managed computational environment, one that likely differed significantly from the original developers&rsquo;. This blog post will chronicle my entire journey from the initial attempt to the final successful run, focusing on my chain of thought as I diagnosed and resolved a series of tricky issues.</p><p>Our battlefield was a typical HPC cluster, &ldquo;Barkla2,&rdquo; running Rocky Linux 9 and using Slurm as its job scheduler. This meant all operations had to be performed via the command line, and any time-consuming computational tasks had to be submitted as batch jobs rather than run directly on the login node. The RetinaSim project itself is a complex hybrid, merging Python scripts for main workflow control with a high-performance fluid dynamics simulator (Reanimate) written in C++ and a vessel generation program (RetinaGen) written in .NET (C#). This heterogeneous technology stack almost guaranteed that we would encounter a variety of unexpected compilation and runtime problems in a new environment.</p><p>My first step was to obtain the code and draft an initial Slurm script. The code was cloned via <code>git</code>, and its directory structure clearly laid out the various submodules. After a preliminary analysis of files like <code>README</code> and <code>CMakeLists.txt</code>, I understood that compiling Reanimate required CMake and a C++ compiler, while RetinaGen needed the .NET SDK. The Python part depended on a <code>requirements.txt</code> file.</p><p>Based on this information, I wrote the first version of my Slurm script. The script&rsquo;s goal was to complete all preparatory work sequentially: load the necessary environment modules (like the GCC compiler, CMake, and Python), create a Python virtual environment and install dependencies, compile the C++ and .NET submodules, and finally, attempt to run the main Python script, <code>main.py</code>. This was a standard, seemingly straightforward process. However, reality quickly delivered its first blow.</p><h2 id=first-failure-cmakeliststxt-not-found>First Failure: CMakeLists.txt not found<a hidden class=anchor aria-hidden=true href=#first-failure-cmakeliststxt-not-found>#</a></h2><p>Seconds after submitting the first job, it failed. Checking the error log, I saw a familiar and fundamental error message: <code>CMake Error: The source directory "..." does not appear to contain CMakeLists.txt</code>. This error means that CMake could not find its core configuration file, <code>CMakeLists.txt</code>, in the directory I had specified.</p><p>My immediate reaction was to check the paths in my script. To compile the Reanimate submodule, my Slurm script had changed directory to <code>retinasim/Reanimate/Reanimate</code> and then executed the <code>cmake .</code> command. This command tells CMake to use the current directory as the root of the source tree. However, after carefully inspecting the project structure with the <code>ls -R</code> command, I discovered that the <code>CMakeLists.txt</code> file was actually located in the <code>retinasim/Reanimate</code> directory, not in its subdirectory <code>retinasim/Reanimate/Reanimate</code>. This was a classic relative path error, one that is easy to make, especially when dealing with nested subprojects.</p><p>The diagnosis was straightforward. Since <code>CMakeLists.txt</code> was in the parent directory, the solution was to tell CMake to look there. In Unix-like systems, &ldquo;..&rdquo; represents the parent directory. Therefore, I needed to change the compile command from <code>cmake .</code> to <code>cmake ..</code>. This small change had significant meaning: it instructed CMake to look one level up from the current directory (<code>Reanimate/Reanimate</code>) for <code>CMakeLists.txt</code>, while still using the current <code>Reanimate/Reanimate</code> directory as the build directory. This way, CMake could find the configuration file and place all generated build files (like Makefiles, object files, and the final executable) in my current location, keeping the source tree clean.</p><p>This small episode, though simple, served as a reminder that when dealing with unfamiliar and complex projects, especially those involving multiple languages and build systems, the first step is always to carefully and patiently review the directory structure and build scripts. Assuming that configuration files will be in some &ldquo;obvious&rdquo; location is a common cause of elementary errors. After fixing this issue, I resubmitted the job with renewed confidence, hoping the compilation process would now proceed smoothly. However, the complexity of the HPC environment was far greater than this, and a deeper problem was waiting for me.</p><h2 id=second-failure-conflict-between-nvhpc-and-gcc>Second Failure: Conflict Between NVHPC and GCC<a hidden class=anchor aria-hidden=true href=#second-failure-conflict-between-nvhpc-and-gcc>#</a></h2><p>After correcting the CMake path issue, the compilation process did indeed begin, but it came to a grinding halt shortly thereafter. This time, the error log was far more cryptic. It was no longer a simple file-not-found error but pointed deep inside the header files of a C++ library, reporting a series of errors like <code>error: extra text after expected end of number</code>. These errors all originated from the file <code>armadillo_bits/include_superlu.hpp</code>, which is part of the Armadillo linear algebra library and is used to integrate the SuperLU sparse matrix solver.</p><p>The error message itself was very strange. It complained about extraneous text following a number, and all instances pointed to the same preprocessor macro line: <code>#if __has_include(ARMA_INCFILE_WRAP(ARMA_SLU_HEADER_A)) && __has_include(ARMA_INCFILE_WRAP(ARMA_SLU_HEADER_B))</code>. <code>__has_include</code> is a feature supported by modern C++ compilers to check for the existence of a header file at compile time. This type of error usually implies that the compiler is having trouble parsing this macro; it might not recognize the syntax, or the expanded content of the macro might not meet its expectations.</p><p>Initially, I suspected that the versions of the Armadillo or SuperLU libraries were incompatible with the code. However, I was using Armadillo and SuperLU loaded via the Spack package manager, as recommended by the HPC administrators. The versions were relatively new and shouldn&rsquo;t have had such basic syntax issues. I began to carefully review the job&rsquo;s output log for more clues about the compilation process. Soon, I found the critical piece of information in the output from the CMake configuration stage:</p><p><code>-- The C compiler identification is NVHPC 25.3.0</code>
<code>-- The CXX compiler identification is NVHPC 25.3.0</code></p><p>The mystery was solved. Even though I had loaded the GCC compiler via <code>module load gcc/14.2.0</code>, CMake had automatically selected the compiler from the NVIDIA HPC SDK (NVHPC). This is a common phenomenon on many modern HPC clusters, as the NVHPC compiler is often deeply integrated with the GPU environment, and the system may set it as the default C++ compiler.</p><p>This created a problem: the <code>armadillo</code> and <code>superlu</code> libraries I had loaded via Spack were almost certainly compiled using the system&rsquo;s primary compiler, GCC 14.2.0. But now, CMake was instructing the NVHPC compiler to compile the Reanimate code, which depended on these GCC-compiled libraries. Armadillo&rsquo;s header files, in an effort to be cross-platform compatible, contain a large number of preprocessor macros targeted at different compilers. The line that was failing, <code>__has_include</code>, was likely a modern feature specific to GCC or Clang. The version of NVHPC 25.3.0 I was using might not have supported it well, or it might have produced syntactically incompatible code upon macro expansion. This was a classic case of &ldquo;environment hell&rdquo; caused by mixed compilers.</p><p>The solution had to be to force CMake to use the GCC compiler I had specified, ensuring consistency across the entire build toolchain. To achieve this, I took several steps. First, in the Slurm script, I exported two crucial environment variables: <code>export CC=gcc</code> and <code>export CXX=g++</code>. These variables are a convention in Unix-like environments for specifying the default C and C++ compilers. When CMake starts, it checks these environment variables and gives them priority.</p><p>However, simply setting environment variables wasn&rsquo;t foolproof. CMake has a very important feature: it caches the compiler and environment information it detects during the first configuration into a file named <code>CMakeCache.txt</code>. If I didn&rsquo;t clear this cache, CMake would stubbornly continue to use the NVHPC compiler it had found in the previous failed attempt, even with the new environment variables set. Therefore, before running the <code>cmake</code> command, I had to clean the build directory and remove all old CMake-generated files. I added the command <code>rm -rf CMakeCache.txt CMakeFiles cmake_install.cmake Makefile</code> to my script to ensure a completely fresh configuration every time.</p><p>To be absolutely certain, I also added parameters directly to the <code>cmake</code> command itself to specify the compilers: <code>cmake -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++ ..</code>. This method has the highest priority and overrides any environment variables or system defaults.</p><p>This repair process was far more complex than the first. It required an understanding of the HPC&rsquo;s module system, the Spack package manager, CMake&rsquo;s inner workings, and the differences between C++ compilers. This problem also highlighted how crucial the principle of &ldquo;explicit is better than implicit&rdquo; is when building software in an HPC environment. One cannot rely on automatic detection by tools; one must explicitly tell them which compiler and which libraries to use to achieve predictable and reproducible results in a complex environment. With this deeper understanding, I resubmitted the job. This time, the compilation process successfully generated the object files, but the linking stage threw a new challenge.</p><h2 id=third-failure-linker-cannot-find-libraries>Third Failure: Linker Cannot Find Libraries<a hidden class=anchor aria-hidden=true href=#third-failure-linker-cannot-find-libraries>#</a></h2><p>The compilation process went through smoothly; all <code>.cpp</code> files were successfully compiled into <code>.o</code> object files. However, during the final linking stage, when <code>ld</code> (the linker) tried to link all the object files and external libraries into the final executable <code>Reanimate</code>, it failed. The error message was crystal clear:</p><p><code>/usr/bin/ld: cannot find -lopenblas</code>
<code>/usr/bin/ld: cannot find -lsuperlu</code></p><p>The linker was complaining that it couldn&rsquo;t find the <code>openblas</code> and <code>superlu</code> libraries. This was very puzzling because I had explicitly loaded them at the beginning of the script using <code>module load openblas/0.3.29/gcc-14.2.0</code> and <code>spack load superlu@5.3.0</code>. Theoretically, the environment should have been configured correctly.</p><p>To diagnose this issue, I needed to understand how library search paths work during compilation and linking. In Unix-like systems, there are two key environment variables: <code>LD_LIBRARY_PATH</code> and <code>LIBRARY_PATH</code>. <code>LD_LIBRARY_PATH</code> is primarily for <strong>runtime</strong>, telling the dynamic linker where to find shared libraries (<code>.so</code> files) when a program starts. <code>LIBRARY_PATH</code>, on the other hand, is for <strong>compile-time</strong>, providing the linker <code>ld</code> with an additional list of paths to search for both static (<code>.a</code>) and shared libraries. The <code>module load</code> and <code>spack load</code> commands typically update <code>LD_LIBRARY_PATH</code> correctly, but whether they update <code>LIBRARY_PATH</code>, or whether <code>cmake</code> automatically uses <code>LIBRARY_PATH</code>, is not always guaranteed.</p><p>The <code>CMakeLists.txt</code> file used commands like <code>link_libraries(-llapack -lopenblas -lsuperlu)</code>. The <code>-l&lt;name></code> syntax only tells the linker that it needs a library named <code>lib&lt;name>.so</code> or <code>lib&lt;name>.a</code>, but it doesn&rsquo;t tell it where to find it. The linker searches a series of default paths (like <code>/usr/lib</code>, <code>/usr/local/lib</code>) as well as paths specified by the <code>-L/path/to/lib</code> argument. Clearly, the installation paths for <code>openblas</code> and <code>superlu</code> were not in the default search paths, and CMake had not automatically added them.</p><p>My solution had to be to find the actual installation paths of these libraries in the Slurm script and explicitly pass them to CMake. This required some scripting skills. For Spack-installed packages, I could use the command <code>spack location -i &lt;package_name></code> to get the installation root directory. For example, <code>spack location -i superlu@5.3.0</code> would return a path like <code>/mnt/data2/users/.../superlu-5.3.0-...</code>. The library files are usually in a <code>lib</code> or <code>lib64</code> subdirectory. For packages loaded via <code>module</code>, there is often an environment variable like <code>OPENBLAS_ROOT</code> that points to the installation root. If not, I could fall back to parsing the <code>LD_LIBRARY_PATH</code> environment variable to find the path containing &ldquo;openblas&rdquo;.</p><p>I added logic to my script to automatically detect these paths and store them in variables like <code>SUPERLU_LIB</code>, <code>ARMADILLO_LIB</code>, and <code>OPENBLAS_LIB</code>. Next, I needed to pass this path information to the linker. The most direct and robust method is to use CMake&rsquo;s <code>CMAKE_EXE_LINKER_FLAGS</code> variable. I constructed a string like <code>LINKER_FLAGS="-L/path/to/superlu/lib -L/path/to/openblas/lib"</code> and then passed it to CMake via the argument <code>-DCMAKE_EXE_LINKER_FLAGS="$LINKER_FLAGS"</code>. This ensures that in the final <code>g++</code> linking command, these <code>-L</code> flags are correctly added, allowing <code>ld</code> to find the necessary library files. To be safe, I also passed the corresponding header file paths (<code>-I/path/to/include</code>) to <code>CMAKE_CXX_FLAGS</code>.</p><p>This issue once again confirmed the importance of explicitly specifying paths in an HPC environment. Merely loading a module is not always sufficient for all toolchains (especially complex build systems like CMake) to work seamlessly. A developer needs to understand the entire process from compilation to linking and know how to intervene manually when necessary to &ldquo;translate&rdquo; environment information into a language the build tools can understand. This fix gave me a deeper appreciation for the interaction between CMake and environment modules. With both compilation and linking successful, the Reanimate executable was finally generated. Next up was the .NET part.</p><h2 id=fourth-failure-net-runtime-not-found>Fourth Failure: .NET Runtime Not Found<a hidden class=anchor aria-hidden=true href=#fourth-failure-net-runtime-not-found>#</a></h2><p>With the C++ part successfully compiled and the .NET <code>dotnet build</code> also completed, generating <code>RetinaGen.dll</code>, I was on the verge of success. I eagerly awaited the execution of the Python script. However, when the main program <code>main.py</code> reached the point where it called RetinaGen, the job crashed again. The error log showed:</p><p><code>You must install .NET to run this application.</code>
<code>App: /.../RetinaGen/bin/Debug/net6.0/RetinaGen</code>
<code>.NET location: Not found</code></p><p>This was a very perplexing problem. I had already loaded <code>spack load dotnet-core-sdk@6.0.25</code> at the beginning of the script, and the <code>dotnet build</code> command had executed successfully, proving that the .NET SDK was present. Why, then, could the same program, when called from a Python script via <code>subprocess.Popen</code>, not find the .NET runtime?</p><p>Diagnosing this requires an understanding of how .NET is deployed on Linux and the mechanisms of subprocess environment inheritance. The <code>RetinaGen</code> file generated by <code>dotnet build</code> is actually an &ldquo;AppHost&rdquo; executable. It&rsquo;s a small, native launcher whose primary job is to find the .NET runtime on the system, load it, and then hand over the <code>RetinaGen.dll</code> (the actual assembly) to the runtime for execution. When this AppHost launcher fails to find the .NET runtime, it reports the error above.</p><p>The cause was likely related to environment propagation. Although I had loaded the Spack environment at the top level of my Slurm script, setting variables like <code>PATH</code> to make the <code>dotnet</code> executable visible, this environment might not have been fully inherited by the subprocess. When the Python interpreter starts as a process, and then it forks a child process to execute <code>RetinaGen</code>, this new child process may not inherit all the environment variables from its parent process (the Slurm job&rsquo;s shell), especially those dynamically set by Spack to locate the .NET runtime (like <code>DOTNET_ROOT</code>).</p><p>To solve this, I decided to use a more robust way of calling the .NET program. Instead of running the AppHost (<code>RetinaGen</code>) directly, I could call the <code>dotnet</code> CLI directly and pass the DLL file as an argument: <code>dotnet RetinaGen.dll</code>. The advantage of this approach is that I&rsquo;m directly using the <code>dotnet</code> executable, which itself knows how to find its associated runtime, thus bypassing the AppHost&rsquo;s environment search problem. As long as <code>dotnet</code> is in the <code>PATH</code>, this command should work.</p><p>To implement this change, I couldn&rsquo;t directly modify the Python source code in the repository, as this would affect its portability and integrity. The best approach was to &ldquo;patch&rdquo; it dynamically within the Slurm script. I once again turned to <code>sed</code>, the powerful stream editor. I first located the Python file that calls RetinaGen, which was <code>retinasim/vascular.py</code>. Then, before running <code>main.py</code>, I wrote a <code>sed</code> command to replace the line <code>cmd = [exe_path, fname]</code> in <code>vascular.py</code> with <code>cmd = ['dotnet', exe_path, fname]</code>, while also changing the definition of <code>EXE_PATH</code> to point to <code>RetinaGen.dll</code> instead of <code>RetinaGen</code>. For safety, I created a backup file <code>vascular.py.bak</code> before making the modification.</p><p>This solution demonstrates an advanced technique for adapting to a specific runtime environment without altering the original codebase. In a batch processing environment, the ability to non-interactively and dynamically modify code to resolve environmental issues is an extremely practical skill. It not only solved the immediate problem but also kept the codebase clean, with all modifications documented in the Slurm script, making the entire process fully reproducible. After applying this patch, the .NET part of the call finally succeeded. But just when I thought I was done, one last obstacle related to the graphical interface appeared.</p><h2 id=fifth-failure-open3d-rendering-crash>Fifth Failure: Open3D Rendering Crash<a hidden class=anchor aria-hidden=true href=#fifth-failure-open3d-rendering-crash>#</a></h2><p>After resolving all compilation and dependency issues, the program finally began executing its core simulation logic. However, within the <code>generate_lsystem</code> function, it crashed yet again. This time, the error was a Python runtime error related to the Open3D library:</p><p><code>[Open3D WARNING] GLFW Error: Failed to detect any supported platform</code>
<code>[Open3D WARNING] GLFW initialized for headless rendering.</code>
<code>[Open3D WARNING] GLFW Error: OSMesa: Library not found</code>
<code>[Open3D WARNING] Failed to create window</code>
<code>AttributeError: 'NoneType' object has no attribute 'background_color'</code></p><p>The first part of the error message consists of warnings from Open3D. It tried to initialize a graphics window (via the GLFW library) but failed because it was running on a &ldquo;headless&rdquo; compute node without a physical display. It then attempted to fall back to offscreen rendering using <code>OSMesa</code> but also failed because the corresponding library was not found in the environment. Ultimately, because it could not create a window, the <code>vis.create_window()</code> call likely returned <code>None</code>.</p><p>The final <code>AttributeError</code> confirmed this diagnosis. The next line of code, <code>opt.background_color = np.asarray(self.bgcolor)</code>, was trying to set the background color on a <code>None</code> object, causing the program to crash. Analyzing the <code>generate_lsystem</code> function call in <code>main.py</code>, I found a parameter <code>screen_grab=True</code>. This meant that even though I hadn&rsquo;t requested an interactive display, the code was still trying to initialize a rendering environment to save an image.</p><p>For scientific computing tasks running on an HPC, intermediate visualizations are often unnecessary and should even be avoided. The goal is to obtain the final simulation data, not debug images. Therefore, the most direct and pragmatic solution was to disable this screenshot functionality.</p><p>I once again resorted to <code>sed</code>. In the Slurm script, before running <code>main.py</code>, I added a command to patch the <code>main.py</code> file: <code>sed -i 's/screen_grab=True/screen_grab=False/g' main.py</code>. This command finds all instances of <code>screen_grab=True</code> in <code>main.py</code> and replaces them with <code>screen_grab=False</code>. I also added extra modifications to change the default values in <code>argparse</code>, ensuring that all plotting-related behaviors were turned off by default, even without command-line arguments. This fundamentally prevented any code path that would call Open3D&rsquo;s window creation functions.</p><p>This fix embodies an important way of thinking in research and engineering practice: prioritizing what matters most. Fixing the complex headless rendering environment on an HPC node (which might require administrator privileges to install system-level dependencies) would have been a time-consuming task that strayed from the main objective. My core goal was to reproduce the simulation. Bypassing this problem with a simple code patch allowed me to focus on the final scientific output rather than struggling in the quagmire of environment configuration.</p><p>After applying this final patch, I resubmitted the job. This time, there were no more errors in the log. I saw the program&rsquo;s output, executing step by step as expected: creating the L-system seed network, writing Amira files, and launching the CCO vessel generation&mldr; The program was finally running completely and successfully on the Barkla2 cluster.</p><p>This end-to-end reproduction process was full of challenges, but every step of debugging and resolution deepened my understanding of HPC environments, multi-language project builds, and software dependency management. From simple path errors to complex compiler and linker issues, and finally to runtime environment differences, this series of obstacles is a microcosm of the universal challenges faced in migrating and reproducing scientific computing software. Through systematic analysis, bold hypotheses, careful validation, and a few scripting tricks, we can ultimately tame this complex &ldquo;beast,&rdquo; allowing scientific research to proceed smoothly on powerful computational resources.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://blog.tategotoazarasi.me/en/tags/retinasim/>Retinasim</a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/hpc/>Hpc</a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/scientific-computing/>Scientific-Computing</a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/reproduction/>Reproduction</a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/cmake/>Cmake</a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/slurm/>Slurm</a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/spack/>Spack</a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/dotnet/>Dotnet</a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/c-plus-plus/>C-Plus-Plus</a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/python/>Python</a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/debugging/>Debugging</a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/open3d/>Open3d</a></li><li><a href=https://blog.tategotoazarasi.me/en/tags/headless-rendering/>Headless-Rendering</a></li></ul><nav class=paginav><a class=prev href=https://blog.tategotoazarasi.me/en/posts/matrix-multiplication-benchmark-hpc-cuda-python/><span class=title>« Prev</span><br><span>Analysis of HPC Matrix Multiplication Performance Benchmarking</span>
</a><a class=next href=https://blog.tategotoazarasi.me/en/posts/building-a-data-driven-city-python-gis-visualization/><span class=title>Next »</span><br><span>Building a Data-Driven City: Integrating Housing, Safety, and Deprivation Metrics with Python</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://blog.tategotoazarasi.me/en/>Tategoto Azarasi</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>